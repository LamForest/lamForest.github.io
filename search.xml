<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>[噩耗] gitee图床用不了了，本博客所有图片都裂开了</title>
      <link href="2022/05/03/diary/e-hao-gitee-tu-chuang-yong-bu-liao/"/>
      <url>2022/05/03/diary/e-hao-gitee-tu-chuang-yong-bu-liao/</url>
      
        <content type="html"><![CDATA[<p>原因：<a href="https://github.com/zhanghuid/picgo-plugin-gitee/issues/11">https://github.com/zhanghuid/picgo-plugin-gitee/issues/11</a><br>解决方法：<a href="https://zhuanlan.zhihu.com/p/138582151">https://zhuanlan.zhihu.com/p/138582151</a></p>]]></content>
      
      
      <categories>
          
          <category> 新闻 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gitee </tag>
            
            <tag> github </tag>
            
            <tag> Typora </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[C++][标准库源码] sort ———— 性能极致的排序算法</title>
      <link href="2022/05/01/cpp/c-biao-zhun-ku-sort-xing-neng-ji-zhi-de-pai-xu-suan-fa/"/>
      <url>2022/05/01/cpp/c-biao-zhun-ku-sort-xing-neng-ji-zhi-de-pai-xu-suan-fa/</url>
      
        <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><blockquote><p><code>std::sort</code> 的实现基于 David Musser 1996年提出的一种混合式排序算法 Introspective Sorting，即IntroSort，其行为几乎与median-of-three QuickSort完全相同，但是当Partition有恶化的倾向时，能够自我侦测，转而改用HeapSort，使复杂度维持在HeapSort的 <code>O(NlonN)</code>。</p><p>—— 侯捷《STL源码剖析》</p></blockquote><p>《STL源码剖析》这本书历史悠久，至今已有20年历史。不过20年时间在C++的历史长河中算不了什么。20年过去，GCC C++ STL 大部分算法、数据结构的实现和SGI STL基本相同。所以，侯捷20年前对于 <code>std::sort</code> 的描述依然适用，书中对  <code>std::sort</code> 源码的剖析依然准确。如果想从头了解 <code>std::sort</code> ，可以去看这本书；这里我的侧重点与《STL源码剖析》中不同，更关注一些细节和trick。</p><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><ol><li><p>introsort，大体框架上遵循传统的快排算法，在极端情况下转为堆排序；<code>std::sort</code>在最后，会在整个数组上做一次插入排序。</p></li><li><p>introsort在GCC源码中涉及了两处 以<code>__unguarded__......</code> 为名的函数，它们的名字暗示了它们不会做越界检查，已达到更快的速度。本文特地分析了这两个函数，尤其是它们<strong>不做越界检查的情况下，如何避免越界</strong>。</p></li><li>在快排中 pivot的选择是一件non-trivial的事情，introsort选用了median-of-three方法，这又与一个 <code>__unguarded</code> 函数产生了联动。</li><li>在源码中，插入排序有两种实现，它们有什么区别？</li></ol><h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><p>所代码来自 <a href="https://github.com/gcc-mirror/gcc/blob/releases/gcc-5/libstdc%2B%2B-v3/include/bits/stl_algo.h#L1956">gcc stl_algo.h</a>，建议搭配源码阅读。</p><p><code>std::sort</code> 进行如下两步</p><ol><li><code>__introsort_loop(__first, __last, __lg(__last - __first) * 2)</code>; 第三个参数中的<code>__lg</code> 为 $log_2$，该参数用于控制快排的递归层数；例如，当数组长度为1024时，<code>__lg(1024) * 2</code>为 20，即最多递归20层。如果递归层数过深，那么很容易趋向最坏时间复杂度 $O(n^2)$。</li><li><code>__final_insertion_sort(__first, __last)</code> ：对整个数组做一次插入排序。一般情况下，插入排序的复杂度为 $O(n^2)$。不过，由于 <code>__introsort_loop</code> 在区间不大于 16 时停止排序，此时，整个序列呈现<strong>整体有序，局部乱序</strong>的结果；也就是说，每个子序列里的数一定大于等于前一个子序列中任何数。在这种情况下，插入排序的时间复杂度并不高。</li></ol><h3 id="introsort-loop"><a href="#introsort-loop" class="headerlink" title="__introsort_loop"></a><code>__introsort_loop</code></h3><p>该函数和传统的QuickSort具有类似的框架，只是细节上有一些不同。</p><p>如果区间长度大于 <code>S_threshold( = 16)</code>，则：</p><ol><li><p>如果<code>depth_limit == 0</code>，则转为堆排序 <code>__partial_sort</code>。</p><p>这意味着尽管通过median-of-three选择了较优的pivot，但是左右子数组仍然不平衡，趋向于最坏复杂度 $O(N^2)$。这意味着快排在这个数组上不适用，于是转而使用堆排序。</p></li><li><p>调用<code>__unguarded_partition_pivot</code> ，进行<strong>choice of pivot</strong>和<strong>partition</strong>。</p><ul><li><p><strong>choice of pivot</strong>：快排的时间复杂度度与pivot的选择息息相关，最简单的办法是选择区间的第一个元素，但是，如果待排序数组为有序数组，那么时间复杂度会退化至 $O(N^2)$。其他方法包括 random pivot， median-of-three等。但是，选择Pivot的复杂度需要严格控制，过于复杂的规则会引入较高的overhead，反而导致时间复杂度升高。</p><p>关于pivot的选择，是个经过了非常深入研究的问题，可以参考[1]，[1]表明random pivot的平均复杂度约为$1.385nlogn$ ，而median-of-three的复杂度约为 $1.188nlogn$。对于特别大的数组，ninther rule更适用，它是median-of-three的递归形式，尽管这引入了更大的overhead，但是对于足够大的数组，这一点overhead影响不大。</p><p>gcc中使用的是”median-of-three“， </p><blockquote><p>median-of-three并不是完美的方法，[3]中构造了一种特别的数组针对该方法。</p></blockquote></li><li><p><strong>Partition</strong>：Partition由<code>__unguarded_partition</code> 实现，</p></li></ul><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token boolean">true</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>  <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token function">__comp</span><span class="token punctuation">(</span>__first<span class="token punctuation">,</span> __pivot<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">//不会判断 first &lt;= last</span>    <span class="token operator">++</span>__first<span class="token punctuation">;</span>  <span class="token operator">--</span>__last<span class="token punctuation">;</span>  <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token function">__comp</span><span class="token punctuation">(</span>__pivot<span class="token punctuation">,</span> __last<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">//同上</span>    <span class="token operator">--</span>__last<span class="token punctuation">;</span>  <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span><span class="token punctuation">(</span>__first <span class="token operator">&lt;</span> __last<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> __first<span class="token punctuation">;</span>  std<span class="token operator">::</span><span class="token function">iter_swap</span><span class="token punctuation">(</span>__first<span class="token punctuation">,</span> __last<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token operator">++</span>__first<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>它的操作与一般的快排基本一致。唯一的区别是，循环时不会判断 <code>first &lt;= last</code>，但这并不会导致越界。</p><p>这是因为之前使用了median-of-three，所以数组中至少有一个元素大于或等于pivot，所以 <code>while (__comp(__first, __pivot))</code>一定会在某个元素处停止，一定不会越界。这为代码减少了2处越界条件判断，略微提升了速度。</p><blockquote><p>经常会有这么一种bug，如果<code>__comp</code> 仿函数存在错误，可能会导致越界[2]，比如：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">std<span class="token operator">::</span>vector<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> values<span class="token punctuation">&#123;</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">&#125;</span><span class="token punctuation">;</span>std<span class="token operator">::</span><span class="token function">sort</span><span class="token punctuation">(</span>values<span class="token punctuation">.</span><span class="token function">begin</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> values<span class="token punctuation">.</span><span class="token function">end</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">(</span><span class="token keyword">int</span> v1<span class="token punctuation">,</span> <span class="token keyword">int</span> v2<span class="token punctuation">)</span><span class="token punctuation">&#123;</span><span class="token keyword">return</span> v1 <span class="token operator">>=</span> v2<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><a href="https://onlinegdb.com/qVqxgESB8">试试看</a></p><p>这就是__unguarded_partition不做越界检查导致的。</p></blockquote></li></ol><ol><li><p>将右半部分进行<strong>递归</strong>，继续调用 <code>__introsort_loop</code>。</p></li><li><p>将左半部分进行<strong>循环</strong>。传统的快排中，会将左右部分分别进行递归快排，gcc中为了省去递归函数调用时所产生的开销，省去了左半部分的递归，改用循环处理。</p><blockquote><p>这种写法可读性较差，但效率并不比两次递归更好。—— 《STL源码剖析》</p></blockquote></li></ol><p>如果序列长度 <code>&lt;= 16</code>，则返回，等待最后的插入排序处理。因为快排对于小的数据量并不划算，快排产生的多次递归调用开销导致其速度甚至不如插入排序。</p><h3 id="final-insertion-sort"><a href="#final-insertion-sort" class="headerlink" title="__final_insertion_sort"></a><code>__final_insertion_sort</code></h3><p>这个部分主要包含两个函数 <code>__insertion_sort</code> 和 <code>__unguarded_linear_sort</code>。</p><p>他们都是插入排序，区别在于： </p><ul><li><code>__unguarded_linear_sort</code>需要传入的数组的前方存在一个较小或相等的值，像一个guard一样，阻止循环越界。</li><li><code>__insertion_sort</code> 不需要任何要求，可以接受任何数组；</li></ul><h4 id="unguarded-linear-sort"><a href="#unguarded-linear-sort" class="headerlink" title="__unguarded_linear_sort"></a><code>__unguarded_linear_sort</code></h4><p>插入排序维护两个部分，已排序、未排序部分。它包含两重循环，每次外部循环将当前数插入到已排序部分合适的位置。</p><p> <code>__unguarded_insertion_sort</code> 了实现外层循环，内层循环（即插入）调用了函数 <code>__unguarded_linear_insert</code>完成。</p><ul><li><p><code>__unguarded_linear_insert</code> 该函数用于寻找合适的位置并插入。它的核心代码如下：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"> <span class="token function">__unguarded_linear_insert</span><span class="token punctuation">(</span>_RandomAccessIterator __last<span class="token punctuation">,</span>      _Compare __comp<span class="token punctuation">)</span>  <span class="token punctuation">&#123;</span>__val <span class="token operator">=</span> <span class="token function">_GLIBCXX_MOVE</span><span class="token punctuation">(</span><span class="token operator">*</span>__last<span class="token punctuation">)</span><span class="token punctuation">;</span>    _RandomAccessIterator __next <span class="token operator">=</span> __last<span class="token punctuation">;</span>    <span class="token operator">--</span>__next<span class="token punctuation">;</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span><span class="token function">__comp</span><span class="token punctuation">(</span>__val<span class="token punctuation">,</span> __next<span class="token punctuation">)</span> <span class="token comment">/* 缺少：&amp;&amp; __next >= __first */</span><span class="token punctuation">)</span> <span class="token comment">//可能越界</span>      <span class="token punctuation">&#123;</span>        <span class="token operator">*</span>__last <span class="token operator">=</span> <span class="token function">_GLIBCXX_MOVE</span><span class="token punctuation">(</span><span class="token operator">*</span>__next<span class="token punctuation">)</span><span class="token punctuation">;</span>        __last <span class="token operator">=</span> __next<span class="token punctuation">;</span>        <span class="token operator">--</span>__next<span class="token punctuation">;</span>      <span class="token punctuation">&#125;</span>    <span class="token operator">*</span>__last <span class="token operator">=</span> <span class="token function">_GLIBCXX_MOVE</span><span class="token punctuation">(</span>__val<span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token punctuation">&#125;</span>  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看到，这个函数完全不关注 <code>__first</code>，根本不考虑 <code>__next</code> 是否可能越界  。所以，<code>__unguarded_linear_insert</code>的调用者需要<strong>保证</strong>，<code>__last</code>之前必须存在一个数小于等于 <code>*__last</code>，使得while循环终止。间接的，<code>__unguarded_linear_sort</code> 的调用者也需要保证这点。</p></li></ul><h4 id="insertion-sort"><a href="#insertion-sort" class="headerlink" title="__insertion_sort"></a><code>__insertion_sort</code></h4><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token function">__insertion_sort</span><span class="token punctuation">(</span>_RandomAccessIterator __first<span class="token punctuation">,</span>   _RandomAccessIterator __last<span class="token punctuation">,</span> _Compare __comp<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>__first <span class="token operator">==</span> __last<span class="token punctuation">)</span> <span class="token keyword">return</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span>_RandomAccessIterator __i <span class="token operator">=</span> __first <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">;</span> __i <span class="token operator">!=</span> __last<span class="token punctuation">;</span> <span class="token operator">++</span>__i<span class="token punctuation">)</span>    <span class="token punctuation">&#123;</span>        <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token function">__comp</span><span class="token punctuation">(</span>__i<span class="token punctuation">,</span> __first<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">// 特例，不满足__unguarded_linear_insert的前提条件</span>        <span class="token punctuation">&#123;</span>            <span class="token keyword">typename</span> <span class="token class-name">iterator_traits</span><span class="token operator">&lt;</span>_RandomAccessIterator<span class="token operator">></span><span class="token operator">::</span>value_type        __val <span class="token operator">=</span> <span class="token function">_GLIBCXX_MOVE</span><span class="token punctuation">(</span><span class="token operator">*</span>__i<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">//move</span>            <span class="token function">_GLIBCXX_MOVE_BACKWARD3</span><span class="token punctuation">(</span>__first<span class="token punctuation">,</span> __i<span class="token punctuation">,</span> __i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">//copy_backward</span>            <span class="token operator">*</span>__first <span class="token operator">=</span> <span class="token function">_GLIBCXX_MOVE</span><span class="token punctuation">(</span>__val<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">&#125;</span>        <span class="token keyword">else</span>        std<span class="token operator">::</span><span class="token function">__unguarded_linear_insert</span><span class="token punctuation">(</span>__i<span class="token punctuation">,</span>                __gnu_cxx<span class="token operator">::</span>__ops<span class="token operator">::</span><span class="token function">__val_comp_iter</span><span class="token punctuation">(</span>__comp<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在<code>__insertion_sort</code> 中，如果 <code>__i</code> 比 已排序序列中的最小的数 <code>__i</code> 还小，则不满足<code>__unguarded_linear_insert</code>的前提条件。所以需要单独处理，将 <code>__i</code> 插到第一位，后面的元素整体后移一位。</p><h4 id="final-insertion-sort-1"><a href="#final-insertion-sort-1" class="headerlink" title="__final_insertion_sort"></a><code>__final_insertion_sort</code></h4><p>了解了<code>__unguarded_linear_insert</code>的前提条件，<code>__final_insertion_sort</code> 就很好理解了</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token function">__final_insertion_sort</span><span class="token punctuation">(</span>_RandomAccessIterator __first<span class="token punctuation">,</span>            _RandomAccessIterator __last<span class="token punctuation">,</span> _Compare __comp<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>    <span class="token keyword">if</span> <span class="token punctuation">(</span>__last <span class="token operator">-</span> __first <span class="token operator">></span> <span class="token keyword">int</span><span class="token punctuation">(</span>_S_threshold<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">&#123;</span>        std<span class="token operator">::</span><span class="token function">__insertion_sort</span><span class="token punctuation">(</span>__first<span class="token punctuation">,</span> __first <span class="token operator">+</span> <span class="token keyword">int</span><span class="token punctuation">(</span>_S_threshold<span class="token punctuation">)</span><span class="token punctuation">,</span> __comp<span class="token punctuation">)</span><span class="token punctuation">;</span>        std<span class="token operator">::</span><span class="token function">__unguarded_insertion_sort</span><span class="token punctuation">(</span>__first <span class="token operator">+</span> <span class="token keyword">int</span><span class="token punctuation">(</span>_S_threshold<span class="token punctuation">)</span><span class="token punctuation">,</span> __last<span class="token punctuation">,</span>                        __comp<span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span>    <span class="token keyword">else</span>        std<span class="token operator">::</span><span class="token function">__insertion_sort</span><span class="token punctuation">(</span>__first<span class="token punctuation">,</span> __last<span class="token punctuation">,</span> __comp<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在区间 &gt; 16时，先在[0, 16)上调用了 <code>__insertion_sort</code>；因为[0, 16)中的数可能是完全乱序的，完全不具备调用<code>__unguarded_linear_insert</code> 的前提条件，所以只能用 <code>__insertion_sort</code>。</p><p>接着，在[16,  end) 处调用了 <code>__unguarded_insertion_sort</code>。由于经过了 <code>__introsort</code> 的数组是整体有序局部乱序的，即 16个一组的子数组内部是乱序的，但是不同子数组之间是有序的，即后一个子数组的最小值一定大于等于前一个子数组的最大值。这意味着[16,  end)中的所有数都 大于 [0, 16)中的任一个数。于是，[16, end)满足了 <code>__unguarded_insertion_sort</code> 的前提条件。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] Bentley, Jon L.; McIlroy, M. Douglas (1993). <a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.14.8162">“Engineering a sort function”</a></p><p>[2] <a href="https://blog.csdn.net/albertsh/article/details/119523587">https://blog.csdn.net/albertsh/article/details/119523587</a></p><p>[3] <a href="https://baobaobear.github.io/post/20191019-qsort-talk-3/">https://baobaobear.github.io/post/20191019-qsort-talk-3/</a></p><p>[4] 更详细的_sort解读 <a href="https://leetcode-cn.com/circle/discuss/wItoZk/">https://leetcode-cn.com/circle/discuss/wItoZk/</a></p><p>[5] 侯捷《STL源码剖析》</p>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图形学 </tag>
            
            <tag> C++ </tag>
            
            <tag> IEEE 754 </tag>
            
            <tag> 牛顿迭代法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[算法] 平方根倒数速算法中的魔数0x5f3759df的来源</title>
      <link href="2021/11/23/cpp/suan-fa-kuai-su-ping-fang-gen-dao-shu-suan-fa-zhong-de-0x5f3759df-de-lai-yuan/"/>
      <url>2021/11/23/cpp/suan-fa-kuai-su-ping-fang-gen-dao-shu-suan-fa-zhong-de-0x5f3759df-de-lai-yuan/</url>
      
        <content type="html"><![CDATA[<h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p><strong>平方根倒数速算法</strong> 即 Fast Inverse Square Root算法 [3]，是一个相当出名的算法，其具体作者已不可考，由于雷神之锤中大量使用了该种算法使得它为人所知。</p><p>为什么需要平方根倒数而不是平方根本身？因为图形学中大量运算需要用到平方根倒数，而不需要平方根，比如 向量的<code>normalize()</code>操作 $\frac{1}{\sqrt{x^2 + y^2 + z^2}}(x,y,z)$ 就包含平方根倒数；而图形学中每一帧的渲染都至少需要调用上万次normalize。如果能加速平方根倒数的计算，那么渲染的速度将会大大提升。</p><p>该算法诞生于20多年前，如今，该算法仅具有研究价值，但不具备使用价值[1]，CPU已经内置了速度更快，误差更小的指令；我们只需要使用 <code>1/sqrt(y)</code>。</p><p>不过，这不影响我们去理解这个算法的精妙之处，在短短<strong>4行C代码</strong>里面，涉及了 1) IEEE 754标准 2) C 语言的undefined behavior 3) 牛顿迭代法。</p><p>这里的重点是第二步，即如何推导出magic number <code>0x5f3759df</code>，更细节的部分可以参考 [2] [4]。</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><pre class="line-numbers language-c" data-language="c"><code class="language-c"><span class="token comment">//from wikipedia [3]</span><span class="token keyword">float</span> <span class="token function">Q_rsqrt</span><span class="token punctuation">(</span> <span class="token keyword">float</span> number <span class="token punctuation">)</span><span class="token punctuation">&#123;</span><span class="token keyword">long</span> i<span class="token punctuation">;</span><span class="token keyword">float</span> x2<span class="token punctuation">,</span> y<span class="token punctuation">;</span><span class="token keyword">const</span> <span class="token keyword">float</span> threehalfs <span class="token operator">=</span> <span class="token number">1.5F</span><span class="token punctuation">;</span>x2 <span class="token operator">=</span> number <span class="token operator">*</span> <span class="token number">0.5F</span><span class="token punctuation">;</span>y  <span class="token operator">=</span> number<span class="token punctuation">;</span>        <span class="token comment">/*核心代码*/</span>    <span class="token comment">// 1. reinterpret_cast from float to int</span>i  <span class="token operator">=</span> <span class="token operator">*</span> <span class="token punctuation">(</span> <span class="token keyword">long</span> <span class="token operator">*</span> <span class="token punctuation">)</span> <span class="token operator">&amp;</span>y<span class="token punctuation">;</span>                       <span class="token comment">// evil floating point bit level hacking（对浮点数的邪恶位元hack）</span>    <span class="token comment">// 2. 估算平方根倒数</span>i  <span class="token operator">=</span> <span class="token number">0x5f3759df</span> <span class="token operator">-</span> <span class="token punctuation">(</span> i <span class="token operator">>></span> <span class="token number">1</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>               <span class="token comment">// what the fuck?</span>    <span class="token comment">// reinterpret_cast from int to float</span>y  <span class="token operator">=</span> <span class="token operator">*</span> <span class="token punctuation">(</span> <span class="token keyword">float</span> <span class="token operator">*</span> <span class="token punctuation">)</span> <span class="token operator">&amp;</span>i<span class="token punctuation">;</span>  <span class="token comment">// 3. 牛顿法</span>y  <span class="token operator">=</span> y <span class="token operator">*</span> <span class="token punctuation">(</span> threehalfs <span class="token operator">-</span> <span class="token punctuation">(</span> x2 <span class="token operator">*</span> y <span class="token operator">*</span> y <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>   <span class="token comment">// 1st iteration （</span><span class="token comment">//      y  = y * ( threehalfs - ( x2 * y * y ) );   // 2nd iteration, this can be removed</span><span class="token keyword">return</span> y<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>算法大致分为3部分：</p><ul><li>reinterpret_cast 用于在float和int之间相互转换，否则任何一个编译器都会禁止对float型做位运算。</li><li>利用magic number <code>0x5f3759df</code>估算平方根导数（见下文）</li><li>一次牛顿迭代法（见下文）</li></ul><h2 id="估算平方根导数"><a href="#估算平方根导数" class="headerlink" title="估算平方根导数"></a>估算平方根导数</h2><p>IEEE 754标准中，32位浮点数包含1个符号位，8位指数 $E$，23位尾数 $M$，任一浮点数可以被表示为：$(1.M) \times 2^{E - 127} = (1 + \frac{M}{2^{23}}) \times 2^{E - 127}, M \in[0, 2^{23} ), E \in [0,256)$  .</p><p><strong>目标</strong>：需要计算 $y$ 的平方根倒数 $\frac{1}{\sqrt{y}} = y^{-\frac{1}{2}}$， $y$ 的 IEEE 754形式为 $y = (1 + \frac{M_y}{2^{23}}) * 2^{E_y - 127}$.</p><p>出于数学上的直觉，我们将 $y$ 取以2为底的对数：</p><script type="math/tex; mode=display">\begin{align}\log y &= \log{((1 + M_y) * 2^{E_y - 127})} \\&= \log{(1+\frac{M_y}{2^{23}})} + E_y - 127\end{align}</script><p>$\frac{M_y}{2^{23}}$ 显然是在 $[0,1]$ 这个区间上，而在这个区间中，有$\log(1+x) \approx x$；</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes/image-20211120210324606.png" alt="image-20211120210324606"></p><p>如果再加上某个常数 $\mu$，那么这个给近似效果会更好一些：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes/image-20211120210608035.png" alt="image-20211120210608035"></p><p>使用这个近似，可以进一步简化：</p><script type="math/tex; mode=display">\begin{align}\log y&= \log{(1+\frac{M_y}{2^{23}})} + E_y - 127 \\&\approx \frac{M_y}{2^{23}} + \mu + E_y - 127 \\&= \frac{1}{2^{23}}(M_y + E_y * 2^{23}) + \mu - 127\end{align}</script><p><strong>而 $M_y + E_y * 2^{23}$ 正是 float型 y 被转型为 int 型后的值</strong>。</p><p>现在对 $y^{-\frac{1}{2}}$ 同样取对数：</p><script type="math/tex; mode=display">\log y^{-\frac{1}{2}} = -\frac{1}{2} \log y \approx -\frac{1}{2}\left( \frac{1}{2^{23}}(M_y + E_y * 2^{23}) + \mu - 127 \right)</script><p>假设  $y^{-\frac{1}{2}}$ 的准确值为 $A$，其指数和尾数部分分别为 $E_A$ 和 $M_A$，同样对 $A$ 取对数：</p><script type="math/tex; mode=display">\log A \approx \frac{1}{2^{23}}(M_A + E_A * 2^{23}) + \mu - 127</script><p><strong>则以上两式用不同的方式表达了 $\log A $ 即 $\log y^{-\frac{1}{2}}$ ，故联立以上两式可解得 $M_A + E_A * 2^{23}$</strong>：</p><script type="math/tex; mode=display">\begin{align}\log A &= \log y^{-\frac{1}{2}}\\-\frac{1}{2}\left( \frac{1}{2^{23}}(M_y + E_y * 2^{23}) + \mu - 127 \right) &= \frac{1}{2^{23}}(M_A + E_A * 2^{23}) + \mu - 127 \\\end{align}</script><p>略去中间过程，可得：</p><script type="math/tex; mode=display">\begin{align}M_A + E_A * 2^{23} &= 2^{23} * \frac{3}{2}(127 - \mu) - \frac{1}{2}(M_y + E_y * 2^{23}) \\\end{align}</script><p>第一项中的 $\mu$ 取 $ 0.0450461875791687011756$ 时[3]，第一项的结果为 $1597463011$，16进制表示为 <code>0x5F3759e3</code>；如果将第二项的 $\frac{1}{2}$ 变为移位运算，则上式和代码中的 这一行无比相近</p><pre class="line-numbers language-c" data-language="c"><code class="language-c">i  <span class="token operator">=</span> <span class="token number">0x5f3759df</span> <span class="token operator">-</span> <span class="token punctuation">(</span> i <span class="token operator">>></span> <span class="token number">1</span> <span class="token punctuation">)</span><span class="token punctuation">;</span>  <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>只是magic number差了一点点，如果 $\mu$ 的值恰当，应该可以得到完全一样的数字。</p><h2 id="牛顿迭代法"><a href="#牛顿迭代法" class="headerlink" title="牛顿迭代法"></a>牛顿迭代法</h2><p>牛顿迭代法的更新公式为：</p><script type="math/tex; mode=display">x_{t+1} = x_t - f(x_t) / f^\prime(x_t)</script><p>牛顿迭代法的前提条件：要求初始点在函数零点附近，因为<code>i  = 0x5f3759df - ( i &gt;&gt; 1 );</code>所估算的结果误差应该不大，可以认为前提条件已经满足。</p><p>此时，我们的问题为，已知 $y$， 需要求 $x$ 使得 $\frac{1}{\sqrt{y}} = x$，即 $\frac{1}{x^2} = y$，移项得$f(x) = \frac{1}{x^2} - y = 0$ ，对 $f(x)$ 应用牛顿迭代可得迭代公式为：</p><script type="math/tex; mode=display">\begin{align}x_{t+1} &= x_t - f(x_t) / f^\prime(x_t) \\x_{t+1} &= \frac{3}{2}x_t - \frac{y}{2}x_t^3\end{align}</script><p>翻译成代码就是：</p><pre class="line-numbers language-c" data-language="c"><code class="language-c">y  <span class="token operator">=</span> y <span class="token operator">*</span> <span class="token punctuation">(</span> threehalfs <span class="token operator">-</span> <span class="token punctuation">(</span> x2 <span class="token operator">*</span> y <span class="token operator">*</span> y <span class="token punctuation">)</span> <span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] Is fast inverse square root still used? <a href="https://www.quora.com/Is-fast-inverse-square-root-still-used">https://www.quora.com/Is-fast-inverse-square-root-still-used</a></p><p>[2] Fast Inverse Square Root — A Quake III Algorithm <a href="https://www.youtube.com/watch?v=p8u_k2LIZyo">https://www.youtube.com/watch?v=p8u_k2LIZyo</a></p><p>[3] wiki-平方根倒数速算法 <a href="https://zh.wikipedia.org/zh-hans/%E5%B9%B3%E6%96%B9%E6%A0%B9%E5%80%92%E6%95%B0%E9%80%9F%E7%AE%97%E6%B3%95">https://zh.wikipedia.org/zh-hans/%E5%B9%B3%E6%96%B9%E6%A0%B9%E5%80%92%E6%95%B0%E9%80%9F%E7%AE%97%E6%B3%95</a></p><p>[4] 论文 <a href="https://www.lomont.org/papers/2003/InvSqrt.pdf">https://www.lomont.org/papers/2003/InvSqrt.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图形学 </tag>
            
            <tag> C++ </tag>
            
            <tag> IEEE 754 </tag>
            
            <tag> 牛顿迭代法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实时光线追踪 联合双边滤波 单帧降噪</title>
      <link href="2021/08/24/cg/rtrt-dan-zheng-jiang-zao/"/>
      <url>2021/08/24/cg/rtrt-dan-zheng-jiang-zao/</url>
      
        <content type="html"><![CDATA[<p>Summary:</p><ul><li>为什么实时光线追踪（RTRT）需要降噪</li><li>用联合双边滤波对RTRT的结果进行降噪（附伪代码和结果）</li><li>Deferred Hybrid Ray Tracing 和 naive Path Tracing之间的区别</li><li>论文Edge-Avoiding À-TrousWavelet Transform for fast Global Illumination Filtering 的</li></ul><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210824192654885.png" alt="Deferred Hybrid Real-Time Ray-Tracing Overview, 图源 [4]"></p><h2 id="为什么需要降噪？"><a href="#为什么需要降噪？" class="headerlink" title="为什么需要降噪？"></a>为什么需要降噪？</h2><p>光线追踪的效果非常真实：</p><p><img src="https://pic.vibaike.com/img/2020/08/bt-poj.jpg" alt="光线追踪一个例子"></p><p><img src="https://mlhr8q6s8c91.i.optimole.com/VVLFqVU-3ep7C8ue/w:899/h:844/q:90/https://www.imaginationtech.com/wp-content/uploads/2017/01/hard-shadows-ray-tracing-raster.gif" alt="光追生成软阴影"></p><p>比如，渲染Cornell Box，不需要复杂的技术，只用光线追踪就可以实现软阴影、color bleeding、全局光照等效果。下图是我之前实现的<a href="https://github.com/LamForest/GAMES101-Computer-Graphics-Assignment/blob/main/Assignment7-Windows/Assignment7/Renderer.cpp#L70">Path Tracing算法</a>：</p><p><img src="https://user-images.githubusercontent.com/17798738/113547117-e0869100-961f-11eb-8ebd-ef2b4d7d8545.png" alt="img"></p><p>然而，对于这个比较小的256x256的场景，在CPU下 SPP = 128时，一帧画面需要多达40s时间渲染：</p><p><img src="https://user-images.githubusercontent.com/17798738/113546781-3e66a900-961f-11eb-84ce-37255cf09e88.png" alt="image"></p><p>就算是用GPU进行优化，也达不到实时(30fps)的要求。</p><hr><p>于是，实时光追采取的方案是：用较小的SPP（比如1SPP）迅速渲染出每一帧的带有大量噪点画面，然后利用单帧降噪（联合双边滤波）和多帧降噪（Temporal Accumulation）提升画面质量：</p><p><img src="https://gitee.com/getleft/pics/raw/master/cs229/image-20210529102937056.png" alt="image-20210529102937056"></p><p>这套Denoising方案可以得到比较不错的结果：</p><p><img src="https://gitee.com/getleft/pics/raw/master/cs229/image-20210529103156895.png" alt="image-20210529103156895"></p><h2 id="联合双边滤波"><a href="#联合双边滤波" class="headerlink" title="联合双边滤波"></a>联合双边滤波</h2><p>说到降噪，能想到最简单的方案之一就是高斯滤波，然而高斯滤波只是单纯的加权平均一块区域的像素值，在降噪的同时，也模糊了不同物体之间的边界：</p><p><img src="https://gitee.com/getleft/pics/raw/master/cs229/image-20210605102156200.png" alt="高斯模糊"></p><p>于是就有了<a href="https://zh.wikipedia.org/wiki/%E9%9B%99%E9%82%8A%E6%BF%BE%E6%B3%A2%E5%99%A8">双边滤波</a>，不仅考虑两个像素之间的距离，还考虑了两个像素之间的颜色差。双边滤波假设：如果两个颜色之间的颜色差别较大，那他们就属于两个物体，那么，在滤波时不应该对彼此有贡献：</p><p><img src="https://gitee.com/getleft/pics/raw/master/cs229/image-20210605102708026.png" alt="双边滤波"></p><p>然而，对于上面的图片，降噪结果貌似还可以；但是对于1SPP生成的实时光追图片，双边滤波的效果就不行了，因为噪点太多了，双边滤波的假设并不成立。</p><hr><p>观察高斯模糊和双边滤波，究其本质，是在用不同的度量（距离，颜色）来计算两个像素之间的相似性。但是由于实时光追的图片中大量噪声的存在，<strong>颜色这个度量本身是有噪声的</strong>，所以颜色并不是一个好的度量。</p><p>那么有没有其他不带噪声的度量呢？有的，比如<strong>G-buffer</strong>中的Normal, Position(世界坐标系下的位置), Depth等等:</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210824172516933.png" alt="场景"></p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210824172108827.png" alt="Normal"></p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210824172154024.png" alt="Position"></p><p><img src="https://gitee.com/getleft/pics/raw/master/cs229/image-20210605104116531.png" alt="image-20210605104116531"></p><p>假设我们的联合双边滤波考虑了像素之间的距离 $|\mathbf{i}-\mathbf{j}|^{2}$，像素之间颜色的差别 $|\widetilde{C}[\mathrm{i}]-\widetilde{C}[\mathrm{j}]|^{2}$ 和 法线方向的差别:</p><script type="math/tex; mode=display">D_{\text {normal }}(\mathbf{i}, \mathbf{j})=\arccos (\text { Normal }[\mathbf{i}] \cdot \text { Normal }[\mathbf{j}])</script><p>那最终 $i,j$ 像素彼此之间的贡献值就是：</p><script type="math/tex; mode=display">J(\mathbf{i}, \mathbf{j})=\exp \left(-\frac{\|\mathbf{i}-\mathbf{j}\|^{2}}{2 \sigma_{p}^{2}}-\frac{\|\widetilde{C}[\mathrm{i}]-\widetilde{C}[\mathrm{j}]\|^{2}}{2 \sigma_{c}^{2}}-\frac{D_{\text {normal }}(\mathbf{i}, \mathbf{j})^{2}}{2 \sigma_{n}^{2}}\right)</script><p>其中 $\widetilde{C}$ 是光线追踪的带有噪声的结果，$\sigma_*$是超参。联合双边滤波的伪代码如下：</p><pre class="line-numbers language-none"><code class="language-none">def JBF(frame, normal): &#x2F;&#x2F;frame 原始噪声图像，normal G-buffer中的法线filtered &#x3D; new [][]; &#x2F;&#x2F;和 frame大小一样的数组for each pixel (x,y):sum_of_weighted_values &#x3D; 0sum_of_weights &#x3D; 0for adjacent pixel (i,j):weight &#x3D; compute_J((x,y), (i,j), normal, frame) &#x2F;&#x2F;按照上式计算的Jsum_of_weights +&#x3D; weightsum_of_weighted_values +&#x3D; weight * frame(i,j)filtered[x][y] &#x3D; sum_of_weighted_values &#x2F; sum_of_weights;return filtered;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>原始噪声图像：</p><p><img src="/images/cg/box-input.gif" alt="box-input"></p><p>联合双边降噪结果：</p><p><img src="/images/cg/box-filter.gif" alt="box-filter"></p><h2 id="更多降噪技术-A-Trous-Wavelet"><a href="#更多降噪技术-A-Trous-Wavelet" class="headerlink" title="更多降噪技术 A-Trous Wavelet"></a>更多降噪技术 A-Trous Wavelet</h2><p>在联合双边滤波的基础上，有很多更快更先进的滤波算法。这里对 Edge-Avoiding À-TrousWavelet Transform for fast Global Illumination Filtering进行了实现，这篇文章结合了A-trous算法和联合双边滤波，通过多趟较小的滤波器模拟大滤波核的卷积操作。效果如下，在我的电脑上速度达到了联合双边滤波的8-10倍。</p><p><img src="/images/cg/box-atrous.gif" alt="box-atrous"></p><h2 id="补充知识-Deferred-Hybrid-Ray-Tracing"><a href="#补充知识-Deferred-Hybrid-Ray-Tracing" class="headerlink" title="补充知识: Deferred Hybrid Ray Tracing"></a>补充知识: Deferred Hybrid Ray Tracing</h2><h3 id="Deferred-Shading"><a href="#Deferred-Shading" class="headerlink" title="Deferred Shading"></a>Deferred Shading</h3><p>联合双边滤波需要G-buffer中的信息，G-buffer是Deferred Shading中的一个概念。</p><p>一般而言，最简单的Shading方式是Forward Shading，它的伪代码可以这么写：</p><pre class="line-numbers language-none"><code class="language-none">1. Vertex Shading2. Rasterizationfor each triangle:for each pixel in triangle:if pass depth test:3. fragment shadingcolor &#x3D; fragment_shader(...); &#x2F;&#x2F; fragment_shader需要做 1)片元属性插值 2)Lighting着色frame_buffer[pixid] &#x3D; color;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Forward Shading有很明显的缺点：太慢了，调用了太多次fragment_shader，其中大部分调用是不必要的，因为会被后续更靠前的fragment覆盖，一个最极端的例子就是从场景最后方的三角形开始往前渲染。</p><p>为了解决这个问题，Deferred Shading出现了，它的伪代码可以这么表示</p><pre class="line-numbers language-none"><code class="language-none">1. Vertex Shading2. Rasterizationfor each triangle:for each pixel in triangle:if pass depth test:3. write to G-buffernormal, position, albedo, ... &#x3D; fragment_geometry_shader(...);g_buffer_normal[pixid] &#x3D; normal;g_buffer_position[pixid] &#x3D; position;g_buffer_albedo[pixid] &#x3D; albedo;3. Deferred Shadingfor each pixel:frame_buffer[pixid] &#x3D; fragment_lighting_shader(g_buffer_normal[pixid],g_buffer_position[pixid],g_buffer_albedo[pixid],            ...       );<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>其核心思想是将昂贵的光照计算移到后面，使得 光照计算的次数 与 场景中的物体个数无关，只与像素个数有关。</p><p><img src="http://learnopengl.com/img/advanced-lighting/deferred_overview.png" alt="deferred_overview"></p><p>更多Deferred Shading的知识可以参考 [1] [2] [3]。</p><h3 id="Hybrid-Rendering"><a href="#Hybrid-Rendering" class="headerlink" title="Hybrid Rendering"></a>Hybrid Rendering</h3><p><a href="https://github.com/LamForest/GAMES101-Computer-Graphics-Assignment/blob/3fb956f2d16a3546e0b5bc23c4aeb2dd55d6ce72/Assignment7-Windows/Assignment7/Renderer.cpp#L70">Path Tracing算法</a>是一种纯粹的蒙塔卡罗光线追踪算法，和光栅化可以说是没有任何关系；但由上文可知，联合双边滤波降噪所需的G-Buffer是光栅化的产物，光线追踪中为什么又有光栅化呢？</p><p>现在，为了尽量加快速度，工业界常用的算法并不是纯粹的光线追踪算法，而是一种混合了光栅化和光线追踪的算法，Hybrid Rendering。</p><p><img src="https://gitee.com/getleft/pics/raw/master/cs229/image-20210529102754498.png" alt="image-20210529102754498"></p><p>Hybrid Rendering 会先进行一遍光栅化得到G-buffer，再从G-buffer的每个像素出发，进行光线追踪，整个过程如下图所示：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210824192654885.png" alt="图源 [4]"></p><h2 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h2><p>这篇文章部分内容来源于<a href="https://www.bilibili.com/video/BV1YK4y1T7yY?p=13">GAMES202 闫令琪</a> ，文中的部分截图来自于课中使用的幻灯片，非常感谢闫老师深入浅出的课程以及实现框架的助教们。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ol><li><a href="https://learnopengl-cn.readthedocs.io/zh/latest/05%20Advanced%20Lighting/08%20Deferred%20Shading/">learn opengl 延迟着色法</a></li><li><a href="https://www.cnblogs.com/polobymulberry/p/5126892.html">【原】实时渲染中常用的几种Rendering Path</a></li><li><a href="https://zhuanlan.zhihu.com/p/54694743">游戏引擎中的光照算法</a></li><li><a href="https://link.springer.com/content/pdf/10.1007%2F978-1-4842-4427-2_26.pdf">Deferred Hybrid Path Tracing, RayTracing Gem</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 图形学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图形学 </tag>
            
            <tag> Real Time Rendering </tag>
            
            <tag> Ray-Tracing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习目录</title>
      <link href="2021/08/16/ml/qiang-hua-xue-xi-mu-lu/"/>
      <url>2021/08/16/ml/qiang-hua-xue-xi-mu-lu/</url>
      
        <content type="html"><![CDATA[<h3 id="RL-An-Introduction-second-edition-by-Sutton-and-Barto-读书笔记"><a href="#RL-An-Introduction-second-edition-by-Sutton-and-Barto-读书笔记" class="headerlink" title="RL: An Introduction, second edition, by Sutton and Barto 读书笔记"></a>RL: An Introduction, second edition, by Sutton and Barto 读书笔记</h3><h3 id="论文笔记"><a href="#论文笔记" class="headerlink" title="论文笔记"></a>论文笔记</h3><ol><li><a href="https://lamforest.github.io/2021/08/10/ml/qiang-hua-xue-xi/paper/alphago/">[Nature 2016] AlphaGo: Deep RL 与 Tree Search 的成功结合</a></li><li><a href="https://lamforest.github.io/2021/08/16/ml/qiang-hua-xue-xi/paper/alphago-zero/">[Nature 2018] AlphaGo Zero: 无需监督学习的AlphaGo</a></li><li><a href="https://lamforest.github.io/2021/07/21/ml/qiang-hua-xue-xi/paper/dqn/">[Nature 2015] DQN论文笔记 及 实现</a></li><li><a href="https://lamforest.github.io/2021/07/28/ml/qiang-hua-xue-xi/paper/a3c/">[ICML 2016] A3C</a></li><li><a href="https://lamforest.github.io/2021/07/29/ml/qiang-hua-xue-xi/paper/prior-replay/">[ICLR 2016] Prioritized Experience Replay</a></li><li><a href="https://lamforest.github.io/2021/07/31/ml/qiang-hua-xue-xi/paper/double-dqn/">[AAAI 2016] Double DQN</a></li><li><a href="https://lamforest.github.io/2021/08/01/ml/qiang-hua-xue-xi/paper/dueling-dqn/">[ICML 2016] Dueling DQN</a></li><li><a href="https://lamforest.github.io/2021/08/02/ml/qiang-hua-xue-xi/paper/distributional-dqn/">[ICML 2017] Distributional RL</a></li><li><a href="https://lamforest.github.io/2021/08/05/ml/qiang-hua-xue-xi/paper/rainbow/">[AAAI 2018] Rainbow</a></li></ol><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><ol><li><a href="https://lamforest.github.io/2021/07/20/ml/qiang-hua-xue-xi/meng-te-qia-luo-sou-suo-shu/">[python] 在 tic-tac-toe 上实现 蒙特卡洛搜索树 MCTS 算法</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Paper][RL][Nature 2018] AlphaGo Zero: 无需监督学习的AlphaGo</title>
      <link href="2021/08/16/ml/qiang-hua-xue-xi/paper/alphago-zero/"/>
      <url>2021/08/16/ml/qiang-hua-xue-xi/paper/alphago-zero/</url>
      
        <content type="html"><![CDATA[<p>论文地址：<a href="https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ">不可下载</a>，<a href="https://discovery.ucl.ac.uk/id/eprint/10045895/1/agz_unformatted_nature.pdf">可下载，草稿</a></p><p>辨析：AlphaGo有好几个版本，按照时间顺序：AlphaGo Fan（即AlphaGo paper），AlphaGo Lee，AlphaGo Master，AlphaGo Zero（下文中有时会称之为Zero, this paper），AlphaZero（后续工作，Science Paper）。他们之间的区别参见 这篇论文的附录。</p><h2 id="引子-amp-总结"><a href="#引子-amp-总结" class="headerlink" title="引子 &amp; 总结"></a>引子 &amp; 总结</h2><p>AlphaGo 确实超越了顶尖人类棋手（AlphaGo Lee 4:1打败了李世石），但是，AlphaGo无论是在成绩上还是算法上，还有很大的提升空间。</p><ul><li><p>从成绩角度来谈，成熟的围棋AI需要对人类有绝对100%的胜率，而不是负于李世石一盘。</p></li><li><p>至于AlphaGo的算法，从我的角度来看，最不自然的一点，就是train test的流程 不一致。按照深度监督学习的观点，如果想模型在测试集上的表现尽可能接近训练集上的表现，需要保证测试的流程和训练一致，否则会严重损害模型性能。然而，在AlphaGo的训练阶段，没有MCTS的参与。</p><p>此外，现在深度学习追求的是end2end，而AlphaGo的training pipeline显得不够elegant。</p></li><li><p>最后是这篇文章所强调的核心，Mastering without Human Knowledge。用DL的话讲，就是不需要监督学习预训练和围棋专家数据集，只用强化学习的技术。</p></li></ul><p>于是 AlphaGo Zero的算法相比于AlphaGo有了以下提升：</p><ol><li>保证了train test 的一致性，都包含了APV-MCTS</li><li>抛弃了pipeline，用一个网络代替了所有网络</li><li>移除了MCTS中的rollout阶段，完全由 value network决定。</li><li>用 MCTS 提升 policy network 输出的action probability，并作为 label，训练policy network。 </li></ol><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>Zero 的 训练方式可以看作是 off-policy的，一边在与环境交互（self-play）并把样本存入一个replay memory中；一边在做神经网络的训梯度下降（optimization)。</p><p>所以，算法由2部分构成：self-play, optimize(sgd)。还有一个相对不太重要的部分为 Evaluation。三个部分之间是并行执行的。</p><h3 id="1-Self-play-with-MCTS"><a href="#1-Self-play-with-MCTS" class="headerlink" title="1. Self-play with MCTS"></a>1. Self-play with MCTS</h3><p>Self-play的目的是在为 optimize阶段生成训练数据。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210812175049426.png" alt="image-20210812175049426"></p><p><strong>和AlphaGo APV-MCTS的区别：</strong> 如果我对附录中的search algorithm理解没错的话，这部分和AlphaGo基本没有区别，除了省略了rollout（下式中 $\lambda = 0$ ，只根据value network 的输出来确定叶子结点的value）以及expand的方式。</p><script type="math/tex; mode=display">V\left(s_{L}\right)=(1-\lambda) v_{\theta}\left(s_{L}\right)+\lambda z_{L}</script><p><strong>具体的Self-play流程</strong>：</p><ul><li>在每个state $s_t$ 处 执行MCTS，MCTS的每次simulation重复以下步骤：</li></ul><ol><li><p>selection: （与AlphaGo中完全一样），用下式选出best edge/action：</p><script type="math/tex; mode=display">a_{}=\underset{a}{\operatorname{argmax}}\left(Q\left(s_{t}, a\right)+u\left(s_{t}, a\right)\right)</script></li><li><p>expand: （与AlphaGo中不同，总是expand）重复selection直到叶子节点，扩张。利用 current best network计算出 $P(s,a)$ 作为edge 的先验概率。</p></li><li><p>rollout: 无。</p></li><li><p>backup: （与AlphaGo中完全一样）利用 current best network计算出 叶子节点的 value  $V(s^{\prime})$，对从root到叶子节点上的这条path上的所有结点更新 $Q, N$。</p></li></ol><ul><li>MCTS结束之后，在root处，获取每个edge的 $N(root,a)$，并计算该action 的 概率 $\pi_a$:</li></ul><script type="math/tex; mode=display">\pi_{a} \propto N(s, a)^{1 / \tau}</script><p>所有action 的概率组成了一个向量 $\pi_t$ ，则当前这个state的transition为 ：$(s_t,  \pi_t , z_t)$。其中 $z_t$ 的值只有当这一盘围棋结束后才得知。</p><ul><li>当前state的动作 $a_t$ 采样于 $\pi_t$，和 (stocastic) Policy Gradient 中action 的获取方式一致：</li></ul><script type="math/tex; mode=display">a_{1} \sim \pi_{1}</script><ul><li>当这一盘围棋结束之后（黑或白胜），得到 reward $z = \pm1$，则这一盘围棋的所有transition 的 $z_t = z$，并将所有transition $(s_t,  \pi_t , z_t), t = 0,1,2,….$ 全放入<strong>Replay memory</strong>中。Memory 大小为 500,000，淘汰最旧的。</li></ul><h3 id="2-Optimize-with-replay-memory"><a href="#2-Optimize-with-replay-memory" class="headerlink" title="2. Optimize with replay memory"></a>2. Optimize with replay memory</h3><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210812175529187.png" alt="image-20210812175529187"></p><p>这部分比较简单：</p><ol><li><p>sample uniformly from replay memory.</p></li><li><p>X = $s_t$ 。对于 policy 分支，Y = $\pi_t$ , 损失函数为cross entropy，对于 value 分支，$Y = z_t$ ，损失函数为 平方误差函数，再加上 L2 norm：</p><script type="math/tex; mode=display">l=(z-v)^{2}-\boldsymbol{\pi}^{\mathrm{T}} \log \boldsymbol{p}+c\|\theta\|^{2}</script><p><em>注：一般如果有两个branch，它们之间的loss会由一个超参进行调节，比如：</em></p><script type="math/tex; mode=display">l=\lambda(z-v)^{2}-\boldsymbol{\pi}^{\mathrm{T}} \log \boldsymbol{p}+c\|\theta\|^{2}</script><p><em>在这篇论文附录里提到了，$\lambda = 1$ 即 两个loss权重一样。</em></p></li><li><p>利用mini-bach SGD训练网络。</p></li></ol><h3 id="3-Evaluation"><a href="#3-Evaluation" class="headerlink" title="3. Evaluation"></a>3. Evaluation</h3><p>每1000次 mini-BSGD之后，会存一个checkpoint a 。由于网络训练有波动，需要确保用最好的网络self-play生成训练数据。于是，作者让 a 和 current best network进行400局对弈，如果a胜出55%，则current best network = a。</p><p>同时，Evalutaion可以一直跟踪checkpoint的表现，保证网络不会diverge。</p><blockquote><p>不过，在后续工作AlphaZero中，作者移除了400局对弈这一步，直接current best network = a。</p></blockquote><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>从AlphaGo中的4个网络，简化成了1个网络，输入为棋局（以及过去几步的棋局，类似于DQN的输入有4帧），共享feature extractor，末端有两个分支，分别为policy branch：输出#actions 的概率；value branch： 棋局的value $V$ 。</p><p>相比于AlphaGo中的网络，这篇论文的网络要深一些，并且与时俱进，引入了BN和残差网络。实验部分为了消除网络带来的影响，做了相关的实验，有兴趣的可以看看，这里省略了。</p><p>输入相比于AlphaGo减少了一些，为19x19x17。17 = 8（8 most recent position，类似DQN的输入为4帧) + 8（8 most recent position of opponents）+ 1 （which player）</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="1-Evaluation-VS-AlphaGo"><a href="#1-Evaluation-VS-AlphaGo" class="headerlink" title="1. Evaluation VS AlphaGo"></a>1. Evaluation VS AlphaGo</h3><p>首先，最重要的就是AlphaGo Zero的效果，在围棋里面就是Elo rating。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210812181421233.png" alt="image-20210812181421233"></p><ol><li>图中可以看出AlphaGo Zero的Elo大幅超过了 AlphaGo Lee，这可以说明，AlphaGo Zero确实是比AlphaGo 更好的算法。此外，它们对战时，AlphaGo Zero以100：0的绝对优势胜出。</li><li>尽管人类棋手数据训练出的 SL network在预测expert move 上更准确，但是Zero 却拥有更高的胜率。这意味着Zero找到了比人类棋手更优的解法。</li><li>Elo 4000+已经远超人类棋手了，目前世界第一的Elo才为 3600分。</li></ol><h3 id="2-不同时期学到的不同Knowledge"><a href="#2-不同时期学到的不同Knowledge" class="headerlink" title="2. 不同时期学到的不同Knowledge"></a>2. 不同时期学到的不同Knowledge</h3><p>Zero训练的总时间为72h，作者展示了在不同训练阶段中Zero进行自我对弈self-play时，所生成的棋局。由于图比较大，这里就不放了。</p><p>总的来说，在训练早期，Zero 的策略主要是贪心的，想要尽快围住对面，吃掉对面的棋子；在训练中期，Zero的眼光开始放的比较远，不再纠结于一棋一子的得失。在训练后期，Zero的大局观逐渐形成，打法多变，远超普通人类所能达到的水平。</p><h3 id="3-Final-Performance"><a href="#3-Final-Performance" class="headerlink" title="3. Final Performance"></a>3. Final Performance</h3><p>作者最后训练了一个终极版AlphaGo Zero，从29 million盘棋中学习了40天，结果如下：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210812200455582.png" alt="image-20210812200455582"></p><p>有几点值得注意的：</p><ol><li>纯端到端 DL 的方法 （raw network）与 DL + MCTS 的方法差距甚远。个人认为，这是因为围棋的可能的position数目过于庞大，不足以生成足够大量的样本让网络充分学习。然而，如果引入MCTS方法，那对于神经网络的要求不需要太高；在MCTS中，会从root向树的深处探索，而处于树深处的state，无论是policy还是value ，相比于root处，网络都会预测的都更精确一些，这使得最终MCTS给出的的 $\pi_t$ 显著优于只使用policy network得出的 action probability。所以，作者称MCTS为 policy improvement operator。</li><li>ELO 达到 5000，训练时间40天。。除了Google估计其他企业以后也不太会继续在围棋上做研究了，除非硬件或深度学习理论层面有革新。</li></ol><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] AlphaGo</p><p>[2] Policy Gradient Therom, NIPS 1999</p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> MCTS </tag>
            
            <tag> Model-Based RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Paper][RL][Nature 2016] AlphaGo: Deep RL 与 Tree Search 的成功结合</title>
      <link href="2021/08/10/ml/qiang-hua-xue-xi/paper/alphago/"/>
      <url>2021/08/10/ml/qiang-hua-xue-xi/paper/alphago/</url>
      
        <content type="html"><![CDATA[<p>论文：Mastering the game of Go with deep neural networks and tree search, AlphaGo</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>AlphaGo 结合了 Deep RL, Deep SL 与 Tree Search 。AlphaGo 的 训练方式并不是end2end的，但是取得了非常优秀的结果，将围棋算法的水平从 业余 直接提升到了职业5段。</p><p>AlphaGo 的 贡献可以总结为2个部分：</p><ul><li>APV(Asynchronous policy and value)-MCTS：AlphaGo 算法的雏形来自于MCTS算法，蒙特卡洛树搜索。然而直接将MCTS应用在围棋上是非常困难的，为此，作者设计了几个网络来魔改MCTS，称之为 APV-MCTS：</li></ul><ol><li>rollout policy nework: vanilla MCTS 的 rollout policy 是 完全随机的，在围棋这种巨大搜索空间下，完全随机的policy 得到的 estimated value显然是非常不准确的（<em>见算法 4 Value network小节的图</em>）；但是太过复杂的rollout policy又会增加时间成本，减少rollout的次数。综合以上两点，rollout policy的网络结构为线性网络。</li><li>SL policy network: 在MCTS扩张的过程中，不能盲目的扩张，应该向着比较promising的方向扩张，这样才可以尽可能深的探索。为此，作者设计了SL policy network，给出了MCTS每条边的先验概率。</li><li>value network: 局势评估函数。由于搜索空间过大，有限时间内，仅凭rollout不能得出准确的value。为此，作者设计了 value network。综合 value network的预测和rollout的结果，才得到最终的value用于backup。</li></ol><p>APV-MCTS仍然是在MCTS的框架下，分为4步，selection，expand，rollout，backup。</p><ul><li>AlphaGo Pipeline: 作者结合了监督学习和强化学习，分为多个步骤训练value network。</li></ul><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>不考虑强化学习，想想看如何利用监督学习实现围棋算法 [2]：</p><blockquote><p>直接给围棋 AI 看人类围棋大师的棋谱，让它预测人类大师的下棋位置。很显然，这就是一个分类问题：输入当前棋局状态，输出落子位置。2015年之前最强的基于监督学习的围棋 AI 也只能达到 35% 的分类准确率，根本没法跟人类顶尖棋手过招。</p></blockquote><p>强化学习应用在围棋上的问题：</p><ul><li>状态太多，search space太大，但不是major concern，在 9x9的小围棋盘上就很难了。</li></ul><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210721004835680.png" alt="image-20210721004835680"></p><ul><li><p>没有一个好的评估局势的方法。象棋则很好评估局势，棋子比对方多则分数高，强力棋子的权重更大，Artificial Intelligence A Modern Approach中minmax那一章详细介绍了国际象棋的评估函数。但是，围棋，很难用一个简单的方法判断局势的好坏。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210721005533762.png" alt="image-20210721005533762"></p></li></ul><p>MCTS的引入大幅提升了围棋的强化学习成绩。因为MCTS不需要局势评估函数，MCTS会用rollout policy（或叫default policy）一直走到棋局结束，并不会在中途终止。当然由于rollout policy是随机的，围棋的搜索空间又很大，可以想象MCTS的效果也是相当有限的，在AlphaGo之前，MCTS方法在Go上的最好水平相当于weak amateur player[1]。</p><h2 id="算法-APV-MCTS"><a href="#算法-APV-MCTS" class="headerlink" title="算法 APV-MCTS"></a>算法 APV-MCTS</h2><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210810184947087.png" alt="AlphaGo Pipeline"></p><h3 id="0-为什么使用CNN"><a href="#0-为什么使用CNN" class="headerlink" title="0. 为什么使用CNN"></a>0. 为什么使用CNN</h3><p>SL, RL, Value Network 的结构几乎相同，都是以 CNN 为骨架。在Atari这种视频游戏上使用 CNN是很自然的，那么围棋使用CNN的理由在哪里呢？AlphaGo这篇论文中没有解释，AlphaZero[4]中提到了，是因为围棋的规则和CNN的特点匹配：</p><blockquote><p>AlphaGo Zero (and AlphaGo) used a convolutional neural network architecture that is particularly well-suited to Go: the rules of the game are translationally invariant (matching the weight sharing structure of convolutional networks) and are defined in terms of liberties corresponding to the adjacencies between points on the board (matching the local structure of convolutional net-works).</p></blockquote><h3 id="1-SL-policy-network"><a href="#1-SL-policy-network" class="headerlink" title="1. SL policy network"></a>1. SL policy network</h3><p>（目的在于模拟人类棋手）</p><p>SL 意为 Supervised Learning，所以是 SL Policy Network $p_{\sigma}(a \mid s)$ 的训练完全是监督学习的分类问题。</p><p>网络结构：13 conv，输出层为softmax，#class =19x19，代表棋盘上每一个落子点。</p><p>既然是监督学习：</p><ul><li>X = state, 维度 为 19 x 19 x 48，即每个落子处用48维特征表示（48维每一维的作用见论文附录表20）</li><li>Y = action，19x19维 one-hot vector。</li><li>SL network在测试集上的分类准确率是 57.0%。考虑到这是个 19x19 分类问题；且 state space太大，很可能遇见训练时未见过的情况，这个准确率相当不错了。</li></ul><p>值得注意的是，每个落子处有48个features，尽管这仅仅对SL Policy network 的分类准确率只有1.3% 的提升，但是 这 1.3%的提升可以使得最终的AlphaGo的win rate有比较大的提升。（见下图）</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210810205414938.png" alt="image-20210810205414938"></p><h3 id="2-rollout-policy"><a href="#2-rollout-policy" class="headerlink" title="2. rollout policy"></a>2. rollout policy</h3><p>rollout policy $p_{\pi}(a \mid s)$ 为 SL network丐版，训练方式一样，只是网络只有一层，是个线性模型，准确率仅有 24.2，但是做一次分类只需要 2us，而 SL network需要 3ms。</p><h3 id="3-RL-policy-network"><a href="#3-RL-policy-network" class="headerlink" title="3. RL policy network"></a>3. RL policy network</h3><p>RL policy network $p_{\rho}(a \mid s)$ 是用RL的方法（policy gradient）训练 监督学习预训练过的模型（SL policy network）。</p><p>SL policy network是通过学习人类棋谱训练的。然而，毕竟人的算力有限，人类棋谱在绝大部分情况下都是sub optimal的，这样学习得到的SL network的成绩有限。</p><p>于是，作者引入了policy gradient 方法，来训练 RL network（ 用训练好的SL network 的权重初始化），$z_t$ 就是 $G_t$ ：</p><script type="math/tex; mode=display">\Delta \rho \propto \frac{\partial \log p_{\rho}\left(a_{t} \mid s_{t}\right)}{\partial \rho} z_{t}</script><p>（<em>根据上式来看，这里似乎使用的vanilia Policy gradient ，即 论文 Policy Gradient Therom 中提出的方法，而不是Actor Critic，原因可能是Actor Critic需要一个value network作为critic，这对于RL policy network是一个鸡生蛋蛋生鸡的问题</em>）</p><p>SL network只能说是在”模仿人类下棋“，但是RL network在 “为了赢而下棋”。论文中<strong>将SL network 和 RL network进行了对战，RL network 胜率为 80%</strong>。</p><h3 id="4-Value-network"><a href="#4-Value-network" class="headerlink" title="4. Value network"></a>4. Value network</h3><p>Value network $v_{\theta}(s)$ 是一个局势评估函数，目的是判断当前局面的好坏。</p><p>在这篇论文里，Value network的训练仍然是一个监督学习的回归问题，先固定住RL policy network进行self-play收集大量数据，然后利用这些数据进行Value network 的监督学习。所以，$v_{\theta}(s)$ 是 RL policy 的 evaluation：</p><ul><li>X: state</li><li>Y: $G_t = \pm1$</li><li>样本生成方式：2 个 RL policy network 相互对战形成episode，每个episode可以生成许多个 $(X,Y)$ pair。 将这些 $(X,Y)$ shuffle 得到数据集。</li><li>网络结构：和SL/RL network相同，只是输出从 19x19 变为了 1。</li></ul><blockquote><p>我个人看论文中的一些问题：</p><ol><li>DQN不也是学习一个value network吗，为什么DQN是强化学习算法，需要一边训练一边收集transition；而这里是一个监督学习问题？</li></ol><p>答：因为DQN中所evaluate的policy是时刻在变化的，而这里仅仅是在evalute  RL policy network。所以是监督学习问题。</p><ol><li>另一个问题是，为什么学习 $v $ 而不是 $Q$？</li></ol><p>答：没有必要，可以参看 [3] 中 Afterstate那一节。</p></blockquote><p><strong>value network的准确性</strong>： 作者对比了 value network(不需要rollout) 和 各种policy经过100次rollout 估计出的 value 与 groudtruth之间的MSE，可以看到，value network的MSE与 RL policy相差无几。然而，RL policy 做 100 次 rollout需要的时间是 value network跑一趟的15000倍。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210810205710365.png" alt="image-20210810205710365"></p><h3 id="5-APV-MCTS"><a href="#5-APV-MCTS" class="headerlink" title="5. APV-MCTS"></a>5. APV-MCTS</h3><p>综合上述网络，可以得到APV-MCTS。在APV-MCTS中，每条边持有：$N(s,a)$ 访问次数， $u(s,a)$ SL network计算的 prior probability，$Q(s,a)$ 平均获胜概率，又叫expected return。</p><ol><li><p>Selection: 根据以下式子从 root 开始进行selection，直到叶子节点。</p><script type="math/tex; mode=display">a_{t}=\underset{a}{\operatorname{argmax}}\left(Q\left(s_{t}, a\right)+u\left(s_{t}, a\right)\right)</script><script type="math/tex; mode=display">u(s, a) \propto \frac{P(s, a)}{1+N(s, a)}</script><p>当该节点初次加入树时，prior probability $P(s,a)$ 由 SL network计算得到，为选取每个动作的概率。每个动作（即每条边）的  $P(s,a)$ 只计算一次，此后不再变化。 $u$ 的作用是，随着访问次数 $N$ 的增加，逐渐鼓励exploration。</p><p>当到达某个叶子结点 时，selection停止。如果该叶子结点不是terminal state，进行expand。</p><blockquote><p>注：这里不太严谨，在 这篇论文里，叶子结点并不是总是expand，需要满足一个条件（见附录）。但是在后续工作AlphaGo Zero中，叶子节点总是expand。</p></blockquote></li><li><p>Expand: 如上所述，新结点$s_L$ 的每条边会存入 SL network 计算的$P(s,a)$，并初始化 $N, Q$。</p></li><li><p>Simulation:</p><p>从新结点出发，重复使用 rollout policy 一直到terminal state，得到reward $z_L$ 。再用value network估计 $s_L$ 的 value。两者综合得出 从 $z_L$ 出发，获胜的平均概率 $V(s_L)$：</p><script type="math/tex; mode=display">V\left(s_{L}\right)=(1-\lambda) v_{\theta}\left(s_{L}\right)+\lambda z_{L}</script></li><li><p>Backup: 对于自 root 到 $s_L$ 上的所有edge，更新其上的 $Q, N$ ：</p><script type="math/tex; mode=display">Q = \frac{Q * N + V(s_L) )} { N+1} \\N += 1</script><p>论文中的公式比这个复杂，但是意思是一样的。</p><p>和vanilla MCTS一样，要注意 $V$ 的正负号，每往上一层，都要 反号。</p></li><li><p>决策阶段，即当MCTS树生成完成之后，会在root选择访问次数最多的一条边，$argmax_aN(root,a)$。</p></li></ol><p>有几点需要注意的：</p><ul><li>先验概率 $P$ 非常重要，它一定程度决定了 每条边的访问次数，因为一开始所有边的 $Q = 0$，selection规则 $a_{t}=\underset{a}{\operatorname{argmax}}\left(Q\left(s_{t}, a\right)+u\left(s_{t}, a\right)\right)$ 相当于是在选择先验概率最大的 边。</li><li>虽然在前面提到了，RL network 比 SL network 的 win rate 更高，但是先验概率  $P(s,a)$ 是由 SL network而不是 RL network计算得到的，这一点比较值得考究。但是，作者通过实验发现，与人类对战时，使用  SL network计算   $P(s,a)$ 的效果强于 RL network。作者推测这是因为 SL network的行为更接近人类棋手（人类棋手会从很多种落子的可能种选择比较优的一个），而 RL network更倾向于 寻找最优解，落子的位置很单一；这使得在于人类对战时，使用更接近人类行为模式的 SL network 的效果更好。</li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h3><p>MCTS的特点之一是在live play 时实时演算，这使得复杂度不高的vanilla MCTS的耗时就很长了，引入了 DL 的 APV-MCTS 每一步的耗时就更是多出了几个数量级。</p><p>不过，Google本来就是分布式计算的引领者，GFS、MapReduce都是出自Google，实现一个分布式的AlphaGo 不是难事。论文实现了2个版本：</p><ul><li>多线程版本：40 APV-MCTS search threads, 48 CPUs(MCTS simulation), 8GPUs(value network/ SL policy network).</li><li>分布式版本：40 APV-MCTS search threads, 1202 CPUs(MCTS simulation), 176GPUs(value network/ SL policy network).</li></ul><p>两个版本每一步MCTS的时间都是 5s，但是分布式版本在相同的时间里可以进行更多次的simulation，决策更好，win rate 更高。</p><h3 id="AlphaGo-Evaluation"><a href="#AlphaGo-Evaluation" class="headerlink" title="AlphaGo Evaluation"></a>AlphaGo Evaluation</h3><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210811170205762.png" alt="image-20210811170205762"></p><p>这张图信息量很大，先从图a开始说起：</p><ol><li><blockquote><p><strong>樊麾</strong>（1981年12月27日－），中国大陆出生的法国职业二段<a href="https://zh.wikipedia.org/wiki/圍棋">围棋</a>棋士。</p><p>2015年10月，樊麾受邀与<a href="https://zh.wikipedia.org/wiki/Google_DeepMind">Google DeepMind</a>所研发出的<a href="https://zh.wikipedia.org/wiki/AlphaGo">AlphaGo</a>进行<a href="https://zh.wikipedia.org/wiki/分先">分先</a>五局竞赛，AlphaGo以5:0全胜的纪录击败樊麾<a href="https://zh.wikipedia.org/wiki/樊麾#cite_note-4">[4]</a>；樊麾成为世界上第一个于十九路棋盘上，以分先<a href="https://zh.wikipedia.org/wiki/手合_(圍棋">手合</a>)被围棋软件击败的职业棋手。</p><p>—— wiki</p></blockquote><p>他也算在人类历史上留下了自己的印记，比许多八九段的还要出名。</p></li><li><p>MCTS是一种耗时可长可短的算法，耗时只与simulation次数有关，论文里统一规定了耗时为5s（或2s）。</p></li><li><p>Elo Rating 听上去像是 某种 在线游戏的天梯分，但其实是适用于各种对弈、对战的等级划分系统。</p></li></ol><p>图b是Ablation Study：</p><ol><li><p>第二列为不使用 value network，即 $\lambda = 1$ ，此时elo下降明显，这说明 对于围棋这种 搜索空间极大的env，只使用rollout完全不足以<strong>在有限时间</strong>中得出比较准确的  $V(s_L)$ ，需要 value network 的辅助。</p><p>第三列为不使用 rollout，此时elo下降更为明显，这说 value  network预测的  $V(s_L)$ 也不够准确。只有rollout 与 value network结合才能得出比较准确的  $V(s_L)$ 。</p><p>value network本质上是 $p_\rho$ 的policy evaluation，所以value network的输出的意义是从 $s_L$ 不断执行 $p_\rho$ 的expected return；而rollout 是 在估计 rollout policy $p_\pi$ 的 expected return，作者认为两者是互补的。</p></li><li><p>令我意外的是，不使用policy network时，elo下降最为明显。论文中没有提原因，但我觉得，SL policy network的作用是指导MCTS的探索方向，使APV-MCTS在promising方向在有限时间内探索比较深的距离；而不用把有限的时间浪费在一些完全错误的分支上。</p></li><li><p>后三列不做解释了，论文中也没有提。</p></li></ol><p>图c略。</p><h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><p>论文中图5展示了APV-MCTS的可视化结果：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210811174507745.png" alt="image-20210811174507745"></p><p>图 a,b,c,d,e都是self-explained的，这里说一下 f：</p><ul><li>Principal variation是指从MCTS root出发，每次选择most visited edge，直到MCTS叶子节点为止的这条路径。可以理解为MCTS所得出的最佳走法。</li><li>AlphaGo下了这一步（红色）之后，它认为对手下 label = 1这一步是最优的。然而，樊麾下了另外一步。复盘时，樊麾认为AlphaGo的走法是更好的。</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf">AlphaGo</a></p><p>[2] <a href="https://mp.weixin.qq.com/s?__biz=MzA5MDE2MjQ0OQ==&amp;mid=2652786766&amp;idx=1&amp;sn=bf6f3189e4a16b9f71f985392c9dc70b&amp;chksm=8be52430bc92ad2644838a9728d808d000286fb9ca7ced056392f1210300286f63bd991bde84#rd">https://mp.weixin.qq.com/s?__biz=MzA5MDE2MjQ0OQ==&amp;mid=2652786766&amp;idx=1&amp;sn=bf6f3189e4a16b9f71f985392c9dc70b&amp;chksm=8be52430bc92ad2644838a9728d808d000286fb9ca7ced056392f1210300286f63bd991bde84#rd</a></p><p>[3] RL, an introduction 2nd, Sutton et al </p><p>[4] <a href="https://kstatic.googleusercontent.com/files/2f51b2a749a284c2e2dfa13911da965f4855092a179469aedd15fbe4efe8f8cbf9c515ef83ac03a6515fa990e6f85fd827dcd477845e806f23a17845072dc7bd">AlphaZero Science 2018</a></p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> MCTS </tag>
            
            <tag> Model-Based RL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Paper][RL][AAAI 2018] Rainbow</title>
      <link href="2021/08/05/ml/qiang-hua-xue-xi/paper/rainbow/"/>
      <url>2021/08/05/ml/qiang-hua-xue-xi/paper/rainbow/</url>
      
        <content type="html"><![CDATA[<p>论文：[AAAI 2018] Rainbow: Combining Improvements in Deep Reinforcement Learning</p><p>后续工作:  <a href="http://proceedings.mlr.press/v139/ceron21a/ceron21a.pdf">[ICML 2021]Revisiting Rainbow: Promoting more Insightful and Inclusive Deep Reinforcement Learning Research</a> </p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>顾名思义，Rainbow是各种颜色的集合，也是各种 Deep Q-learning RL算法的合体。这篇文章做了以下事情：</p><ul><li>将6种Deep Q-learning RL算法组合成Rainbow算法</li><li>做了大量实验，研究了各种算法对Rainbow的影响，并稍微解释了造成影响的原因。</li></ul><p>总的来说，这是一篇实验导向型的文章，对于实验结果的解释和讨论不够充分。今年 ICML 有一篇论文 后续工作:  <a href="http://proceedings.mlr.press/v139/ceron21a/ceron21a.pdf">[ICML 2021]Revisiting Rainbow: Promoting more Insightful and Inclusive Deep Reinforcement Learning Research</a> 对Rainbow的结果进行了更深入的分析和解释，不过在某些方面，得出了和Rainbow不一样的结论，有兴趣的可以看看。</p><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><ul><li><p>DQN </p></li><li><p>Priortized Experience Replay</p></li><li><p>Double DQN</p></li><li>Dueling Network</li><li>A3C (A3C是Actor Critic算法，这篇论文只是利用它的multistep思想)</li><li><p>Distributional RL （Distributional不是指分布式RL，而是指学习目标从 expected retrun 变为 value distribution)</p></li><li><p>Noisy Nets</p></li></ul><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>尽管 以上 6 种算法各不相同（DQN属于公有部分），但是它们从不同的角度在改进DQN，彼此之间并不冲突，将它们综合在一起并没有想象中那么困难。</p><ol><li><p>从Distributional RL 开始，Distributional RL学习的目标为 #atom  = N 的离散概率分布，直接将Distributional RL 的 cross entropy 的loss function拿来就好了，不需要额外的改动。</p></li><li><p>Multistep reward: 这个其实也就是将Distributional RL的target distribution做一个小的修改。Distributional RL是在一步 $\hat{\mathcal{T}} z_{j} = R + \gamma Q(s_{t+1},a<em>)$ 后进行projection，得到target distribution $\Phi_{\boldsymbol{z}} d_{t}^{(1)}$。Rainbow是在多步 $\hat{\mathcal{T}}^{(n)} z_{j} =  R_{t+1} + … + R_{t+n} + \gamma^n Q(s_{t+n},a</em>)$ 之后进行投影，得到target distribution $\Phi_{\boldsymbol{z}} d_{t}^{(n)}$。 loss function为：</p><script type="math/tex; mode=display">D_{\mathrm{KL}}\left(\Phi_{\boldsymbol{z}} d_{t}^{(n)} \| d_{t}\right)</script></li><li><p>Double DQN：在 $\hat{\mathcal{T}}^{(n)} z_{j} =  R_{t+1} + … + R_{t+n} + \gamma^n Q(s_{t+n},a<em>)$ 中，$Q$ 由 target network计算，$a</em>$ 的选择由online network计算，与 Double DQN一致，即：</p></li></ol><script type="math/tex; mode=display">\hat{\mathcal{T}}^{(n)} z_{j} =  R_{t+1} + ... + R_{t+n} + \gamma^n Q_{target}(s_{t+n}, argmax_a{Q(s_{t+n}, a)})</script><ol><li><p>Priortized Experience Replay中样本被选中的概率正比于TD error $\delta = R + \gamma$  。在Distributional RL 的框架下， 样本被选中的概率正比于TD error $D_{KL}$。 因为，TD error和 $D_{\mathrm{KL}}$ 是等价的概念。</p></li><li><p>Dueling Network: 在 Distributional RL 的框架下， Dueling Network也只需要做一点小的改动。仍然是有个共享卷积 $\xi$， value branch $\eta$ 的输出维度从 1 变为 #atoms，而 advange branch $\psi$ 的输出维度从 #actions 变为  #actions x #atoms。输出层的构造方式仍然和 Dueling Network 中的一样。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210809220423770.png" alt="image-20210809220423770"></p></li></ol><ol><li>Noisy Net：没有变化，仍然是用Noisy layer代替神经网络中的 $\boldsymbol{y}=\boldsymbol{b}+\mathbf{W} \boldsymbol{x}$ layer：<script type="math/tex; mode=display">\boldsymbol{y}=(\boldsymbol{b}+\mathbf{W} x)+\left(\boldsymbol{b}_{\text {noisy }} \odot \epsilon^{b}+\left(\mathbf{W}_{\text {noisy }} \odot \epsilon^{w}\right) \boldsymbol{x}\right)</script></li></ol><p>至此， 6种 DQN的improvement 集于一身，得到了 <strong>Rainbow</strong>。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="超参tuning"><a href="#超参tuning" class="headerlink" title="超参tuning"></a>超参tuning</h3><p>不同论文都会引入不同的超参，可以想象，Rainbow 的作者花了很多时间调参，也在论文中分享了一些经验，感兴趣的可以去看论文，这里展示作者的调参结果：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210810145722706.png" alt="红框为各种方法引入的超参，其他超参为DQN固有的"></p><h3 id="Results-on-Atari"><a href="#Results-on-Atari" class="headerlink" title="Results on Atari"></a>Results on Atari</h3><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210810151116823.png" alt="image-20210810151116823"></p><h3 id="Ablation-study"><a href="#Ablation-study" class="headerlink" title="Ablation study"></a>Ablation study</h3><p>显而易见，将各种方法结合起来肯定会提高scores。但是，我们更感兴趣的是，哪种方法的提升更大？作者做了相关的Ablatioin Study，将每个方法分别从Rainbow中移除，观察移除后的scroes。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210810150654674.png" alt="image-20210810150654674"></p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210810150625637.png" alt="image-20210810150625637"></p><p>从图上可以看出几点（更详细请看论文）：</p><ol><li>Prioritized replay，multi-step learning 的移除对结果影响最大。</li><li>其次是 Disbritional DQN，再其次是 Noisy Net。</li><li>对于 Dueling network，在 &gt;20% &gt;50% human上，移除dueling甚至会提升结果。作者没有解释原因。</li><li>移除Double Q-learning同样会提升结果，作者认为，这是因为true  Q值一般在10以上，而Distributional DQN将Q clip到了 -10 10之间，相当于产生了underestimation，这与 Double Q-learning的设计目的（解决overestimation）产生了冲突。所以，加上了Double Q-learning甚至产生了副作用。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> Model-Free </tag>
            
            <tag> Deep Q-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Paper][RL][ICML 2017] Distributional RL</title>
      <link href="2021/08/02/ml/qiang-hua-xue-xi/paper/distributional-dqn/"/>
      <url>2021/08/02/ml/qiang-hua-xue-xi/paper/distributional-dqn/</url>
      
        <content type="html"><![CDATA[<p>论文：ICML 2017 A Distributional Perspective on Reinforcement Learning</p><h2 id="引子-amp-Motivation"><a href="#引子-amp-Motivation" class="headerlink" title="引子 &amp; Motivation"></a>引子 &amp; Motivation</h2><p>Motivation &amp; idea: Approximate value distribution instead of expected return. </p><p>然而，idea虽然简单，实现起来却并不容易。最简单的想法使用一个 高斯分布 对 $Q$ 建模，然而这其实与 approximate expected return区别不大。这篇文章采取的是另一种方法，这使得所拟合的分布不仅限于 单峰形态的概率分布函数（比如 高斯分布）：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210809193347237.png" alt="image-20210809193347237"></p><h2 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h2><p>略</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><h3 id="1-区间离散化"><a href="#1-区间离散化" class="headerlink" title="1. 区间离散化"></a>1. 区间离散化</h3><p>如何方便的建模任意的概率分布函数呢？作者使用了一个离散分布来建模：</p><ol><li><p>将 $[V_{min}, V_{max}]$ 上均匀采样 $N$ 个点，$z_0, z_1, …z_{N}$。（$V_{min}, V_{max}, N$ 是三个超参，在本文的实验部分，使用的超参为 $-10, 10, 51$，即相邻点的相差 $0.4$。</p></li><li><p>每个点的概率 为 </p><script type="math/tex; mode=display">P[Q(x,a) = z_i] = p_{i}(x, a)=\frac{e^{\theta_{i}(x, a)}}{\sum_{j} e^{\theta_{j}(x, a)}} (0 \le i < N)</script><p>这里的 $\theta_{i}(x, a)$ 为需要学习的神经网络，神经网络的输出通过归一化转化为概率。</p></li></ol><h3 id="2-Projected-Bellman-Update"><a href="#2-Projected-Bellman-Update" class="headerlink" title="2. Projected Bellman Update"></a>2. Projected Bellman Update</h3><p>DQN中的target value 为 ：</p><script type="math/tex; mode=display">Q_{target}(s_t,a) = R + \gamma Q(s^{t+1}, a^*)</script><p>这篇论文中的target distribution为：</p><ol><li><p>计算 next state 的 best action $a^*$ 的distribution：</p><script type="math/tex; mode=display">\begin{aligned}&Q\left(x_{t+1}, a\right):=\sum_{i} z_{i} p_{i}\left(x_{t+1}, a\right) \\&a^{*} \leftarrow \arg \max _{a} Q\left(x_{t+1}, a\right)\end{aligned}</script></li><li><p>计算 离散点 $z_j$ 经过Bellman Update后所处的位置：</p></li></ol><script type="math/tex; mode=display">\begin{aligned}&\hat{\mathcal{T}} z_{j} \leftarrow\left[r_{t}+\gamma_{t} z_{j}\right]_{V_{\mathrm{MIN}}}^{V_{\mathrm{MAX}}} \end{aligned}</script><ol><li>由于 $\gamma$ 和 $r_t$ 影响，Bellman Update之后的 $\hat{\mathcal{T}} z_{j}$ 可能并不在预先设好的离散点 $z_0, z_1, …z_{N}$ 上。于是根据  $\hat{\mathcal{T}} z_{j}$ 与相邻离散点 $l,u$ 的 距离将  $p_{j}\left(x_{t+1}, a^{*}\right)$ 分配到  $l,u$ 上。（  $l,u$ 是  $z_0, z_1, …z_{N}$ 中的点）：</li></ol><script type="math/tex; mode=display">\begin{aligned}&b_{j} \leftarrow\left(\hat{\mathcal{T}} z_{j}-V_{\mathrm{MIN}}\right) / \Delta z \quad \# b_{j} \in[0, N-1]\\&l \leftarrow\left\lfloor b_{j}\right\rfloor, u \leftarrow\left\lceil b_{j}\right\rceil \text {   # Distribute probability of } \hat{\mathcal{T}} z_{j}\\&m_{l} \leftarrow m_{l}+p_{j}\left(x_{t+1}, a^{*}\right)\left(u-b_{j}\right)\\&m_{u} \leftarrow m_{u}+p_{j}\left(x_{t+1}, a^{*}\right)\left(b_{j}-l\right)\end{aligned}</script><ol><li>所得到的 $m_0, m_1, ….m_N$ 即为 target distribution 的 每个离散点 $z_0, z_1, …z_{N}$ 上的概率。这里由于是两个概率之间的差值，所以loss function采用<a href="https://en.wikipedia.org/wiki/Cross_entropy">cross-entropy</a> ：<script type="math/tex; mode=display">-\sum_{i} m_{i} \log p_{i}\left(x_{t}, a_{t}\right)</script></li></ol><p>至此，得到最终的算法：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210809200918751.png" alt="image-20210809200918751"></p><p>附示意图一张：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210809201130640.png" alt="image-20210809201130640"></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>待补充</p><h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p>待补充</p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> Model-Free </tag>
            
            <tag> Deep Q-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Paper][RL][ICML 2016] Dueling DQN</title>
      <link href="2021/08/01/ml/qiang-hua-xue-xi/paper/dueling-dqn/"/>
      <url>2021/08/01/ml/qiang-hua-xue-xi/paper/dueling-dqn/</url>
      
        <content type="html"><![CDATA[<p>论文：[ICML 2016] Dueling Network Architectures for Deep Reinforcement Learning</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>如果说Double DQN，Prioritized Replay 是在 算法上对DQN的改进，那Dueling Network就是在网络结构上对 DQN的改进。和DQN相比，只需要将DQN中的网络替换成Dueling Network，其他部分保持不变。</p><p><strong>Why Dueling Network work?</strong></p><ol><li>Dueling Network将 $Q$ 的学习拆分为 $V$ 和 $A$ 的学习。不同于 Single Network中每次更新时只有对应action的参数得到了更新，在Dueling Network中，每次更新，$V$ 都得到了更新。$V$ 的学习非常充分，对 $true \ V$ 的拟合也就更好，所以Q-learning 这种 Temporal Difference(TD) 算法的效果也好。</li><li>对于一个state，可能其 $Q(s,a)$ 的绝对大小较大，但是 $Q(s,a_1), Q(s,a_2)…$ 之间的相差并不大，论文中以 game Seaquest举了例子。如果直接学习 $Q$ ，可能Q的绝对大小学的大差不差，但是 $Q(s,a_1), Q(s,a_2)…$ 之间的大小关系并不容易学习。所以这里将大小关系单独提出来进行学习（即 Advantage Branch）</li></ol><h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>Dueling Network的网络结构如下，两个branch分别预测 $V(s)$ 和 $A(s,a)$：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210806195520435.png" alt="image-20210806195520435"></p><p>回忆 Advantage 的定义：$A(s,a) = Q(s,a) - V(s)$，似乎可以将Dueling Q-network的output layer构造成两个branch的简单相加：</p><script type="math/tex; mode=display">Q(s, a ; \theta, \alpha, \beta)=V(s ; \theta, \beta)+A(s, a ; \theta, \alpha)</script><p><em>（注： $\theta$ 为共享卷积层的参数， $\alpha $ 为 branch A的参数， $\beta$ 为 branch V 的参数）</em></p><p>但是作者认为这样是没有意义的（lack of identifiability）。因为，一种最极端的情况是，branch V全部输出0，branch A输出Q，这就退化成了DQN。总的来说，<strong>branch V需要知道自己在approximate V，branch A也需要知道自己在approximate A。</strong></p><p>作者也做了相关的实验，如果直接这么构造输出层，实际的结果很差。</p><p>于是，作者接下来提出了这样来构造output layer：</p><script type="math/tex; mode=display">\begin{aligned}&Q(s, a ; \theta, \alpha, \beta)=V(s ; \theta, \beta)+ \left(A(s, a ; \theta, \alpha)-\max _{a^{\prime} \in|\mathcal{A}|} A\left(s, a^{\prime} ; \theta, \alpha\right)\right)\end{aligned}</script><p>当 $a = a^<em>$ 时，显然上式中第二项 $ A(s, a ; \theta, \alpha)-\max _{a^{\prime} \in|\mathcal{A}|} A\left(s, a^{\prime} ; \theta, \alpha\right) $ 为0，于是branch V 需要 approximate  $Q(s, a^</em> )$ 即 $V(s)$。</p><p>这样一来，branch V 就知道自己在approximate V了。既然branch V 知道自己在approximate V，这也使得了 branch A 知道自己在approximate A了。（这个地方有一点难以理解，反正最终work了）</p><p>不过，作者最终使用的是下面这个式子：</p><script type="math/tex; mode=display">\begin{aligned}&Q(s, a ; \theta, \alpha, \beta)=V(s ; \theta, \beta)+\left(A(s, a ; \theta, \alpha)-\frac{1}{|\mathcal{A}|} \sum_{a^{\prime}} A\left(s, a^{\prime} ; \theta, \alpha\right)\right)\end{aligned}</script><p>它与之前那个式子没有本质上的区别，仅仅是为了提高训练的稳定性 [1] 。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Policy-Evaluation"><a href="#Policy-Evaluation" class="headerlink" title="Policy Evaluation"></a>Policy Evaluation</h3><p>Dueling DQN比DQN优越之处，就是在于 预测 $Q$ 比 single stream(DQN) 更准。 而如何比较谁的 $Q$ 更准，最直接的就是抛开 policy improvement， 比较谁的 Evaluation的结果更精确。</p><p>于是作者在一个toy environment(corridor) 做了实验。Dueling Network预测的 $Q$ 与 true $Q$ 相差更小，且收敛更快。当 #action 变大时，这个现象更明显。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210806210647553.png" alt="image-20210806210647553"></p><h3 id="Atari"><a href="#Atari" class="headerlink" title="Atari"></a>Atari</h3><p>在Atari上的结果如下；</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210807032829811.png" alt="image-20210807032829811"></p><p>有几点值得注意的：</p><ol><li>We clip the gradients to have their norm less than or equal to 10. 这篇论文发现 gradient clip 尤其的有效。比如表中 Single vs Single Clip，有不小的提升，所以对于所有模型几乎都使用了 Clip。</li><li>Prioritized Replay , Double Q-learning, Dueling Network 可以结合在一起，共同提升结果。</li></ol><h3 id="Saliency-Map"><a href="#Saliency-Map" class="headerlink" title="Saliency Map"></a>Saliency Map</h3><p>Saliency Map有多种实现方式[2]。这里采取的是计算 Jacobian 矩阵。</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] <a href="https://ai.stackexchange.com/questions/8128/difficulty-in-understanding-identifiability-in-the-dueling-network-architecture">Difficulty in understanding identifiability in the “Dueling Network Architectures for Deep Reinforcement Learning” paper</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/135376948">【强化学习 117】Saliency Map - 张楚珩的文章 - 知乎 </a></p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> Model-Free </tag>
            
            <tag> Deep Q-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Paper][RL][AAAI 2016] Double DQN</title>
      <link href="2021/07/31/ml/qiang-hua-xue-xi/paper/double-dqn/"/>
      <url>2021/07/31/ml/qiang-hua-xue-xi/paper/double-dqn/</url>
      
        <content type="html"><![CDATA[<h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>这篇文章的创新之处并不多，可以用一句话概括：将 DQN 中的 Q-learning换成了 Double Q-learning。DQN算法和代码中需要修改的地方不超过2行。</p><p>所以，这篇文章基本没提算法，而是用大量篇幅描写各种实验，以展示Q-learning引入的bias，以及Double Q-learning 如何改善这种bias，进而改进policy。甚至，因为和DQN太像了，在论文中没有给出 Double DQN的算法伪代码。</p><p>不过，这篇论文图文并茂的解释了Q-learning所带来的overeestimation问题。之前看Sutton的书时，理解并不深刻，这篇论文却从多个角度让我加深了理解。</p><h2 id="论文总结-amp-Motivation"><a href="#论文总结-amp-Motivation" class="headerlink" title="论文总结 &amp; Motivation"></a>论文总结 &amp; Motivation</h2><p>这篇文章提出并解答了几个问题：</p><ol><li>Q-learning(DQN) 是否存在 overestimation现象？存在。</li><li>overestimation是否一定会影响policy？如果uniformly overestimate，则不会。但大多数情况下并不是uniformly，所以会。</li><li>如何将Double Q-learning加到DQN中？非常简单的修改。</li><li>Double Q-learning(Double DQN) 是否缓解了overestimation，并使得最终得到的policy有了提升？是的。</li></ol><p><em>注：overestimation是否只能通过Double learning解决？不一定，Rainbow[1]中认为 Distributional RL[2] 可以部分替代 Double Q-learning的作用</em></p><h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>Double DQN相比DQN唯一不同的地方，就是target的计算方式。</p><p>DQN中 target 的计算公式为：</p><script type="math/tex; mode=display">Y_{t}^{\mathrm{DQN}} \equiv R_{t+1}+\gamma \max _{a} Q\left(S_{t+1}, a ; \theta_{t}^{-}\right)</script><p>套用Double Q-learning，将上式改写为：</p><script type="math/tex; mode=display">Y_{t}^{\text {DoubleDQN }} \equiv R_{t+1}+\gamma Q_{target}\left(S_{t+1}, \underset{a}{\operatorname{argmax}} Q\left(S_{t+1}, a ; \boldsymbol{\theta}_{t}\right), \boldsymbol{\theta}_{t}^{-}\right)</script><p>Double DQN中，选择action由 $\theta_{t}$ 完成，计算Q由 $\theta_{t}^{-}$(target network) 完成。定期将  $\theta_{t}^{-}$拷贝到  $\theta_{t}^{-}$ 上， 不需要像 Double Q-learning那样交替更新。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Double-DQN缓解了-Overestimation"><a href="#Double-DQN缓解了-Overestimation" class="headerlink" title="Double DQN缓解了 Overestimation"></a>Double DQN缓解了 Overestimation</h3><p>Double DQN选取了一些Atari中的游戏，与DQN做了对比，结果在下图中。这幅图清楚的展示了Double Q-learning的优点：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210806002030210.png" alt="image-20210806002030210"></p><p>这幅图有几点需要解释的：</p><p>top row:</p><ol><li><p>top row中的横线是 true value。指的是：当训练完成时，用训练好的网络，在某个state上，模拟多个episode，用episode的reward计算出真实的 $Q$ 值。所以，横线是 true Q of final policy。 所以，需要关注的是训练完成时曲线与横线的差距。</p><p>显然训练完成时，Double DQN离true Double DQN更近一些，这说明Double DQN 的overestimation现象更不明显。</p></li><li><p>true Double DQN &gt; true DQN。这说明 Double DQN 是比 DQN 更好的 policy（Q值的意义就是 expected reward）</p></li></ol><p>middle &amp; bottom row:</p><ol><li>middle row是Atari中两种游戏的value estimate，注意到y轴是log scale的，这意味着value estimate与true Value相差非常之多。</li><li>middle row 中DQN在50 millons step时<strong>上涨明显，</strong>此时bottom row中scores中的DQN的曲线开始<strong>急速下降</strong>，论文中认为这并不是一种巧合。这说明overestimation确实对scores有坏的影响。</li><li>尽管这里只展示了2种游戏，但上述现象在Atari 中的 所有游戏中都发生了，这可以证明 <em>“在DQN中，overestimation对scores有坏的影响”</em> 是一个普遍的结论</li></ol><h3 id="Double-DQN-提高了-scores"><a href="#Double-DQN-提高了-scores" class="headerlink" title="Double DQN 提高了 scores"></a>Double DQN 提高了 scores</h3><p>文中分了两小节说明 Double DQN在 Atari上 1）仅就score而言，表现比 DQN要好  2）在human start上，泛化能力也更强。</p><h4 id="Part-1-Quality-of-the-learned-policies"><a href="#Part-1-Quality-of-the-learned-policies" class="headerlink" title="Part 1 Quality of the learned policies"></a>Part 1 Quality of the learned policies</h4><p>Part 1按照DQN的evaluton流程，starting point是通过no-op操作生成的，存在一定的随机性，但是随机性有限。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210806135502427.png" alt="image-20210806135502427"></p><p>（注：表中Double DQN所用的超参与DQN保持一致，并没有调过。）</p><h4 id="Part-2-Robustness-to-Human-starts"><a href="#Part-2-Robustness-to-Human-starts" class="headerlink" title="Part 2 Robustness to Human starts"></a>Part 2 Robustness to Human starts</h4><p>Part 2  obtained 100 starting points sampled for each game from a human expert’s trajectory, as proposed by Nair et al. We start an evaluation episode from each of these starting points and run the emulator for up to 108,000 frames.</p><p>与Part 1相比，Part 2 的starting points更为随机，更考验模型的<strong>泛化能力</strong>。实验结果也佐证了这一点，DQN、Double DQN均有下滑。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210806140402258.png" alt="image-20210806140402258"></p><p>（注：表中Double DQN所用的超参与DQN保持一致，并没有调过。而Double DQN(tuned) 经过了仔细调参）</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] [AAAI 2018] Rainbow: Combining Improvements in Deep Reinforcement Learning<br>[2] ICML 2017 A Distributional Perspective on Reinforcement Learning</p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> Model-Free </tag>
            
            <tag> Deep Q-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Paper][RL][ICLR 2016] Prioritized Experience Replay</title>
      <link href="2021/07/29/ml/qiang-hua-xue-xi/paper/prior-replay/"/>
      <url>2021/07/29/ml/qiang-hua-xue-xi/paper/prior-replay/</url>
      
        <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/pdf/1511.05952.pdf">ICLR 2016 Prioritized Experience Replay</a></p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>本篇论文是DQN中Experience Replay的后续工作。</p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>The key idea is that an RL agent can learn more effectively from some transitions than from others.</p><h3 id="Example-Blind-Cliffwalk"><a href="#Example-Blind-Cliffwalk" class="headerlink" title="Example: Blind Cliffwalk"></a>Example: Blind Cliffwalk</h3><p>论文中用了一个例子来说明不同样本需要不同权重的必要性。</p><p>在下图中的 Cliffwalk中，显然，agent只有 $1/2^{n-1}$ 的概率到达终点，所以，replay memory中有大量的雷同的transition，只有极少数到达终点的episode的transition。而这些极少数的transition相对而言更加重要。</p><p>（注：这个example的详细setup可以在 Appendix中找到。 由于Cliffwalk的可能的episode个数是有限的，所以replay memory不会进行淘汰，包含了所有可能的episode的transition。对于 n 个 state的Cliffwalk来说，可能的transition个数为 $2^{n+1} - 2$，成功到达终点的transition个数为 $n-1$，如果均匀抽样，选中的概率可以说是非常小了）</p><p>下图右展示了最优抽样（oracle)和均匀抽样情况下，Q函数收敛所需update次数与 cliffwalk中state[注1]的个数的关系。显然，两者有数量级的差异。图中折线是多次实验的中位数。</p><p>（注1：图中用的是#samples，和#states是等价的，成正比关系，#samples = $2^{n+1} - 2$ , n = #states）</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210804174010767.png" alt="image-20210804174010767"></p><p>尽管这是个非常极端的例子，也某种程度上说明了  Prioritized Experience Replay 的重要性。</p><h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><h3 id="Prioritizing-with-TD-error"><a href="#Prioritizing-with-TD-error" class="headerlink" title="Prioritizing with TD error"></a>Prioritizing with TD error</h3><p>这篇论文的核心问题为 如何estimate一个transition的重要性？这时候，可能有人会想到用另一个神经网络来estimate。且慢，有更简单的方法：</p><blockquote><p>One idealised criterion would be the amount the RL agent can learn from a transition in its current state (expected learning progress).</p></blockquote><p>而 <em>the amount the RL agent can learn from a transition</em> 用 TD error 来衡量。$\delta$ 越大，说明 loss 越大，$Q$ network 估计的越不准，被选中的优先级越高。</p><h3 id="Stocastic-Prioritization"><a href="#Stocastic-Prioritization" class="headerlink" title="Stocastic Prioritization"></a>Stocastic Prioritization</h3><p>只使用 $\delta$ 作为priority，根据 $\delta$ 的大小逐个选取transition 会产生了一系列的问题。比如：</p><ul><li><p>TD-error要实时更新。可能一开始某个transition的 $\delta$ 很低，但后来$\delta$ 变得很高。如果不实时更新，该transition的优先级一直都很低，永远得不到更新。</p><p>然而，Replay memory 的数量级在 $10^6$ ，不可能做到实时更新。$\delta$ 只在transition被放入memory时 或 再次被放入时的 $\delta$ 才更新。</p></li><li><p>一些其他问题，参见论文。</p></li></ul><p>基于以上问题，不能只根据 $\delta$ 来抽取样本，而是要随机一点。每个transition 被选中的概率：</p><script type="math/tex; mode=display">\begin{equation}P(i)=\frac{p_{i}^{\alpha}}{\sum_{k} p_{k}^{\alpha}}\end{equation}</script><p>其中 $\alpha$ 为超参，在Atari 2600中设为 0.6 / 0.7。$p_k$ 为 priority，有两种不同的选择，这两种选择的performance互有好坏，需要按照情况选择。</p><ol><li><strong>Proportional Prioritization</strong> : $p_{i}=\left|\delta_{i}\right|+\epsilon$ ，其中 $\epsilon$ 的作用是使 $p_i &gt; 0$</li><li><strong>Rank-based Prioritization</strong> :  $p_{i}=\frac{1}{\operatorname{rank}(i)}$  rank(i) 为 按照 $\left|\delta_{i}\right|$ 从大到小排序的序号。</li></ol><blockquote><p>看上去 Rank-based Prioritization 的效果应该会更好，但实际上不是，在Discussion部分作者解释为：</p><p>we expected the rank-based variant to be more robust because it is not affected by outliers nor error magnitudes. Furthermore, its heavy-tail property also guarantees that samples will be diverse, and the stratified sampling from partitions of different errors will keep the total minibatch gradient at a stable magnitude throughout training.</p><p>Perhaps surprisingly, both variants perform similarly in practice; we suspect this is due to the heavy use of clipping (of rewards and TD-errors) in the DQN algorithm, which removes outliers.</p></blockquote><p>上述两者都是  $\left|\delta_{i}\right|$ 越大，$p_i$ 越大。只不过作者认为第二种更robust一点。</p><h3 id="Importance-Sampling"><a href="#Importance-Sampling" class="headerlink" title="Importance Sampling"></a>Importance Sampling</h3><p>当我们根据Stocastic Prioritization来采样transition时，high priority的transition会被采样许多次，而low priority的transition会被采样较少的次数。用数学语言来说就是，原本我们uniformly 采样，求得 $\mathbb{E_{\sim uniform}[gradient]}$ ，现在换了一个分布采样，显然会引入bias。</p><p>（一个比较容易理解的例子是利用蒙特卡洛积分求曲线下面积，也引入了Importance Sampling修正bias，否则算出的曲线下面积是错误的）</p><p>为了修正这点，引入了Importance Sampling：</p><script type="math/tex; mode=display">\begin{equation}w_{i}=\left(\frac{1}{N} \cdot \frac{1}{P(i)}\right)^{\beta}\end{equation}</script><p>当 $\beta = 1$ 时，bias完全被修正了。</p><p>在论文中采取的是逐步增大 $\beta$ 的操作，从 $\beta = \beta_0$ 逐步增大到 $\beta = 1$ 。</p><h3 id="Algorithm-Double-DQN-with-proportional-prioritization"><a href="#Algorithm-Double-DQN-with-proportional-prioritization" class="headerlink" title="Algorithm: Double DQN with proportional prioritization"></a>Algorithm: Double DQN with proportional prioritization</h3><p>论文结合当时的SOTA Double DQN给出了最终的算法，红框中为与 Double DQN 不同的地方 ：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210804190132858.png" alt="image-20210804190132858"></p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>实验的细节、参数基本和 DQN一致，这里不再赘述，只讲几点比较值得注意的：</p><ol><li>memory 的淘汰方式并不是淘汰 $P(i)$ 最低的，仍然是淘汰最先进入memory的。</li><li>超参 $\alpha$ = 0.7, $\beta_0$ = 0.5 （只有 $\beta$ 有 annealing）</li></ol><h3 id="1-Normalized-Score"><a href="#1-Normalized-Score" class="headerlink" title="1. Normalized Score"></a>1. Normalized Score</h3><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210804193245644.png" alt="image-20210804193245644"></p><h3 id="2-Learning-speed"><a href="#2-Learning-speed" class="headerlink" title="2. Learning speed"></a>2. Learning speed</h3><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210804193228198.png" alt="image-20210804193228198"></p><h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p>待补充</p><h2 id="Extension"><a href="#Extension" class="headerlink" title="Extension"></a>Extension</h2><p>Prioritized Supervised Learning：一般在 Supervised Learning中，由于我们知道各个类的数据量，对于unbalanced的dataset，一般是对数据量少的那一类做data augmentation，使得不同类的数据量持平。不过，假如我们不知道各个类的数据量呢？这篇论文将 Prioritized sampling ( $\alpha$ = 0.7, $\beta$ = 0，no annealing）应用在imbalanced MNIST上：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210804201330159.png" alt="image-20210804201330159"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1]  <a href="https://datascience.stackexchange.com/questions/32873/prioritized-replay-what-does-importance-sampling-really-do">Prioritized-replay-what-does-importance-sampling-really-do</a></p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> Model-Free </tag>
            
            <tag> Deep Q-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Paper][RL][ICML 2016] A3C</title>
      <link href="2021/07/28/ml/qiang-hua-xue-xi/paper/a3c/"/>
      <url>2021/07/28/ml/qiang-hua-xue-xi/paper/a3c/</url>
      
        <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/pdf/1602.01783.pdf">Asynchronous Methods for Deep Reinforcement Learning</a> ICML 2016</p><h2 id="引子-amp-总结"><a href="#引子-amp-总结" class="headerlink" title="引子 &amp; 总结"></a>引子 &amp; 总结</h2><p>这篇文章的Motivation比较简单，据论文所说，是为了在Experience Replay之外，找到一种训练深度强化学习网络的方法。</p><p>文章中提出了多种算法，但是最出名的是A3C算法，在更短时间内，达到甚至超过了 Experience Replay 的 Performance。A3C的思路极其简单，可以在此之上做很多A+B之类的研究。</p><p>A3C的想法是这样的：每个agent在自己独立的环境中进行探索，互不干扰。每个agent都在执行自己的Actor-Critic 算法，进行一定次数的探索之后，得到policy network 和 value-network的梯度，<strong>异步更新</strong>全局的 policy network 和 value-network。</p><p>A3C与Experience Replay都在试图解决同一个问题：</p><ul><li><p>用强化学习的方法训练Network的一个难点在于 强化学习获得的样本是连续的，这种连续性会干扰神经网络的学习，降低网络的收敛速度和性能。为了解决这个问题，Experience Replay通过随机采样memory中的transition，使得同一batch中各个transition之间在时间顺序上互不相干，从而解决了这个问题；但是Experience Replay的性质，导致它不能用于on-policy RL算法。</p></li><li><p>A3C某种程度上也是在做同样的事情</p><blockquote><p>This parallelism also decorrelates the agents’ data into a more stationary process, since at any given time-step the parallel agents will be experiencing a variety of different states.</p></blockquote><p>但是，与Experience Replay不同的是，它可以用于on-policy RL算法。</p></li></ul><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Experience Replay存在缺点：</p><ul><li>需要大量计算资源（GPU，memory）</li><li>仅适用于 off-policy 算法</li></ul><p>有没有其他方法？</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>这篇论文的基本想法在引子中提到了，这里不再重复。这种思想可以与很多RL算法结合，比如 Q-learning：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210722151323505.png" alt="image-20210722151323505"></p><p>还可以和 Actor-Critic结合，得到著名的A3C：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210722222151395.png" alt="image-20210722222151395"></p><ul><li><p>这个算法和 Actor-Critic没区别，只是多了并行异步的部分。</p></li><li><p>n-step bootstrapping：每个线程中对于value network和 policy network的更新都是 n-step bootstrapping的。与 Sutton Book中给出的n-step bootstrapping算法基本相同，只是有一点区别：</p><p>在A3C中，每隔 $t_{max}$ 步骤停下来进行更新，从 $t-1 $ 更新到 $t_{start}$。对于 $t-1$ ,  $G_{t-1} = R_{t} + V(s_t)$（ $G_t$ 即上图中的 $R$），对于 $t - 2$ , $G_{t-2} = R_{t-1} + R_{t} + V(s_t)$ ….。可以这么不严谨的理解，对于 $t-1$，相当于 TD(0)，对于 $t-2$ 相当于 TD(1)，对于 t_start，相当于 TD(t_max - 1)，是bootstrapping from last state。</p></li><li><p>正则项：为了鼓励exploration，在Loss Function末尾加上 $H\left(\pi\left(s_{t}\right)\right)$ ，这里 $H$ 代表entropy 。</p><p>可以这么理解entropy和鼓励exploration之间的关系：当 $\pi$ 给出的概率分比较分散的时候，entropy比较大，即 $H\left(\pi\left(s_{t} ; \theta^{\prime}\right)\right)$ 比较大。加上这一 ”正则项“ 使得 $\pi$ 给出的概率不集中于某一个action，而是比较分散，即鼓励exploration。、</p></li><li><p>网络结构：feature extractor (CNN) 共享，最后有两个独立的输出层——1）policy network的fc + softmax层；2）value network的 fc 层。</p></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Atari-2600-与Experience-Replay的对比"><a href="#Atari-2600-与Experience-Replay的对比" class="headerlink" title="Atari 2600: 与Experience Replay的对比"></a>Atari 2600: 与Experience Replay的对比</h3><p>与Experience Replay的对比分两部分。</p><h4 id="1-收敛速度"><a href="#1-收敛速度" class="headerlink" title="1. 收敛速度"></a>1. 收敛速度</h4><p>A3C（在16 个CPU核上）比 DQN（K40）收敛更快。</p><p>鉴于这篇文章想要challenge的是DQN中的Experience Replay，所以做了大量与DQN相关的对比实验。论文选取了5个Atari 2600中的游戏，比较DQN与这篇论文提出的算法的性能。明显看到，A3C的收敛速度要快很多：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210727012205577.png" alt="image-20210727012205577"></p><p><em>注：出于公平，A3C没有选DQN表现最差的游戏（可能A3C在这些游戏上表现也不好），比如 Breakout（打砖块）在DQN下面的结果也很好（不过不是最好的）。</em></p><p><em>注：论文里没有比较A3C和 Prioritized DQN or Dueling D-DQN的收敛速度</em></p><h4 id="2-Scores"><a href="#2-Scores" class="headerlink" title="2. Scores"></a>2. Scores</h4><p>不仅收敛速度 A3C更快，充分训练后的performance也是最好的：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210728185737820.png" alt="image-20210728185737820"></p><p>注：公平的对比是红框之间的对比，因为DQN没有用到LSTM，红框之间的网络结构是相同的。而<code>A3C,LSTM</code> 用到了LSTM。</p><h3 id="更多实验：TORCS-MoJoCo-Labyrinth"><a href="#更多实验：TORCS-MoJoCo-Labyrinth" class="headerlink" title="更多实验：TORCS, MoJoCo, Labyrinth"></a>更多实验：TORCS, MoJoCo, Labyrinth</h3><p>论文中还做了一些其他实验，但是没有涉及和Experience Replay的比较，这里就略了。</p><h3 id="Scalability"><a href="#Scalability" class="headerlink" title="Scalability"></a>Scalability</h3><p>这部分展现的是 线程个数对收敛速度的影响。</p><p>对于前三种方法而言，令人惊讶的是 加速比竟然大于了 线程数量（这在普通多线程中是不可能的的），这说明多个线程同时更新一个模型可以加速模型的训练。</p><blockquote><p>We believe this is due to positive effect of multiple threads to reduce the bias in one-step methods.</p></blockquote><p>然后对于最有效的 A3C 来说，似乎没有这种效果（论文中似乎没有解释原因），但是加速比也很好了。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210726210743380.png" alt="image-20210726210743380"></p><h3 id="Robustness"><a href="#Robustness" class="headerlink" title="Robustness"></a>Robustness</h3><p>这里展现的是A3C对与learning rate和网络初始化方式不敏感，不需要特别细致的调参。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210727010725301.png" alt="image-20210727010725301"></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>这篇文章提出了一种比较泛用的算法框架，可以非常方便的与online RL算法做结合，并取得不错的效果。</p><p>这篇文章提出的方法具有以下优点：</p><ul><li>考虑到 Experience Replay需要大量的内存存放transition以及GPU，这篇文章的方法运作在CPU上，需要的资源更少，但也具有加速/稳定深度强化学习算法的作用。</li><li>此外，A3C的算法与Replay并没有冲突，所以二者可以结合，达到更好的效果，尤其是某些环境的transiton来之不易，需要多次复用（比如论文中的TORCS）。</li><li>这篇文章提出的算法非常的general，所以有很多future work可以做，比如backward view n-step bootstrapping；与double Q-learning的结合等等</li></ul>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> Policy Gradient </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Paper][RL][Nature 2015] DQN论文笔记 及 实现</title>
      <link href="2021/07/21/ml/qiang-hua-xue-xi/paper/dqn/"/>
      <url>2021/07/21/ml/qiang-hua-xue-xi/paper/dqn/</url>
      
        <content type="html"><![CDATA[<p>论文：Human-level control through deep reinforcement learning</p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>这篇论文（DQN）将深度学习引入<strong>端到端</strong>的强化学习。为了提高stability和加快网络收敛，论文又提出了Experience Replay 和 target network。DQN在Atari 2600的大部分游戏上，达到了跟人类差不多的游戏水平。</p><p>对于Atari 2600的40多种游戏，DQN都使用了同一个网络结构，同一套超参，得到了不错的结果，这意味着DQN具有普适性，不是只针对某个问题的特殊解。</p><blockquote><p>来源：Sutton Book 2nd Edition</p><p>Creating artificial agents that excel over a diverse collection of challenging tasks has been an enduring goal of artificial intelligence. The promise of machine learning as a means for achieving this has been frustrated by the need to craft problem-specific representations. DeepMind’s DQN stands as a major step forward by demonstrating that a single agent can learn problem-specific features enabling it to acquire humancompetitive skills over a range of tasks. This demonstration did not produce one agent that simultaneously excelled at all the tasks (because learning occurred separately for each task), but it showed that deep learning can reduce, and possibly eliminate, the need for problem-specific design and tuning.</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol><li><p><strong>Features的提取</strong>: 当task中的状态数目超过一定数量时，必须采用Function Approximation（FA）的方法，这意味着需要将 $state$ 转换为 $feature$，这是一个non-trivial的问题。有没有一个end2end普适性特征提取方法吗？</p><p>这篇文章是在2015年发表的，那几年，许多人都尝试将神经网络引入他们的领域，比如 <a href="https://arxiv.org/abs/1311.2524">RCNN</a>。CNN非常善于从图像中提取特征，和DQN的setting完美吻合。但是，将CNN和FA结合起来，做<strong>端到端</strong>的训练并不容易，神经网络会引入很多问题。</p></li><li><p><strong>CNN的训练</strong>：CNN的训练是mini-batch方式的，而强化学习的场景一般是online的（边探索边学习）。如何将online转化为batch learning 呢？最naive的想法：在agent探索的时候，每隔32个step，收集32个$(state, action) \rightarrow Q$ 作为一个batch训练网络，然而这种时间上强关联的样本，会误导神经网络，让CNN的训练十分困难。</p><p>DQN为了解决这个问题，引入了1） <a href="http://www.incompleteideas.net/lin-92.pdf">Experience replay</a> 打散同一个batch种相邻样本的相关性。</p><p>其次，为了解决Q-learning中的semi-gradient，提出了2）Target Network。</p><p>在附录的Ablation Study中，DQN作者展示了1）和 2）的重要性：<em>（图中为每个episode的平均得分，越大越好）</em></p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210714204316341.png" alt="image-20210714204316341"></p><p><em>(注：关于Target Networ</em>k的作用，DDPG论文中也有做对比实验，感兴趣可以结合着看)</p></li></ol><ol><li><strong>实验场景</strong>：Atari 2600对人来说具有一定的游戏难度，人的分数不会太高（所以论文做到了Human Level，发在了Nature上）；图像大小比较小，不需要太深的网络就可处理，训练难度较低。</li></ol><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210715002850996.png" alt="image-20210715002850996"></p><h3 id="算法重点"><a href="#算法重点" class="headerlink" title="算法重点"></a>算法重点</h3><h4 id="1-Experience-Replay"><a href="#1-Experience-Replay" class="headerlink" title="1. Experience Replay"></a>1. Experience Replay</h4><p><strong>概念</strong>：在传统的Q-learning中，智能体（agent）在 $s_t$ 中执行 $a_t$ ，得到Reward $r_t$，和下一个状态 $s_{t+1}$。此时根据 Q-learning的更新公式有：</p><script type="math/tex; mode=display">Q(s_t, a_t;\theta) = Q(s_t, a_t;\theta) + \alpha \left(r_t + \gamma * max_{a}Q(s_{t+1}, a;\theta) - Q(s_t, a_t;\theta)\right) \\</script><p>将 $e_t = (s_t, a_t, r_t, s_{t+1})$ 带入上式更新 $\theta$（$e_t$ 为t时刻的transition或experience）。</p><p>然而在 Experience Replay 中，智能体在 $s_t$ 中执行 $a_t$ ，得到Reward $r_t$，和下一个状态 $s_{t+1}$。却并不是直接将 $ e_t = (s_t, a_t, r_t, s_{t+1})$ 带入上式，而是将 $e_t$ 放入一个固定大小的experience pool $D$ 中；然后，从 $D$ 中 均匀随机采样出 $B$ 个 $e$ 作为一个batch，比如$\{e_{1984}, e_{38711}, e_{230},…. \}$ ，代入上式。</p><p><strong>淘汰</strong>：既然 $D$ 是固定大小的，那当 $D$ 满了之后，需要淘汰最早进入 $D$ 的transition。</p><p><strong>优势</strong>：Experience Replay的优势论文中提到三点：</p><ol><li>重复使用 Transition，提高data effienciy。传统的Q-learning中，transition使用一次便丢弃了。</li><li>打散连续transition之间的相关性，同一个batch中的transition都是时间上不相邻的。如果同一个batch之间是相邻的，会影响网络的训练。</li></ol><p><strong>off-policy</strong>：Experience Replay 只能用在在off-policy算法上上。on-policy中，当我们利用transition $e_t$更新policy时，$e_t$是同一个policy所生成的。然而在Experience Replay的更新中，每次用很久之前的某个transition $e_j$更新 $Q$， 而生成 $e_j$ 的 policy早已不是当前的policy了，在从 $j \rightarrow t$的这段时间中，policy已经改变了 $t-j$ 次了（因为policy本质上就是 $Q$ ，而 $Q$ 一直都在变）。自然而然，这篇paper选用了一个off policy的算法，Q-learning。</p><h4 id="2-Target-Network"><a href="#2-Target-Network" class="headerlink" title="2. Target Network"></a>2. Target Network</h4><p>在 Sutton 的书中，提到了 semi-gradient Q-learning。下式为Q-learning的Loss Function：</p><script type="math/tex; mode=display">L(\theta) = \frac{1}{2} \left(r_t + \gamma * max_{a}Q(s_{t+1}, a;\theta) - Q(s_t, a_t;\theta)\right)^2</script><p>如果类比监督学习，那么groundtruth $y_t = r_t + \gamma * max_{a}Q(s_{t+1}, a;\theta)$，但是与监督学习不一样，$y$ 中还涉及了 $\theta$ 。论文中提出的解决方式是用一个target Network（参数为 $\theta^{-}$）替换 $y$ 中的网络 （参数为 $\theta^{}$），这样一来，loss function变为了如下形式：</p><script type="math/tex; mode=display">L(\theta) = \frac{1}{2} \left(r_t + \gamma * max_{a}Q(s_{t+1}, a;\theta^{-}) - Q(s_t, a_t;\theta)\right)^2</script><p>现在 $y_t$ 与 $\theta$ 没有关系了，可以按照监督学习的那一套更新 $\theta$。</p><p>（每隔 $C = 10000$ 个 timestep，更新 target Network中的值：$\theta = \theta^{-}$） </p><p>（注：DDPG中对target network进行了一个小的改动（soft target network））</p><h3 id="其他细节"><a href="#其他细节" class="headerlink" title="其他细节"></a>其他细节</h3><h5 id="1-预处理"><a href="#1-预处理" class="headerlink" title="1. 预处理"></a>1. 预处理</h5><p>在t时刻，得到210x160x3的RGB图像 $x_t$，然后进行如下转换：1）变为灰度图像 210x160x1 2）resize为 84x84的图像 3）与 $x_{t-1} x_{t-2}, x_{t-3}$ 的图像stack到一起，得到 84x84x4 ndarray，称之为 $\phi(x_t)$。 $\phi(x_t)$ 就是每一步的状态 $s_t$</p><h5 id="2-Error-Clamp"><a href="#2-Error-Clamp" class="headerlink" title="2. Error Clamp"></a>2. Error Clamp</h5><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210709013714030.png" alt="image-20210709013714030"></p><h3 id="完整的算法"><a href="#完整的算法" class="headerlink" title="完整的算法"></a>完整的算法</h3><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210714231116502.png" alt=""></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>具体的数据这里就不放了，感兴趣的可以去看论文，这里重点说一下DQN的可视化以及不足之处。</p><h3 id="embedding可视化"><a href="#embedding可视化" class="headerlink" title="embedding可视化"></a>embedding可视化</h3><p>作者用t-SNE可视化了embedding（last hidden layer的输出）。在CNN分类中，一般相同类别的图像的embedding相近，在DQN中也是如此，这可以佐证DQN的网络是有意义的，提取到了不错的特征：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210714205530856.png" alt="image-20210714205530856"></p><h3 id="DQN的不足"><a href="#DQN的不足" class="headerlink" title="DQN的不足"></a>DQN的不足</h3><p><strong>Sparse Rewards</strong> 总的来说，DQN在大部分游戏上表现得还不错，但是在某些游戏上得表现并不比random player要好，比如<a href="https://www.retrogames.cz/play_124-Atari2600.php?language=EN">Montezuma’s Revenge</a>。跟breakout不同，这是个相当难的游戏，任何小的失误都会导致死亡，这意味着agent刚开始学习时很难获得reward。在漫长的学习过程中，agent获得的奖励是相当稀疏的(sparse rewards)[2]。</p><p>Sparse rewards问题是Deep RL 难以解决的问题。不过，后续有一些工作利用human demonstrations，不再让agent自己探索环境，而是让网络模仿人类学习[3] [4]。</p><p><strong>大量内存：</strong>  Replay Memory需要大量空间资源</p><p><strong>off-policy：</strong> 由于Experience Replay用的是old-policy的sample更新当前的policy，所以原则上 Experience Replay只能使用off-policy算法。</p><h3 id="复现-amp-结果"><a href="#复现-amp-结果" class="headerlink" title="复现 &amp; 结果"></a>复现 &amp; 结果</h3><p>代码在我的github：<a href="https://github.com/LamForest/RL-learning/blob/master/DQN/Deep%20Q%20Learning.ipynb">https://github.com/LamForest/RL-learning/blob/master/DQN/Deep%20Q%20Learning.ipynb</a></p><p>刚开始训练的网络：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/DQN2.gif" alt="DQN2"></p><p>训练6000个episode后的网络</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/DQN1.gif" alt="DQN1"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1]  <a href="https://stats.stackexchange.com/a/265001">https://stats.stackexchange.com/a/265001</a></p><p>[2] <a href="https://awjuliani.medium.com/on-solving-montezumas-revenge-2146d83f0bc3">https://awjuliani.medium.com/on-solving-montezumas-revenge-2146d83f0bc3</a></p><p>[3] Playing hard exploration games by watching YouTube <a href="https://arxiv.org/pdf/1805.11592.pdf">https://arxiv.org/pdf/1805.11592.pdf</a>    </p><p>[4] Observe and Look Further: Achieving Consistent Performance on Atari <a href="https://arxiv.org/pdf/1805.11593.pdf">https://arxiv.org/pdf/1805.11593.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> python </tag>
            
            <tag> Model-Free </tag>
            
            <tag> Deep Q-Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[强化学习][python] 在 tic-tac-toe 上实现 蒙特卡洛搜索树 MCTS 算法</title>
      <link href="2021/07/20/ml/qiang-hua-xue-xi/meng-te-qia-luo-sou-suo-shu/"/>
      <url>2021/07/20/ml/qiang-hua-xue-xi/meng-te-qia-luo-sou-suo-shu/</url>
      
        <content type="html"><![CDATA[<h2 id="前置知识-UCB"><a href="#前置知识-UCB" class="headerlink" title="前置知识 UCB"></a>前置知识 UCB</h2><p>强化学习的核心问题之一是 探索&amp;利用 问题。最简单的解决方法是 $\epsilon-greedy$ 。 UCB是比 $\epsilon-greedy$  稍微复杂一点的方法，对于UCB而言，其选择下一个动作的方式为：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210716180827344.png" alt=""></p><blockquote><p> 注意到，不同于 $\epsilon-greedy$ 会产生一个概率分布，然后用 <code>np.random.choice</code> 从其中sample一个动作；UCB是determinstic的，每次选择的动作都是确定的。</p></blockquote><p>观察上式，可以分为两部分：</p><ul><li><strong>exploitation</strong>：$argmax_a{Q_t(a)}$ 就是贪心算法。</li><li><strong>exploration</strong>：$c \sqrt{\frac{\ln t}{N_t(a)}}$ 用于衡量动作 $a$ 的 $Q$值的不确定性。  （ c是超参，一般选 $\sqrt2$，t是iteration次数，$N_t(a)$ 是t时刻动作a被选中的次数）。</li></ul><blockquote><p>可以这么理解 $c \sqrt{\frac{\ln t}{N_t(a)}}$ ： 它衡量了 $Q_t(a)$ 的不确定性， 当 $N_t(a)$ 趋向于无穷大时，根据大数定律，$Q_t(a)$ 是准确的， 所以此时$c \sqrt{\frac{\ln t}{N_t(a)}}$也正好趋向于0。详细的解释可以参看 <a href="https://zhuanlan.zhihu.com/p/32356077">https://zhuanlan.zhihu.com/p/32356077</a></p></blockquote><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>(来源：<a href="https://vgarciasc.github.io/mcts-viz/">tic-tac-toe MCTS可视化 </a>  强烈推荐)</p><p>以tic-tac-toe为例，人（下图中的h）走先手。</p><p>对于左子结点，$ucb = Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} = 1 + \sqrt2 \sqrt{\frac{\ln 2}{1}} \approx 2.18$ </p><p>对于右子结点而言，$ucb = Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} = -1 + \sqrt2 \sqrt{\frac{\ln 2}{1}} \approx 0.18$</p><p> <img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210716182648833.png" alt="image-20210716182648833"></p><h2 id="MCTS算法"><a href="#MCTS算法" class="headerlink" title="MCTS算法"></a>MCTS算法</h2><p>MCTS分为 selection，expansion，simulation，backup。</p><p>我在tic-tac-toe上实现了MCTS，该实现参考了<a href="http://www.incompleteideas.net/609%20dropbox/other%20readings%20and%20resources/MCTS-survey.pdf">A Survey of Monte Carlo Tree Search Methods</a> 中的算法，代码见 <a href="https://github.com/LamForest/machine-learning-reimplementation">github (仍在整理中)</a> 。</p><p>在开始叙述MCTS算法之前，先明确几点:</p><ul><li>MCTS是一个 Decision-time Planning, Rollout Algorithm。这代表 MCTS在遇到每个state之后，为该state做多趟蒙特卡洛模拟，估计出各个action的state-value（即 $Q$） 值之后，选择某个动作，然后丢弃刚学到的所有 state-value值。</li><li>MCTS的数据结构是一颗树，树的每一个结点是一个state，边代表动作，子结点是父结点的afterstate，根结点是智能体当前所处的state。</li><li>对于井字棋，$Q$ 的意义是选择这个action之后，赢的平均概率。</li></ul><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210720185116147.png" alt="MCTS 概念图 From Sutton Book 8.1"></p><h3 id="Selection"><a href="#Selection" class="headerlink" title="Selection"></a>Selection</h3><p>selection：通过Tree-Policy从树中选择一个结点进行expansion。</p><p>这里的Tree-Policy选择的是前置知识中的UCB：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210720191801324.png" alt="image-20210720191801324"></p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210720191814652.png" alt="image-20210720191814652"></p><p>（这里的 $Q$ 指的是累计奖赏，不是平均奖赏，不过区别不大）</p><h4 id="例子-1"><a href="#例子-1" class="headerlink" title="例子"></a>例子</h4><p>在下图中，人走了第一步，现在agent需要做MCTS走第二步。</p><p>在第11次蒙特卡洛时，由于根结点Fully Expanded了，所以选择UCB最大的，即第一个子结点。该子结点是not fully expanded的，所以选择该子结点（下图中红色的结点）</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210716183014526.png" alt="image-20210716183014526"></p><h3 id="Expansion"><a href="#Expansion" class="headerlink" title="Expansion"></a>Expansion</h3><p>Expansion：对于selection选中的结点，生成它的一个子结点，如果有多个可能子结点，随机选择一个。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210720192255614.png" alt="image-20210720192255614"></p><p>（这一步没什么好说的，单纯的new一个结点）</p><h3 id="Simulation"><a href="#Simulation" class="headerlink" title="Simulation"></a>Simulation</h3><p>Simulation：以生成的子结点 $v^{\prime}$ 为起点，利用Rollout Policy（也叫 Default Policy）选择动作，并进入下一个结点，直到达到终止状态，得到reward。对于tic-tac-toe，reward为 -1，0，1。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210720192621210.png" alt="image-20210720192621210"></p><ul><li>Simulation 与 Selection都用了Policy，但是Simulation使用的是 更简单的Policy，比如均匀随机选择一个动作。</li></ul><h3 id="Backup"><a href="#Backup" class="headerlink" title="Backup"></a>Backup</h3><p>Backup: 从根结点到新生成的结点，在树中形成了一条路径。 backup所做的，就是利用simulation得到的reward，更新这条路径上的所有结点（包括根结点和新结点）。</p><p>需要更新的有：</p><ul><li>$N(v)$：状态 $v$ 被backup的总次数。</li><li>$Q(v)$：从 $v$ 出发，获得的累计总奖赏。</li></ul><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210720193046997.png" alt="image-20210720193046997"></p><p>这里特别注意的是，对于tic-tac-toe这种两人游戏，假设根结点处在第1层，那么奇数层是站在智能体的角度，偶数层站在对手的角度。当站在智能体的角度，reward不需要修改；当站在对手的角度，<code>reward = - reward</code>。这里实现的时候要小心。</p><h3 id="决策"><a href="#决策" class="headerlink" title="决策"></a>决策</h3><p>在利用蒙特卡洛模拟学习出了 $Q$ 之后，接着要选择一个动作，选择的方式可以有很多种，这里还是根据UCB的值来选择，选择UCB最大的那个action。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210720195140305.png" alt="image-20210720195140305"></p><h2 id="MCTS玩tic-tac-toe"><a href="#MCTS玩tic-tac-toe" class="headerlink" title="MCTS玩tic-tac-toe"></a>MCTS玩tic-tac-toe</h2><p>下面分别在人走先手和agent走先手的情况下，演示了我实现的MCTS：</p><h3 id="人走先手"><a href="#人走先手" class="headerlink" title="人走先手"></a>人走先手</h3><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210720194609634.png" alt="image-20210720194609634"></p><p>最后打成了平局。</p><h3 id="MCTS-Agent走先手"><a href="#MCTS-Agent走先手" class="headerlink" title="MCTS Agent走先手"></a>MCTS Agent走先手</h3><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210720194737508.png" alt=""></p><p>MCTS Agent赢了（人放了点水）</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><h3 id="迭代次数的影响"><a href="#迭代次数的影响" class="headerlink" title="迭代次数的影响"></a>迭代次数的影响</h3><p>本以为对于tic-tac-toe这样简单的游戏，迭代次数不需要特别大，然而似乎并不是这样，当迭代次数分别为1000，5000，10000时，MCTS给出的最佳走法有概率不一致。</p><p>（我检查了好几遍我的代码，没找到问题所在，不知道是我的实现问题，还是MCTS的缺陷）</p><p>比如，当Human走先手，下了这一步棋之后：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210717213614947.png" alt="image-20210717213614947"></p><p>现在MCTS需要计算这个state下，最优的下法，当MCTS的迭代次数为1000时，重复100次MCTS算法，MCTS有 77次选择中间这一步，23次选择其他位置。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210717214259384.png" alt="image-20210717214259384"></p><p>当迭代次数为 5000 的时候， MCTS有97次选择中间这一步，3次选择其他位置。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210717214425183.png" alt="image-20210717214425183"></p><p>当迭代次数为 10000 的时候， MCTS有99次选择中间这一步，1次选择其他位置。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210718033026438.png" alt="image-20210718033026438"></p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> python </tag>
            
            <tag> MCTS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[C++][标准库] 随机数的生成方式、性能对比、mingw的问题</title>
      <link href="2021/07/08/cpp/c-biao-zhun-ku-sui-ji-shu-de-sheng-cheng-fang-shi-xing-neng-dui-bi-mingw-de-wen-ti/"/>
      <url>2021/07/08/cpp/c-biao-zhun-ku-sui-ji-shu-de-sheng-cheng-fang-shi-xing-neng-dui-bi-mingw-de-wen-ti/</url>
      
        <content type="html"><![CDATA[<h3 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h3><p>本篇文章包含了以下内容：</p><ul><li>随机数的两种生成方式：HRNG 、PRNG</li><li>HRNG 和 PRNG 在 C++中的执行时间的比较</li><li>mingw 的 <code>std::random_device</code> 存在的问题</li></ul><h3 id="随机数的生成方式"><a href="#随机数的生成方式" class="headerlink" title="随机数的生成方式"></a>随机数的生成方式</h3><blockquote><p>参考资料：</p><p>[1] <a href="https://en.wikipedia.org/wiki/Random_number_generation">https://en.wikipedia.org/wiki/Random_number_generation</a></p><p>[2] <a href="https://www.zhihu.com/question/20423025/answer/15097735">电脑取随机数是什么原理，是真正的随机数吗？ - 王納米的回答 - 知乎</a></p></blockquote><p>随机数生成（Random Number Generation, RNG）的方式一般有两种，分别为：</p><ul><li><p>硬件生成随机数<a href="https://en.wikipedia.org/wiki/Hardware_random_number_generator">Hardware RNG</a>，原理是用某个仪器一直探测环境中的物理量，将该物理量作为随机数，见[2]。由于人类目前还无法对真实的物理环境进行建模，所以无从预测下一个产生的随机数是什么。因此，HRNG可以看作真随机数，[2]中给出了一个具体的例子。</p><p>另一个例子是 Intel 和 AMD CPU指令集中的 <a href="https://en.wikipedia.org/wiki/RDRAND">RDRAND</a> 指令，该指令基于clock shift生成随机数：</p><blockquote><p>One way to build a <a href="https://en.wikipedia.org/wiki/Hardware_random_number_generator#Clock_drift">hardware random number generator</a> is to use two independent <a href="https://en.wikipedia.org/wiki/Crystal_oscillator">clock crystals</a>, one that for instance ticks 100 times per second and one that ticks 1 million times per second. </p><p>On average the faster crystal will then tick 10,000 times for each time the slower one ticks. <strong>But since clock crystals are not precise, the exact number of ticks will vary.</strong> <strong>That variation can be used to create random bits.</strong> </p><p>For instance, if the number of fast ticks is even, a 0 is chosen, and if the number of ticks is odd, a 1 is chosen. Thus such a 100/1000000 RNG circuit can produce 100 somewhat random bits per second. Typically such a system is biased—it might for instance produce more zeros than ones—and so hundreds of somewhat-random bits are <a href="https://en.wikipedia.org/wiki/Decorrelation">“whitened”</a> to produce a few unbiased bits.</p></blockquote></li><li><p>算法生成随机数，比如c++中的 mt19937梅森旋转算法即为一种软件层面的随机数生成算法算法。如果知道了seed和算法的具体实现，别人就可以知道你生成的随机数序列。所以，又被称为 <a href="https://en.wikipedia.org/wiki/Pseudorandom_number_generator">伪随机数生成器 Pseudo RNG</a>。其他的PRNG算法包括 Xorshift，linear congruential generators等。</p></li></ul><h3 id="HRNG-vs-PRNG"><a href="#HRNG-vs-PRNG" class="headerlink" title="HRNG vs PRNG"></a>HRNG vs PRNG</h3><blockquote><p>参考资料：</p><p>[1] <a href="https://stackoverflow.com/questions/39288595/why-not-just-use-random-device">https://stackoverflow.com/questions/39288595/why-not-just-use-random-device</a></p></blockquote><p>从上面的描述中可以看出，硬件生成的随机数似乎更好一些，是真随机数，而软件层面的随机数容易被人破解。但是在C++代码中，人们一般不直接使用HRNG。而是利用HRNG为PRNG生成一个种子，然后利用PRNG生成随机数，比如如下代码：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">std<span class="token operator">::</span>random_device rd<span class="token punctuation">;</span> <span class="token comment">//linux下，读取/dev/random获取硬件产生的随机数</span>std<span class="token operator">::</span>mt19937 e<span class="token punctuation">&#123;</span><span class="token function">rd</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span class="token punctuation">;</span> <span class="token comment">// or std::default_random_engine e&#123;rd()&#125;; 用HRNG作为PRNG的种子</span>std<span class="token operator">::</span>uniform_int_distribution<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> dist<span class="token punctuation">&#123;</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">&#125;</span><span class="token punctuation">;</span><span class="token function">dist</span><span class="token punctuation">(</span>e<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// get random numbers with PRNG</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>为什么不直接使用 <code>std::random_device</code>呢？ 可以从效率和可解释性两个角度解释了这个问题，见[1]，讲的比较清楚，这里我就不过多解释了，只补充一个两者的执行速度的对比。</p><p>这里用 <a href="https://github.com/google/benchmark">google-benchmark</a> 对比了linux（ubuntu 20.04, gcc 9.2.0)下 <code>std::random_device</code> 和 <code>std::mt19937</code> 的执行速度：</p><ul><li><p>所使用的代码如下：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;benchmark/benchmark.h></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;random></span></span><span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">BM_PRNG</span><span class="token punctuation">(</span>benchmark<span class="token operator">::</span>State<span class="token operator">&amp;</span> state<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    std<span class="token operator">::</span>random_device rd<span class="token punctuation">;</span>    std<span class="token operator">::</span>mt19937 e<span class="token punctuation">&#123;</span><span class="token function">rd</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">auto</span> _ <span class="token operator">:</span> state<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>        <span class="token function">e</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span class="token function">BENCHMARK</span><span class="token punctuation">(</span>BM_PRNG<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">BM_HRNG</span><span class="token punctuation">(</span>benchmark<span class="token operator">::</span>State<span class="token operator">&amp;</span> state<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    std<span class="token operator">::</span>random_device rd<span class="token punctuation">;</span>    std<span class="token operator">::</span>uniform_int_distribution<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> <span class="token function">dist</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">auto</span> _ <span class="token operator">:</span> state<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>        <span class="token function">rd</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span class="token function">BENCHMARK</span><span class="token punctuation">(</span>BM_HRNG<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">BENCHMARK_MAIN</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>运行结果如下：</p><pre class="line-numbers language-none"><code class="language-none">Run on (1 X 2494.14 MHz CPU )CPU Caches:  L1 Data 32 KiB (x1)  L1 Instruction 32 KiB (x1)  L2 Unified 4096 KiB (x1)  L3 Unified 28160 KiB (x1)Load Average: 2.70, 3.15, 16.35-----------------------------------------------------Benchmark           Time             CPU   Iterations-----------------------------------------------------BM_PRNG          35.9 ns         17.8 ns     39254461 BM_HRNG          1310 ns          640 ns      1091579<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><ul><li>“真”随机数的耗时大概是“伪”随机数的 37倍，非常之慢。</li></ul><p>这非常类似非对称加密和对称加密，非对称加密（RSA）通常不用于直接加密信息，而是用于加密并交换对称密钥，然后用对称密钥（比如AES-256）交换要传输的信息，因为对称密钥加密解密的速度更快，但是相对更不安全。</p><h3 id="mingw-中-std-random-device的问题"><a href="#mingw-中-std-random-device的问题" class="headerlink" title="mingw 中 std::random_device的问题"></a>mingw 中 std::random_device的问题</h3><p>分别在GCC，MSVC，Mingw中执行下述代码：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;random></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;iostream></span></span><span class="token keyword">void</span> <span class="token function">test</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    std<span class="token operator">::</span>random_device rd<span class="token punctuation">;</span>    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token function">rd</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;&lt;</span> <span class="token string">", "</span><span class="token punctuation">;</span>    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token function">rd</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;&lt;</span> <span class="token string">", "</span><span class="token punctuation">;</span>    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token function">rd</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token function">test</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">test</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">test</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="msvc"><a href="#msvc" class="headerlink" title="msvc"></a>msvc</h4><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210706202721923.png" alt="image-20210706202721923"></p><h4 id="gcc"><a href="#gcc" class="headerlink" title="gcc"></a>gcc</h4><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210706203142452.png" alt="image-20210706203142452"></p><h4 id="Mingw-8-1-0"><a href="#Mingw-8-1-0" class="headerlink" title="Mingw 8.1.0"></a>Mingw 8.1.0</h4><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210706203235855.png" alt="image-20210706203235855"></p><hr><p>显然，gcc和msvc下的random_device每次都产生了不一样的随机数序列。</p><p>然而，mingw下的random_device每次都产生了同样的序列（deterministic）。</p><blockquote><p>A notable implementation where <code>std::random_device</code> is deterministic is old versions of MinGW (<a href="https://sourceforge.net/p/mingw-w64/bugs/338/">bug 338</a>, fixed since GCC 9.2). The latest MinGW Versions can be downloaded from <a href="https://gcc-mcf.lhmouse.com/">GCC with the MCF thread model</a>.</p><p>From: <a href="https://en.cppreference.com/w/cpp/numeric/random/random_device">https://en.cppreference.com/w/cpp/numeric/random/random_device</a></p></blockquote><p>幸运的是，这个问题在mingw 9.2中被修复了。</p><blockquote><p>Add support for additional sources of randomness to std::random_device,<br>to allow using RDSEED for Intel CPUs and rand_s for Windows. When<br>supported these can be selected using the tokens “rdseed” and “rand_s”.<br>For <em>-w64-mingw32 targets the “default” token will now use rand_s, and<br>for other i?86-</em>-<em> and x86_64-</em>-* targets it will try to use “rdseed”<br>first, then “rdrand”, and finally “/dev/urandom”.</p><p>From :  <a href="https://patchwork.ozlabs.org/project/gcc/patch/20190529144517.GA9078@redhat.com/">https://patchwork.ozlabs.org/project/gcc/patch/20190529144517.GA9078@redhat.com/</a></p></blockquote><p>这算mingw的一个bug吗？实际上，并不算，因为C++标准过于宽松，它允许random_device每次产生同样的随机数序列：</p><blockquote><p><code>std::random_device</code> may be implemented in terms of an implementation-defined pseudo-random number engine if a non-deterministic source (e.g. a hardware device) is not available to the implementation. In this case each <code>std::random_device</code> object may generate the same number sequence.</p><p>From: <a href="https://en.cppreference.com/w/cpp/numeric/random/random_device">https://en.cppreference.com/w/cpp/numeric/random/random_device</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> Template </tag>
            
            <tag> C++ STL </tag>
            
            <tag> GNU C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我的布偶猫灰原</title>
      <link href="2021/06/17/others/hui-yuan-de-cheng-chang-ri-ji/"/>
      <url>2021/06/17/others/hui-yuan-de-cheng-chang-ri-ji/</url>
      
        <content type="html"><![CDATA[<p>我们有了一个小猫咪，是一只蓝双布偶猫妹妹！<br><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/20210617021816.png" alt=""></p><h2 id="灰原的档案"><a href="#灰原的档案" class="headerlink" title="灰原的档案"></a>灰原的档案</h2><ul><li>名字：灰原<br><img src="https://img.mix.sina.com.cn/auto/resize?img=https%3A%2F%2Fn.sinaimg.cn%2Fsinakd10100%2F416%2Fw640h576%2F20200416%2Fe7e2-iskepxs3899437.jpg&size=640_0&blur=1&blur_sigma=2" width="20%" height="20%"></li><li>性别：女</li><li>生日：2021年3月30日</li><li>爱好：玩绳子，鞋带，电源线，拖鞋，逗猫棒，翻垃圾桶，晒太阳，</li><li>体重：</li></ul><div class="table-container"><table><thead><tr><th>日期</th><th>2021年6月17号</th><th>2021年6月24号</th><th>2021年7月1号</th></tr></thead><tbody><tr><td>体重</td><td>0.9kg</td><td>1.05kg （+0.15 kg)</td><td>1.25kg （+0.20 kg)</td></tr></tbody></table></div><p> <img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/20210624001939.png" width="40%" height="40%"></p><ul><li>长度：2021年6月15日 大概32cm（从头到尾巴根部）</li></ul><div class="table-container"><table><thead><tr><th>日期</th><th>2021年6月17号</th><th>2021年6月24号</th></tr></thead><tbody><tr><td>长度</td><td>31cm</td><td>33cm (+ 2cm)</td></tr></tbody></table></div><ul><li>食物：皇家猫奶糕、鸡胸肉、熟鸡蛋黄</li></ul><h2 id="一些照片"><a href="#一些照片" class="headerlink" title="一些照片"></a>一些照片</h2><h3 id="2021年6月17日"><a href="#2021年6月17日" class="headerlink" title="2021年6月17日"></a>2021年6月17日</h3><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/20210617021900.png" alt=""><br><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/70d2a7561c16ce82f090638910f1301.jpg" alt=""><br><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/045e4d296083a2b1bd0590ecb5cd3c2.jpg" alt=""><br><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/20210617022249.png" alt=""><br><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/20210617022322.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 灰原 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 灰原 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>アークナイツ</title>
      <link href="2021/05/26/others/ming-ri-fang-zhou-ji-lu/"/>
      <url>2021/05/26/others/ming-ri-fang-zhou-ji-lu/</url>
      
        <content type="html"><![CDATA[<h3 id="5-23"><a href="#5-23" class="headerlink" title="5-23"></a>5-23</h3><p>新手池2抽，出了2次洁哥</p><p><img src="https://gitee.com/getleft/pics/raw/master/cs229/image-20210526135953342.png" alt="image-20210526135953342"></p><p><img src="https://gitee.com/getleft/pics/raw/master/cs229/image-20210526135937644.png" alt="image-20210526135937644"></p><p>联合池单抽了几次，出了温蒂（前期用不上，蛋疼）</p><p><img src="https://gitee.com/getleft/pics/raw/master/cs229/image-20210526140052834.png" alt="image-20210526140052834"></p><h3 id="5-24"><a href="#5-24" class="headerlink" title="5-24"></a>5-24</h3><p>第一次出资深、高级资深tag，竟然还是一起出的，只选了高级资深赌一把，没想到还是莫斯提马：</p><p><img src="https://gitee.com/getleft/pics/raw/master/cs229/image-20210526140229678.png" alt="image-20210526140229678"></p><p><img src="https://gitee.com/getleft/pics/raw/master/cs229/image-20210526140240185.png" alt="image-20210526140240185"></p><h3 id="5-25"><a href="#5-25" class="headerlink" title="5-25"></a>5-25</h3><p>没忍住，十抽了联合寻访，想出棘刺，却出了铃兰，也还行吧。。。就是不太用得上，赫墨也重复了</p><p><img src="https://gitee.com/getleft/pics/raw/master/cs229/image-20210526140356752.png" alt="image-20210526140356752"></p><h3 id="5-26"><a href="#5-26" class="headerlink" title="5-26"></a>5-26</h3><p>今天出了几个好tag，再梅尔和凛冬之间选了凛冬，希望不会后悔。</p><p>（忘记截图了）</p>]]></content>
      
      
      <categories>
          
          <category> アークナイツ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> アークナイツ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAMES202作业2旋转部分</title>
      <link href="2021/05/05/cg/games202-zuo-ye-2-xuan-zhuan-bu-fen/"/>
      <url>2021/05/05/cg/games202-zuo-ye-2-xuan-zhuan-bu-fen/</url>
      
        <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>本文是写在做完<a href="https://www.bilibili.com/video/BV1YK4y1T7yY">GAMES202 高质量实时渲染</a>作业2之后的总结。通过不严谨的数学公式，从球谐的旋转不变性出发，一步步推导出如何求旋转后的系数${c_l^m}^{\prime}$。</p><blockquote><p>非常感谢闫老师和助教们的课程和代码框架！！！</p></blockquote><p>效果展示：</p><p>(注：环境光为Cornell Box)</p><p><img src="https://gitee.com/getleft/pics/raw/master/imgs/%E6%AD%A3%E7%A1%AE%E7%9A%84%E6%97%8B%E8%BD%AC%E6%95%88%E6%9E%9C_small.gif" alt=""></p><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>定义在球面上的环境光 $L_i(\omega_i)$可以被展开成球谐基函数的线性组合：</p><script type="math/tex; mode=display">L_i(\omega_i) = \sum_{l  = 0}^{+\infty} \sum_{m  = -l}^{l} c_l^mB_l^m(\omega_i)</script><p>假设现在有一个旋转矩阵 $R^{-1}$，现在有一个旋转矩阵，将环境光 $L_i(\omega_i)$进行了旋转，得到了 $L_i(R\omega_i)$。</p><blockquote><p>注：将$y = x^2$向x轴正半轴方向移动了a个单位后，变为了$y = (x-a)^2$，而不是$y = (x+a)^2$，旋转也是这个道理。</p></blockquote><p><strong>问题来了</strong>，<strong>如何求得 $L_i(R\omega_i)$ 的球谐系数${c_l^m}^{\prime}$？</strong></p><p>最简单的想法，重新计算 $L_i(R\omega_i)$ 对每个球谐基函数的投影：</p><script type="math/tex; mode=display">{c_l^m}^{\prime} = \int_{\Omega} B_l^m(\omega_i) L_i(R\omega_i) d\omega_i</script><p>假如环境光源在每一帧都在旋转，那么每一帧渲染之前，都要进行这么一个积分，这是无法接受的。</p><blockquote><p>回顾之前的内容，该积分使用的是黎曼和的形式求解，复杂度很高。</p></blockquote><p>是否有更简单的方法计算${c_l^m}^{\prime}$ 呢？</p><p><strong>比如，是否存在某个方法，可以直接从${c_l^m}$得到${c_l^m}^{\prime}$呢？</strong></p><p>这就是本文探讨的话题。</p><h2 id="什么是旋转不变性"><a href="#什么是旋转不变性" class="headerlink" title="什么是旋转不变性"></a>什么是旋转不变性</h2><p>对于某个球谐基函数$B_l^m(\omega_i)$，假设现在有一个旋转矩阵 $R^{-1}$ 将$B_l^m(\omega_i)$进行了旋转，</p><blockquote><p>注：旋转应该是指，按照空间中过原点的某一个轴旋转了某个角度，由于环境光是无限远的，所以转轴虽然在空间中任意位置，但可以视作在原点。</p></blockquote><p>那么旋转后的函数的表达式为$B_l^m(R^{}\omega_i)$。</p><p><strong>旋转不变性</strong>指的是，$B_l^m(R\omega_i)$可以被拆分成同阶（band）的其他基函数的线性组合，而与其他阶的基函数无关，即：</p><script type="math/tex; mode=display">B_l^m(R\omega_i) = \sum_{k  = -l}^{l} a_k B_l^m(\omega_i) \tag 1</script><h2 id="从旋转不变性出发，开始推导"><a href="#从旋转不变性出发，开始推导" class="headerlink" title="从旋转不变性出发，开始推导"></a>从旋转不变性出发，开始推导</h2><p>根据球谐展开的概念，对环境光 $L_i(\omega_i)$，我们有：</p><script type="math/tex; mode=display">\begin{align}L_i(\omega_i) &= \sum_{l  = 0}^{+\infty} \sum_{m  = -l}^{l} c_l^mB_l^m(\omega_i) \\\end{align}</script><p>在这篇文章里，我们关注 $l = 1$的情况，忽略其他阶，则上式可以写成：</p><script type="math/tex; mode=display">\begin{align}L_i(\omega_i) &= ... + c_1^{-1}B_1^{-1}(\omega_i) + c_1^{0}B_1^{0}(\omega_i) + c_1^{1}B_1^{1}(\omega_i) +...\end{align}</script><p>在继续进行之前，要先达成一个共识（我不确定这是否需要证明）：</p><ul><li><p>考虑这么一件事情：如果将 $L_i(R\omega_i)$在 $B_l^m(R\omega_i)$上进行分解，所得到的球谐系数是多少呢？</p></li><li><p>由于我们将光源和球谐基函数同时用 $R$ 进行了旋转，那么它们之间的相对关系应该是不会发生变化的，也就是球谐系数仍然是 $c_l^m$：</p></li></ul><script type="math/tex; mode=display">c_l^m = \int_{\Omega} B_l^m(R\omega_i) L_i(R\omega_i) d\omega_i</script><p>有了这个共识，我们就可以直接获得 $L_i(R\omega_i)$在 $B_l^m(R\omega_i)$上进行分解的球谐系数，就是${c_l^m}$（同样只关注 第1阶）：</p><script type="math/tex; mode=display">\begin{align}L_i(R\omega_i) &= ... + c_1^{-1}B_1^{-1}(R\omega_i) + c_1^{0}B_1^{0}(R\omega_i) + c_1^{1}B_1^{1}(R\omega_i) + ... \tag 2\end{align}</script><p>有了公式（2）后，借助球谐函数的旋转不变性（式1），我们可以将$B_l^m(R\omega_i)$ 表示为同band其他基函数的线性组合：</p><script type="math/tex; mode=display">\begin{align}B_1^{-1}(R\omega_i) &= \sum_{k  = -1}^{1} x_k B_1^k(\omega_i) =  x_{-1}B_1^{-1}(\omega_i) + x_{0}B_1^{0}(\omega_i) + x_{1}B_1^{1}(\omega_i) \tag 3\\B_1^{0}(R\omega_i) &= \sum_{k  = -1}^{1} y_k B_1^k(\omega_i) =  y_{-1}B_1^{-1}(\omega_i) + y_{0}B_1^{0}(\omega_i) + y_{1}B_1^{1}(\omega_i)\\B_1^{1}(R\omega_i) &= \sum_{k  = -1}^{1} z_k B_1^k(\omega_i) =  z_{-1}B_1^{-1}(\omega_i) + z_{0}B_1^{0}(\omega_i) + z_{1}B_1^{1}(\omega_i)\end{align}</script><p>假设 $x_k,y_k, z_k$全部已知，那将上面的式子代入式（2），有：</p><script type="math/tex; mode=display">\begin{aligned}L_i(R\omega_i) = ... &+ c_1^{-1}\left(x_{-1}B_1^{-1}(\omega_i) + x_{0}B_1^{0}(\omega_i) + x_{1}B_1^{1}(\omega_i) \right) \\&+ c_1^{0} \left(y_{-1}B_1^{-1}(\omega_i) + y_{0}B_1^{0}(\omega_i) + y_{1}B_1^{1}(\omega_i) \right)\\&+ c_1^{1} \left(z_{-1}B_1^{-1}(\omega_i) + z_{0}B_1^{0}(\omega_i) + z_{1}B_1^{1}(\omega_i) \right)\\&+...\end{aligned}</script><p>进行一下合并同类项：</p><script type="math/tex; mode=display">\begin{align}L_i(R\omega_i) = ... &+ (c_1^{-1}x_{-1} + c_1^{0}y_{-1} + c_1^{1}z_{-1})B_1^{-1}(\omega_i)\\&+ (c_1^{-1}x_{0} + c_1^{0}y_{0} + c_1^{1}z_{0})B_1^{0}(\omega_i)\\&+ (c_1^{-1}x_{1} + c_1^{0}y_{1} + c_1^{1}z_{1})B_1^{1}(\omega_i)\\&+...\end{align}</script><p>令 ${c_1^{-1}}^{\prime} = (c_1^{-1}x_{-1} + c_1^{0}y_{-1} + c_1^{1}z_{-1})$ </p><p>${c_1^{0}}^{\prime} = (c_1^{-1}x_{0} + c_1^{0}y_{0} + c_1^{1}z_{0})$</p><p> ${c_1^{1}}^{\prime} = (c_1^{-1}x_{1} + c_1^{0}y_{1} + c_1^{1}z_{1})$ </p><p>则上式被重写成：</p><script type="math/tex; mode=display">\begin{align}L_i(R\omega_i) = ... + {c_1^{-1}}^{\prime}B_1^{-1}(\omega_i)+ {c_1^{0}}^{\prime}B_1^{0}(\omega_i)+ {c_1^{1}}^{\prime}B_1^{1}(\omega_i)&+...\end{align}</script><p>这样，我们就得到了旋转之后，各个球谐基函数的系数${c_l^m}^{\prime}$了。</p><hr><p>下一节定义了 $M_R$, 利用 $M_R$可以将 ${c_1^{-1}}^{\prime},{c_1^{0}}^{\prime},{c_1^{1}}^{\prime} $用矩阵乘法表示：</p><script type="math/tex; mode=display">\begin{pmatrix}{c_1^{-1}}^{\prime}\\{c_1^{0}}^{\prime}\\{c_1^{1}}^{\prime}\\\end{pmatrix} = {M_R}^T\begin{pmatrix}c_1^{-1}\\c_1^{0}\\c_1^{1}\\\end{pmatrix} \tag 4</script><h2 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h2><p>之前，我们假设 $x_k,y_k, z_k$全部已知，但是它们其实是未知的，接下来我们来求解它们。</p><p>以式（3）为例，在这里，我们要求的是 $x_{-1}, x_0, x_1$，如何求解呢？</p><p>其实很简单，3个未知数，需要3个方程，我们只需要随便找3个 $n_0, n_1, n_2$代入，得到三元一次方程组。因为 $B_l^m(R\omega_i)$是已知的函数，所以上述方程组可以很简单的解出来：</p><script type="math/tex; mode=display">\begin{align}\left\{\begin{array}{**lr**} B_1^{-1}(Rn_0) =   x_{-1}B_1^{-1}(n_0) + x_{0}B_1^{0}(n_0) + x_{1}B_1^{1}(n_0) \\B_1^{-1}(Rn_1) =   x_{-1}B_1^{-1}(n_1) + x_{0}B_1^{0}(n_1) + x_{1}B_1^{1}(n_1) \\B_1^{-1}(Rn_2) =   x_{-1}B_1^{-1}(n_2) + x_{0}B_1^{0}(n_2) + x_{1}B_1^{1}(n_2) \\\end{array}  \tag 5\right.  \end{align}</script><p>同样的，对于 $y_k$ 和 $z_k$，也带入 $n_0, n_1, n_2$，可以解出$y_k$ 和 $z_k$。（求解$y_k$ 和 $z_k$ 时，选取的 $n_0, n_1, n_2$不一定要是一样的，但是选一样的接下来比较方便）。</p><p><strong>式（5）就是求解 $x_k,y_k, z_k$ 的核心。</strong>本节剩下的内容就是对其进行向量化，变成矩阵运算的形式，这样一来我们可以少写一些循环语句，二来运算也会快一些。</p><hr><p>为了求解 $x_k,y_k, z_k$，我们用到了3个三元一次方程组，每个方程组3个方程，这9个方程可以被写成更简洁的矩阵形式（前提是，三个方程组所用的$n_0, n_1, n_2$是一致的）：</p><script type="math/tex; mode=display">\begin{pmatrix}B_1^{-1}(Rn_0) & B_1^{-1}(Rn_1)  & B_1^{-1}(Rn_2)\\B_1^{0}(Rn_0) & B_1^{0}(Rn_1)  & B_1^{0}(Rn_2)\\B_1^{1}(Rn_0) & B_1^{1}(Rn_1)  & B_1^{1}(Rn_2)\\\end{pmatrix} = \begin{pmatrix}x_{-1} & x_{0} & x_1 \\y_{-1} & y_{0} & y_1 \\z_{-1} & z_{0} & z_1 \\\end{pmatrix}\begin{pmatrix}B_1^{-1}(n_0) & B_1^{-1}(n_1)  & B_1^{-1}(n_2)\\B_1^{0}(n_0) & B_1^{0}(n_1)  & B_1^{0}(n_2)\\B_1^{1}(n_0) & B_1^{1}(n_1)  & B_1^{1}(n_2)\\\end{pmatrix}</script><p>为了表述方便，这里定义了 $P(\omega_i)$ （有的地方也称之为对$\omega_i$的投影）和$M_R$：</p><script type="math/tex; mode=display">P(\omega_i) =\begin{pmatrix}B_1^{-1}(\omega_i) \\B_1^{0}(\omega_i) \\B_1^{1}(\omega_i) \\\end{pmatrix}, M_R = \begin{pmatrix}x_{-1} & x_{0} & x_1 \\y_{-1} & y_{0} & y_1 \\z_{-1} & z_{0} & z_1 \\\end{pmatrix}</script><p>利用新定义的符号，可以继续写成更简洁的形式：</p><script type="math/tex; mode=display">\begin{bmatrix}P(Rn_0) & P(Rn_1) & P(Rn_2)  \end{bmatrix} = M_R\begin{bmatrix}P(n_0) & P(n_1) & P(n_2)\end{bmatrix}</script><p>再记 $S = [P(Rn_0) \quad P(Rn_1) \quad  P(Rn_2)] $ ， $A = [P(n_0) \quad P(n_1) \quad P(n_2) ]$ ：</p><script type="math/tex; mode=display">S = M_R A</script><blockquote><p>注：这里 $S, M_R, A$ 都是3x3的方阵。如果我们在处理2阶球谐函数的时候， $S M_R A$ 都变为了5x5的方阵，3阶则7x7，以此类推。</p></blockquote><p>至此， $M_R$ （即$x_k,y_k, z_k$的矩阵形式）的解变得更加一目了然了：</p><script type="math/tex; mode=display">M_R = S A^{-1} \tag 6</script><blockquote><p>式（6）和式（5）其实是一样的，只是变成了矩阵的形式。</p></blockquote><h2 id="具体的算法"><a href="#具体的算法" class="headerlink" title="具体的算法"></a>具体的算法</h2><p>根据 $M_R = S A^{-1}$，要求 $M_R$ 我们只需算出 $S$ 和 $A$即可。所以有了下面的算法：</p><ol><li>随机选3个向量，这里选择 $n_0 = [0,0,1], n_1 = [0,1,0], n_2 = [1,0,0]$</li><li>代入 $P(\omega_i)$求得 $A$，并得到 $A^{-1}$</li><li>利用旋转矩阵，求得 $Rn_k$，并带入  $P(\omega_i)$求得 $S$</li><li>$M_R = S A^{-1}$</li><li>给定空间中任意一渲染点的坐标 $p$ 的在未旋转光源下的系数 $c = ({c_1^{-1}}^{},{c_1^{0}}^{},{c_1^{1}}^{})^T $, $c^{\prime} = {M_R}^T c$。</li></ol><p>更具体的例子可以参考[1][1]文章尾部的示例一节。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>关于这部分，有几个问题我也没完全搞懂。</p><ol><li><p>乘以 $M_R$还是 ${M_R}^T$？[1][1]和作业2的任务书里面都是乘以 $M_R$，但是我上面的推导的结果是${M_R}^T$，我实现的时候对比了一下，乘以${M_R}^T$的结果才是对的，乘以${M_R}$的结果会使得渲染的结果与模型的旋转方向相反。</p></li><li><p>不同轴，如果使用同一个RotationMatrix进行模型的旋转和球谐系数的计算，会使得它们在绕不同轴旋转。。。这我暂时不能理解。</p></li></ol><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zhuanlan.zhihu.com/p/51267461">[1]一种简易的旋转球谐函数系数的方法</a>    </p>]]></content>
      
      
      <categories>
          
          <category> 图形学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图形学 </tag>
            
            <tag> PRT </tag>
            
            <tag> Real Time Rendering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>作业2补充</title>
      <link href="2021/05/04/cg/ass2/"/>
      <url>2021/05/04/cg/ass2/</url>
      
        <content type="html"><![CDATA[<h3 id="作业2的提高部分"><a href="#作业2的提高部分" class="headerlink" title="作业2的提高部分"></a>作业2的提高部分</h3><p>Interreflection:<br><img src="CornellBox_inter.gif" alt=""></p><p>Shadowed:<br><img src="正确的旋转效果2.gif" alt=""></p><p>Unshadowed:<br><img src="CornellBox_unshadowed.gif" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 图形学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图形学 </tag>
            
            <tag> PRT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>胡适日记 2021年5月</title>
      <link href="2021/05/01/diary/2021-nian-5-yue/"/>
      <url>2021/05/01/diary/2021-nian-5-yue/</url>
      
        <content type="html"><![CDATA[<h3 id="5-1-5-7"><a href="#5-1-5-7" class="headerlink" title="5.1-5.7"></a>5.1-5.7</h3><h4 id="5-1"><a href="#5-1" class="headerlink" title="5.1"></a>5.1</h4><p>劳动节。学习了PRT。<br>对女朋友生了大气，对不起。</p><h4 id="5-2"><a href="#5-2" class="headerlink" title="5.2"></a>5.2</h4><p>完成了作业2的基础部分的大半（Light项预计算，Shadow和Unshadow的传输项预计算），还是比较容易的，会使用现成的函数就好了。明天要写JS了，想想就蛋疼。还了解到了Yan的黑历史，太牛了。</p><p>晚上吃了10个麦辣鸡翅，真好吃。</p><p>爸爸买了灭蚊灯，不知道效果怎么样，最近被咬的好惨。</p><p>决定在博客上写日记了，不过要好好藏起来</p><h4 id="5-11"><a href="#5-11" class="headerlink" title="5.11"></a>5.11</h4><p>今天看完了 101 P19 和 P20.这两个视频都是科普视频，光场相机，色彩空间，color matching，没啥意思，就是听听看。</p><p>接下来的学习路线是什么呢？</p><p>c++：<br>根据面经和知乎搜藏的查漏补缺</p><p>图形学：</p><ol><li>把当前欠的课补上</li><li>游戏引擎的课程 GAMES 201</li><li>经典论文看原文。哪些是经典的论文呢？大家都推荐的，以及引擎中会用到的一些常用算法</li><li>对于引擎中常用的算法，研究引擎的实现，有需要的时候可以自己实现一下</li><li>opengl的学习，这点暂时没头绪。 learnopengl还是从项目中学习。</li><li>小项目：选自己感兴趣的小项目做一下。比如101的课设</li><li>根据面经查漏补缺</li></ol><p>cv：</p>]]></content>
      
      
      <categories>
          
          <category> 汇编 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 汇编 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[C++][标准库] 完美转发 = 引用折叠 + 万能引用 + std::forward</title>
      <link href="2021/04/29/cpp/wan-mei-zhuan-fa-yin-yong-zhe-die-wan-neng-yin-yong-std-forward/"/>
      <url>2021/04/29/cpp/wan-mei-zhuan-fa-yin-yong-zhe-die-wan-neng-yin-yong-std-forward/</url>
      
        <content type="html"><![CDATA[<h3 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h3><p>完美转发是一个比较简单，却又比较复杂的东西。</p><p>简单之处在于理解<strong>动机</strong>：C++为什么需要完美转发？</p><p>复杂之处在于理解<strong>原理</strong>：完美转发基于万能引用，引用折叠以及std::forward模板函数。</p><p>本文将会结合GCC源码，详细解读完美转发的动机和原理。</p><h3 id="动机：C-为什么需要完美转发？"><a href="#动机：C-为什么需要完美转发？" class="headerlink" title="动机：C++为什么需要完美转发？"></a>动机：C++为什么需要完美转发？</h3><p>我们从一个简单的例子出发。<br>假设有这么一种情况，用户一般使用testForward函数，testForward什么也不做，只是简单的转调用到print函数。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">T</span><span class="token operator">></span><span class="token keyword">void</span> <span class="token function">print</span><span class="token punctuation">(</span>T <span class="token operator">&amp;</span> t<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"Lvalue ref"</span> <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">T</span><span class="token operator">></span><span class="token keyword">void</span> <span class="token function">print</span><span class="token punctuation">(</span>T <span class="token operator">&amp;&amp;</span> t<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"Rvalue ref"</span> <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">T</span><span class="token operator">></span><span class="token keyword">void</span> <span class="token function">testForward</span><span class="token punctuation">(</span>T <span class="token operator">&amp;&amp;</span> v<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>     <span class="token function">print</span><span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">//v此时已经是个左值了,永远调用左值版本的print</span>    <span class="token function">print</span><span class="token punctuation">(</span>std<span class="token operator">::</span>forward<span class="token operator">&lt;</span>T<span class="token operator">></span><span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">//本文的重点</span>    <span class="token function">print</span><span class="token punctuation">(</span>std<span class="token operator">::</span><span class="token function">move</span><span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">//永远调用右值版本的print</span>    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"======================"</span> <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span> <span class="token operator">*</span> argv<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>    <span class="token keyword">int</span> x <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token function">testForward</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">//实参为左值</span>    <span class="token function">testForward</span><span class="token punctuation">(</span>std<span class="token operator">::</span><span class="token function">move</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">//实参为右值</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面的程序的运行结果：<br><pre class="line-numbers language-none"><code class="language-none">Lvalue refLvalue refRvalue ref&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;Lvalue refRvalue refRvalue ref&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>用户希望<code>testForward(x);</code>最终调用的是左值版本的print，而<code>testForward(std::move(x));</code>最终调用的是右值版本的print。</p><p><strong>可惜的是，在testForward中，虽然参数v是右值类型的，但此时v在内存中已经有了位置，所以v其实是个左值！</strong></p><p>所以，<code>print(v)</code>永远调用左值版本的print，与用户的本意不符。<code>print(std::move(v));</code>永远调用右值版本的print，与用户的本意也不符。只有<code>print(std::forward&lt;T&gt;(v));</code>才符合用户的本意，这就是本文的主题。</p><p>不难发现，本质问题在于，左值右值在函数调用时，都转化成了左值，使得函数转调用时无法判断左值和右值。</p><p>在STL中，随处可见这种问题。比如C++11引入的<code>emplace_back</code>，它接受左值也接受右值作为参数，接着，它转调用了空间配置器的construct函数，而construct又转调用了<code>placement new</code>，<code>placement new</code>根据参数是左值还是右值，决定调用拷贝构造函数还是移动构造函数。</p><p>这里要保证从<code>emplace_back</code>到<code>placement new</code>，参数的左值和右值属性保持不变。这其实不是一件简单的事情。</p><h3 id="前置知识-引用折叠-万能引用"><a href="#前置知识-引用折叠-万能引用" class="headerlink" title="前置知识 引用折叠 万能引用"></a>前置知识 引用折叠 万能引用</h3><p>C++ Primer 里面写的比较容易理解，在P608（我的是第5版）。<br>略</p><h3 id="原理：完美转发"><a href="#原理：完美转发" class="headerlink" title="原理：完美转发"></a>原理：完美转发</h3><p>std::forward不是独自运作的，在我的理解里，完美转发 = std::forward + 万能引用 + 引用折叠。三者合一才能实现完美转发的效果。</p><p>std::forward的正确运作的前提，是引用折叠机制，为T &amp;&amp;类型的万能引用中的模板参数T赋了一个恰到好处的值。我们用T去指明std::forward<T>的模板参数，从而使得std::forward返回的是正确的类型。</p><p>当然，我们还是先回到一开始的例子。</p><h4 id="testForward-x"><a href="#testForward-x" class="headerlink" title="testForward(x)"></a>testForward(x)</h4><p>回到上面的例子。先考虑<code>testForward(x);</code>这一行代码。</p><h5 id="step-1-实例化testForward"><a href="#step-1-实例化testForward" class="headerlink" title="step 1 实例化testForward"></a>step 1 实例化testForward</h5><p>根据万能引用的实例化规则，实例化的testForward大概长这样：<br><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">T <span class="token operator">=</span> <span class="token keyword">int</span> <span class="token operator">&amp;</span><span class="token keyword">void</span> <span class="token function">testForward</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">&amp;</span> <span class="token operator">&amp;&amp;</span> v<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>    <span class="token function">print</span><span class="token punctuation">(</span>std<span class="token operator">::</span>forward<span class="token operator">&lt;</span>T<span class="token operator">></span><span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><br>又根据引用折叠，上面的等价于下面的代码：<br><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">T <span class="token operator">=</span> <span class="token keyword">int</span> <span class="token operator">&amp;</span><span class="token keyword">void</span> <span class="token function">testForward</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">&amp;</span> v<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>    <span class="token function">print</span><span class="token punctuation">(</span>std<span class="token operator">::</span>forward<span class="token operator">&lt;</span><span class="token keyword">int</span> <span class="token operator">&amp;</span><span class="token operator">></span><span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><br>如果你正确的理解了引用折叠，那么这一步是很好理解的。</p><h5 id="step-2-实例化std-forward"><a href="#step-2-实例化std-forward" class="headerlink" title="step 2 实例化std::forward"></a>step 2 实例化std::forward</h5><blockquote><p>注：C++ Primer：forward必须通过显式模板实参来调用，不能依赖函数模板参数推导。</p></blockquote><p>接下来我们来看下<code>std::forward</code>在libstdc++中的实现：<br><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token number">68</span>   <span class="token comment">/**69    *  @brief  Forward an lvalue.70    *  @return The parameter cast to the specified type.71    *72    *  This function is used to implement "perfect forwarding".73    */</span><span class="token number">74</span>   <span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">_Tp</span><span class="token operator">></span><span class="token number">75</span>     <span class="token keyword">constexpr</span> _Tp<span class="token operator">&amp;&amp;</span><span class="token number">76</span>     <span class="token function">forward</span><span class="token punctuation">(</span><span class="token keyword">typename</span> <span class="token class-name">std</span><span class="token operator">::</span>remove_reference<span class="token operator">&lt;</span>_Tp<span class="token operator">></span><span class="token operator">::</span>type<span class="token operator">&amp;</span> __t<span class="token punctuation">)</span> <span class="token keyword">noexcept</span><span class="token number">77</span>     <span class="token punctuation">&#123;</span> <span class="token keyword">return</span> <span class="token keyword">static_cast</span><span class="token operator">&lt;</span>_Tp<span class="token operator">&amp;&amp;</span><span class="token operator">></span><span class="token punctuation">(</span>__t<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>由于Step1中我们调用<code>std::forward&lt;int &amp;&gt;</code>，所以此处我们代入<code>T = int &amp;</code>，我们有：<br><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token keyword">constexpr</span> <span class="token keyword">int</span> <span class="token operator">&amp;</span> <span class="token operator">&amp;&amp;</span> <span class="token comment">//折叠</span><span class="token function">forward</span><span class="token punctuation">(</span><span class="token keyword">typename</span> <span class="token class-name">std</span><span class="token operator">::</span>remove_reference<span class="token operator">&lt;</span><span class="token keyword">int</span> <span class="token operator">&amp;</span><span class="token operator">></span><span class="token operator">::</span>type<span class="token operator">&amp;</span> __t<span class="token punctuation">)</span> <span class="token keyword">noexcept</span> <span class="token comment">//remove_reference的作用与名字一致，不过多解释</span> <span class="token punctuation">&#123;</span> <span class="token keyword">return</span> <span class="token keyword">static_cast</span><span class="token operator">&lt;</span><span class="token keyword">int</span> <span class="token operator">&amp;</span> <span class="token operator">&amp;&amp;</span><span class="token operator">></span><span class="token punctuation">(</span>__t<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">&#125;</span> <span class="token comment">//折叠</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><br>这里又发生了2次引用折叠，所以上面的代码等价于：<br><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token keyword">constexpr</span> <span class="token keyword">int</span> <span class="token operator">&amp;</span> <span class="token comment">//折叠</span><span class="token function">forward</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">&amp;</span> __t<span class="token punctuation">)</span> <span class="token keyword">noexcept</span> <span class="token comment">//remove_reference的作用与名字一致，不过多解释</span> <span class="token punctuation">&#123;</span> <span class="token keyword">return</span> <span class="token keyword">static_cast</span><span class="token operator">&lt;</span><span class="token keyword">int</span> <span class="token operator">&amp;</span><span class="token operator">></span><span class="token punctuation">(</span>__t<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">&#125;</span> <span class="token comment">//折叠</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><br>所以最终<code>std::forward&lt;int &amp;&gt;(v)</code>的作用就是将参数强制转型成<code>int &amp;</code>，而<code>int &amp;</code>为左值。所以，调用左值版本的print。</p><h4 id="testForward-std-move-x"><a href="#testForward-std-move-x" class="headerlink" title="testForward(std::move(x))"></a>testForward(std::move(x))</h4><p>接下来，考虑<code>testForward(std::move(x))</code>的情况。</p><h5 id="step-1-实例化testForward-1"><a href="#step-1-实例化testForward-1" class="headerlink" title="step 1 实例化testForward"></a>step 1 实例化testForward</h5><p><code>testForward(std::move(x))</code>也就是<code>testForward(static_cast&lt;int &amp;&amp;&gt;(x))</code>。根据万能引用的实例化规则，实例化的testForward大概长这样：<br><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">T <span class="token operator">=</span> <span class="token keyword">int</span> <span class="token keyword">void</span> <span class="token function">testForward</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">&amp;&amp;</span> v<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>    <span class="token function">print</span><span class="token punctuation">(</span>std<span class="token operator">::</span>forward<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span><span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><br>万能引用绑定到右值上时，不会发生引用折叠，所以这里没有引用折叠。</p><h5 id="step-2-实例化std-forward-1"><a href="#step-2-实例化std-forward-1" class="headerlink" title="step 2 实例化std::forward"></a>step 2 实例化std::forward</h5><blockquote><p>注：C++ Primer：forward必须通过显式模板实参来调用，不能依赖函数模板参数推导。</p><p>这里用到的std::forward的代码和上面的一样，故略去。</p><p>由于Step1中我们调用<code>std::forward&lt;int&gt;</code>，所以此处我们代入<code>T = int</code>，我们有：<br><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token keyword">constexpr</span> <span class="token keyword">int</span> <span class="token operator">&amp;&amp;</span> <span class="token function">forward</span><span class="token punctuation">(</span><span class="token keyword">typename</span> <span class="token class-name">std</span><span class="token operator">::</span>remove_reference<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span><span class="token operator">::</span>type<span class="token operator">&amp;</span> __t<span class="token punctuation">)</span> <span class="token keyword">noexcept</span> <span class="token comment">//remove_reference的作用与名字一致，不过多解释</span> <span class="token punctuation">&#123;</span> <span class="token keyword">return</span> <span class="token keyword">static_cast</span><span class="token operator">&lt;</span><span class="token keyword">int</span> <span class="token operator">&amp;&amp;</span><span class="token operator">></span><span class="token punctuation">(</span>__t<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><br>这里又发生了2次引用折叠，所以上面的代码等价于：<br><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token keyword">constexpr</span> <span class="token keyword">int</span> <span class="token operator">&amp;&amp;</span><span class="token function">forward</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">&amp;</span> __t<span class="token punctuation">)</span> <span class="token keyword">noexcept</span> <span class="token comment">//remove_reference的作用与名字一致，不过多解释</span> <span class="token punctuation">&#123;</span> <span class="token keyword">return</span> <span class="token keyword">static_cast</span><span class="token operator">&lt;</span><span class="token keyword">int</span> <span class="token operator">&amp;&amp;</span><span class="token operator">></span><span class="token punctuation">(</span>__t<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">&#125;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><br>所以最终<code>std::forward&lt;int&gt;(v)</code>的作用就是将参数强制转型成<code>int &amp;&amp;</code>，而<code>int &amp;&amp;</code>为右值。所以，调用右值版本的print。</p></blockquote><h3 id="写在后面"><a href="#写在后面" class="headerlink" title="写在后面"></a>写在后面</h3><p>在GCC源码中，std::forward还有第二个版本：<a href="https://gcc.gnu.org/onlinedocs/gcc-5.4.0/libstdc++/api/a01395_source.html#l00087">link</a>，分析的方法与本文一致，这里就不讲了。。</p><p>右值的概念其实很微妙，一旦某个右值，有了名字，也就在内存中有了位置，它就变成了1个左值。但它又是一个很有用的概念，<strong>允许程序员更加细粒度的处理对象拷贝时的内存分配问题，提高了对临时对象和不需要的对象的利用率</strong>，极大提高程序的效率。当然，也会引入更多的bug。不过，这就是C++的哲学，什么都允许你做，但出了问题，可别赖C++这门语言。</p><p>完美转发基于万能引用，引用折叠以及std::forward模板函数。据我所知，STL出现std::forward，一定出现万能引用。其实这也很好理解，完美转发机制，是为了将左值和右值统一处理，节约代码量，而只有万能引用会出现同时接受左值和右值的情况，所以完美转发只存在于万能引用中。</p>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> Template </tag>
            
            <tag> C++ STL </tag>
            
            <tag> GNU C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>XML 与 JSON 在设计目的和应用场景上的区别</title>
      <link href="2021/02/21/others/xml-yu-json-zai-she-ji-mu-de-he-ying-yong-chang-jing-shang-de-qu-bie/"/>
      <url>2021/02/21/others/xml-yu-json-zai-she-ji-mu-de-he-ying-yong-chang-jing-shang-de-qu-bie/</url>
      
        <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>今天在看《第一行代码》中关于SharedPreference的部分时，突然觉得用XML保存数据很不方便，比如如果要储存数组，用JSON中一行代码就可以解决的问题，XML还得自己写一些代码，比如：<a href="https://link.zhihu.com/?target=https%3A//stackoverflow.com/questions/3876680/is-it-possible-to-add-an-array-or-object-to-sharedpreferences-on-android">Is it possible to add an array or object to SharedPreferences on Android</a>。</p><p>所以我很好奇，XML在竞争如此激烈的时代，还在广泛被使用，是因为它是一个无法解决的历史遗留问题，还是因为XML确实有比JSON更优越的地方。</p><blockquote><p>注：以下部分段落节选并翻译自<a href="https://link.zhihu.com/?target=https%3A//stackoverflow.com/questions/2620270/what-is-the-difference-between-json-and-xml">StackOverflow</a>，不代表本人观点。</p></blockquote><h2 id="XML与JSON最本质的区别"><a href="#XML与JSON最本质的区别" class="headerlink" title="XML与JSON最本质的区别"></a>XML与JSON最本质的区别</h2><blockquote><p><a href="https://link.zhihu.com/?target=https%3A//stackoverflow.com/a/2620466/6109336">来源</a>，我做了一些注解</p></blockquote><p>最根本上来说，XML是一个markup language（标记语言），而JSON是一种用于数据交换（data-interchange）的序列化对象的语言。</p><p>根据Wiki的说法，标记语言是：</p><blockquote><p>In computer text processing, a markup language is a system for annotating a document in a way that is syntactically distinguishable from the text, meaning when the document is processed for display, the markup language is not shown, and is only used to format the text.</p></blockquote><p>标记语言除了文本信息，还包括了一些元信息，这些元信息用来标注如何处理文本信息，比如：</p><pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Document</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Paragraph</span> <span class="token attr-name">Align</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>Center<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>                 <span class="token comment">&lt;!-- Align是元信息 --></span>        Here <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Bold</span><span class="token punctuation">></span></span>is<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Bold</span><span class="token punctuation">></span></span> some text.    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Paragraph</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Document</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>假如试图用JSON完完整整的表述上述的信息：</p><pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>    <span class="token property">"Paragraphs"</span><span class="token operator">:</span> <span class="token punctuation">[</span>        <span class="token punctuation">&#123;</span>            <span class="token property">"align"</span><span class="token operator">:</span> <span class="token string">"center"</span><span class="token punctuation">,</span>            <span class="token property">"content"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                <span class="token string">"Here "</span><span class="token punctuation">,</span> <span class="token punctuation">&#123;</span>                    <span class="token property">"style"</span> <span class="token operator">:</span> <span class="token string">"bold"</span><span class="token punctuation">,</span>                    <span class="token property">"content"</span><span class="token operator">:</span> <span class="token punctuation">[</span> <span class="token string">"is"</span> <span class="token punctuation">]</span>                <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>                <span class="token string">" some text."</span>            <span class="token punctuation">]</span>        <span class="token punctuation">&#125;</span>    <span class="token punctuation">]</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>是不是觉得JSON比XML还要复杂的多？</p><p>原因在于，JSON里面没有<strong>元数据和数据的区别</strong>，<strong>所有的东西都是数据</strong>，所以要人为的加上一些多余的字符串（比如content）进行区分。</p><hr><p>同样的，XML也不擅长做JSON所擅长做的事，那就是序列化对象：</p><pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>    <span class="token property">"firstName"</span><span class="token operator">:</span> <span class="token string">"Homer"</span><span class="token punctuation">,</span>    <span class="token property">"lastName"</span><span class="token operator">:</span> <span class="token string">"Simpson"</span><span class="token punctuation">,</span>    <span class="token property">"relatives"</span><span class="token operator">:</span> <span class="token punctuation">[</span> <span class="token string">"Grandpa"</span><span class="token punctuation">,</span> <span class="token string">"Marge"</span><span class="token punctuation">,</span> <span class="token string">"The Boy"</span><span class="token punctuation">,</span> <span class="token string">"Lisa"</span><span class="token punctuation">,</span> <span class="token string">"I think that's all of them"</span> <span class="token punctuation">]</span><span class="token punctuation">&#125;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果用XML表示上述对象：</p><pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Person</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>FirstName</span><span class="token punctuation">></span></span>Homer<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>FirstName</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>LastName</span><span class="token punctuation">></span></span>Simpsons<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>LastName</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Relatives</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Relative</span><span class="token punctuation">></span></span>Grandpa<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Relative</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Relative</span><span class="token punctuation">></span></span>Marge<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Relative</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Relative</span><span class="token punctuation">></span></span>The Boy<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Relative</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Relative</span><span class="token punctuation">></span></span>Lisa<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Relative</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Relative</span><span class="token punctuation">></span></span>I think that's all of them<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Relative</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Relatives</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Person</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>从这个例子可以看到JSON有2点优于XML的地方：</p><ul><li>对象的内部结构一目了然，简洁明了。</li><li>JSON语法规定[]是数组，{}是对象，而XML没有如此的语法规定，我们只能临时发明一种方式来表示数组，然后自己添加代码来识别这个数组。</li></ul><p>如果我们人为施加一种策略，那么XML的确可以完成JSON的工作，但是JSON本身内建了这种策略。</p><p>比如<a href="https://link.zhihu.com/?target=https%3A//stackoverflow.com/a/7998630/6109336">这里</a>，有人提出了XJSON，可以用XML完成JSON的工作：</p><pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>xjson</span><span class="token punctuation">></span></span>     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>object</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>persons<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>array</span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>object</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>Ford Prefect<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>gender</span><span class="token punctuation">></span></span>male<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>gender</span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>object</span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>object</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>Arthur Dent<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>gender</span><span class="token punctuation">></span></span>male<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>gender</span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>object</span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>object</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>Tricia McMillan<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>gender</span><span class="token punctuation">></span></span>female<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>gender</span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>object</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>array</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>object</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>xjson</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>Once you wrote an XJSON processor, it could do exactly what JSON processor does, for all the types of data that JSON can represent, and you could translate data losslessly between JSON and XJSON.<br>如果你写出一个针对XJSON的解析包，那么它可以完成JSON所有的工作。</p></blockquote><p>不过，这未免也太眼花缭乱了一些。</p><p>所以在表示对象这个问题上面，JSON是远比XML优越的一种语言。</p><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>在我看来，XML和JSON是乍一看有点相似，但设计出发点和应用场景却并不重叠的语言。</p><p>就好像以前我发现MATLAB除了可以做矩阵计算，还可以写GUI，但我觉得不会有人真的用它去写复杂的GUI界面。Python这种语言似乎很万能很流行，什么都可以做，甚至游戏，但不会真的有人去用Python去写游戏引擎的底层部分。</p><p>延伸阅读：</p><p><a href="https://www.zhihu.com/question/25636060">为什么都反对 XML 而支持使用 JSON？</a></p><p>（END）</p>]]></content>
      
      
      <categories>
          
          <category> 编程随想 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> XML </tag>
            
            <tag> JSON </tag>
            
            <tag> 题外话 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
