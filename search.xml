<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="2021/08/15/ml/qiang-hua-xue-xi/paper/dqn/"/>
      <url>2021/08/15/ml/qiang-hua-xue-xi/paper/dqn/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="2021/08/15/ml/qiang-hua-xue-xi/paper/policy-gradient-therom/"/>
      <url>2021/08/15/ml/qiang-hua-xue-xi/paper/policy-gradient-therom/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>[Paper][强化学习][ICML 2017] Distributional RL</title>
      <link href="2021/08/02/ml/qiang-hua-xue-xi/paper/distributional-dqn/"/>
      <url>2021/08/02/ml/qiang-hua-xue-xi/paper/distributional-dqn/</url>
      
        <content type="html"><![CDATA[<p>论文：ICML 2017 A Distributional Perspective on Reinforcement Learning</p><h2 id="引子-amp-Motivation"><a href="#引子-amp-Motivation" class="headerlink" title="引子 &amp; Motivation"></a>引子 &amp; Motivation</h2><p>Motivation &amp; idea: Approximate value distribution instead of expected return. </p><p>然而，idea虽然简单，实现起来却并不容易。最简单的想法使用一个 高斯分布 对 $Q$ 建模，然而这其实与 approximate expected return区别不大。这篇文章采取的是另一种方法，这使得所拟合的分布不仅限于 单峰形态的概率分布函数（比如 高斯分布）：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210809193347237.png" alt="image-20210809193347237"></p><h2 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h2><p>略</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><h3 id="1-区间离散化"><a href="#1-区间离散化" class="headerlink" title="1. 区间离散化"></a>1. 区间离散化</h3><p>如何方便的建模任意的概率分布函数呢？作者使用了一个离散分布来建模：</p><ol><li><p>将 $[V_{min}, V_{max}]$ 上均匀采样 $N$ 个点，$z_0, z_1, …z_{N}$。（$V_{min}, V_{max}, N$ 是三个超参，在本文的实验部分，使用的超参为 $-10, 10, 51$，即相邻点的相差 $0.4$。</p></li><li><p>每个点的概率 为 </p><script type="math/tex; mode=display">P[Q(x,a) = z_i] = p_{i}(x, a)=\frac{e^{\theta_{i}(x, a)}}{\sum_{j} e^{\theta_{j}(x, a)}} (0 \le i < N)</script><p>这里的 $\theta_{i}(x, a)$ 为需要学习的神经网络，神经网络的输出通过归一化转化为概率。</p></li></ol><h3 id="2-Projected-Bellman-Update"><a href="#2-Projected-Bellman-Update" class="headerlink" title="2. Projected Bellman Update"></a>2. Projected Bellman Update</h3><p>DQN中的target value 为 ：</p><script type="math/tex; mode=display">Q_{target}(s_t,a) = R + \gamma Q(s^{t+1}, a^*)</script><p>这篇论文中的target distribution为：</p><ol><li><p>计算 next state 的 best action $a^*$ 的distribution：</p><script type="math/tex; mode=display">\begin{aligned}&Q\left(x_{t+1}, a\right):=\sum_{i} z_{i} p_{i}\left(x_{t+1}, a\right) \\&a^{*} \leftarrow \arg \max _{a} Q\left(x_{t+1}, a\right)\end{aligned}</script></li><li><p>计算 离散点 $z_j$ 经过Bellman Update后所处的位置：</p></li></ol><script type="math/tex; mode=display">\begin{aligned}&\hat{\mathcal{T}} z_{j} \leftarrow\left[r_{t}+\gamma_{t} z_{j}\right]_{V_{\mathrm{MIN}}}^{V_{\mathrm{MAX}}} \end{aligned}</script><ol><li>由于 $\gamma$ 和 $r_t$ 影响，Bellman Update之后的 $\hat{\mathcal{T}} z_{j}$ 可能并不在预先设好的离散点 $z_0, z_1, …z_{N}$ 上。于是根据  $\hat{\mathcal{T}} z_{j}$ 与相邻离散点 $l,u$ 的 距离将  $p_{j}\left(x_{t+1}, a^{*}\right)$ 分配到  $l,u$ 上。（  $l,u$ 是  $z_0, z_1, …z_{N}$ 中的点）：</li></ol><script type="math/tex; mode=display">\begin{aligned}&b_{j} \leftarrow\left(\hat{\mathcal{T}} z_{j}-V_{\mathrm{MIN}}\right) / \Delta z \quad \# b_{j} \in[0, N-1]\\&l \leftarrow\left\lfloor b_{j}\right\rfloor, u \leftarrow\left\lceil b_{j}\right\rceil \text {   # Distribute probability of } \hat{\mathcal{T}} z_{j}\\&m_{l} \leftarrow m_{l}+p_{j}\left(x_{t+1}, a^{*}\right)\left(u-b_{j}\right)\\&m_{u} \leftarrow m_{u}+p_{j}\left(x_{t+1}, a^{*}\right)\left(b_{j}-l\right)\end{aligned}</script><ol><li>所得到的 $m_0, m_1, ….m_N$ 即为 target distribution 的 每个离散点 $z_0, z_1, …z_{N}$ 上的概率。这里由于是两个概率之间的差值，所以loss function采用<a href="https://en.wikipedia.org/wiki/Cross_entropy">cross-entropy</a> ：<script type="math/tex; mode=display">-\sum_{i} m_{i} \log p_{i}\left(x_{t}, a_{t}\right)</script></li></ol><p>至此，得到最终的算法：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210809200918751.png" alt="image-20210809200918751"></p><p>附示意图一张：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210809201130640.png" alt="image-20210809201130640"></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>待补充</p><h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p>待补充</p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Paper][强化学习][AAAI 2016] Double DQN</title>
      <link href="2021/07/31/ml/qiang-hua-xue-xi/paper/double-dqn/"/>
      <url>2021/07/31/ml/qiang-hua-xue-xi/paper/double-dqn/</url>
      
        <content type="html"><![CDATA[<h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>这篇文章的创新之处并不多，可以用一句话概括：将 DQN 中的 Q-learning换成了 Double Q-learning。DQN算法和代码中需要修改的地方不超过2行。</p><p>所以，这篇文章基本没提算法，而是用大量篇幅描写各种实验，以展示Q-learning引入的bias，以及Double Q-learning 如何改善这种bias，进而改进policy。甚至，因为和DQN太像了，在论文中没有给出 Double DQN的算法伪代码。</p><p>不过，这篇论文图文并茂的解释了Q-learning所带来的overeestimation问题。之前看Sutton的书时，理解并不深刻，这篇论文却从多个角度让我加深了理解。</p><h2 id="论文总结-amp-Motivation"><a href="#论文总结-amp-Motivation" class="headerlink" title="论文总结 &amp; Motivation"></a>论文总结 &amp; Motivation</h2><p>这篇文章提出并解答了几个问题：</p><ol><li>Q-learning(DQN) 是否存在 overestimation现象？存在。</li><li>overestimation是否一定会影响policy？如果uniformly overestimate，则不会。但大多数情况下并不是uniformly，所以会。</li><li>如何将Double Q-learning加到DQN中？非常简单的修改。</li><li>Double Q-learning(Double DQN) 是否缓解了overestimation，并使得最终得到的policy有了提升？是的。</li></ol><p><em>注：overestimation是否只能通过Double learning解决？不一定，Rainbow[1]中认为 Distributional RL[2] 可以部分替代 Double Q-learning的作用</em></p><h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>Double DQN相比DQN唯一不同的地方，就是target的计算方式。</p><p>DQN中 target 的计算公式为：</p><script type="math/tex; mode=display">Y_{t}^{\mathrm{DQN}} \equiv R_{t+1}+\gamma \max _{a} Q\left(S_{t+1}, a ; \theta_{t}^{-}\right)</script><p>套用Double Q-learning，将上式改写为：</p><script type="math/tex; mode=display">Y_{t}^{\text {DoubleDQN }} \equiv R_{t+1}+\gamma Q_{target}\left(S_{t+1}, \underset{a}{\operatorname{argmax}} Q\left(S_{t+1}, a ; \boldsymbol{\theta}_{t}\right), \boldsymbol{\theta}_{t}^{-}\right)</script><p>Double DQN中，选择action由 $\theta_{t}$ 完成，计算Q由 $\theta_{t}^{-}$(target network) 完成。定期将  $\theta_{t}^{-}$拷贝到  $\theta_{t}^{-}$ 上， 不需要像 Double Q-learning那样交替更新。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Double-DQN缓解了-Overestimation"><a href="#Double-DQN缓解了-Overestimation" class="headerlink" title="Double DQN缓解了 Overestimation"></a>Double DQN缓解了 Overestimation</h3><p>Double DQN选取了一些Atari中的游戏，与DQN做了对比，结果在下图中。这幅图清楚的展示了Double Q-learning的优点：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210806002030210.png" alt="image-20210806002030210"></p><p>这幅图有几点需要解释的：</p><p>top row:</p><ol><li><p>top row中的横线是 true value。指的是：当训练完成时，用训练好的网络，在某个state上，模拟多个episode，用episode的reward计算出真实的 $Q$ 值。所以，横线是 true Q of final policy。 所以，需要关注的是训练完成时曲线与横线的差距。</p><p>显然训练完成时，Double DQN离true Double DQN更近一些，这说明Double DQN 的overestimation现象更不明显。</p></li><li><p>true Double DQN &gt; true DQN。这说明 Double DQN 是比 DQN 更好的 policy（Q值的意义就是 expected reward）</p></li></ol><p>middle &amp; bottom row:</p><ol><li>middle row是Atari中两种游戏的value estimate，注意到y轴是log scale的，这意味着value estimate与true Value相差非常之多。</li><li>middle row 中DQN在50 millons step时<strong>上涨明显，</strong>此时bottom row中scores中的DQN的曲线开始<strong>急速下降</strong>，论文中认为这并不是一种巧合。这说明overestimation确实对scores有坏的影响。</li><li>尽管这里只展示了2种游戏，但上述现象在Atari 中的 所有游戏中都发生了，这可以证明 <em>“在DQN中，overestimation对scores有坏的影响”</em> 是一个普遍的结论</li></ol><h3 id="Double-DQN-提高了-scores"><a href="#Double-DQN-提高了-scores" class="headerlink" title="Double DQN 提高了 scores"></a>Double DQN 提高了 scores</h3><p>文中分了两小节说明 Double DQN在 Atari上 1）仅就score而言，表现比 DQN要好  2）在human start上，泛化能力也更强。</p><h4 id="Part-1-Quality-of-the-learned-policies"><a href="#Part-1-Quality-of-the-learned-policies" class="headerlink" title="Part 1 Quality of the learned policies"></a>Part 1 Quality of the learned policies</h4><p>Part 1按照DQN的evaluton流程，starting point是通过no-op操作生成的，存在一定的随机性，但是随机性有限。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210806135502427.png" alt="image-20210806135502427"></p><p>（注：表中Double DQN所用的超参与DQN保持一致，并没有调过。）</p><h4 id="Part-2-Robustness-to-Human-starts"><a href="#Part-2-Robustness-to-Human-starts" class="headerlink" title="Part 2 Robustness to Human starts"></a>Part 2 Robustness to Human starts</h4><p>Part 2  obtained 100 starting points sampled for each game from a human expert’s trajectory, as proposed by Nair et al. We start an evaluation episode from each of these starting points and run the emulator for up to 108,000 frames.</p><p>与Part 1相比，Part 2 的starting points更为随机，更考验模型的<strong>泛化能力</strong>。实验结果也佐证了这一点，DQN、Double DQN均有下滑。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210806140402258.png" alt="image-20210806140402258"></p><p>（注：表中Double DQN所用的超参与DQN保持一致，并没有调过。而Double DQN(tuned) 经过了仔细调参）</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1] [AAAI 2018] Rainbow: Combining Improvements in Deep Reinforcement Learning<br>[2] ICML 2017 A Distributional Perspective on Reinforcement Learning</p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> Q-learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Paper][强化学习][ICLR 2016] Prioritized Experience Replay</title>
      <link href="2021/07/29/ml/qiang-hua-xue-xi/paper/prior-replay/"/>
      <url>2021/07/29/ml/qiang-hua-xue-xi/paper/prior-replay/</url>
      
        <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/pdf/1511.05952.pdf">ICLR 2016 Prioritized Experience Replay</a></p><h2 id="引子"><a href="#引子" class="headerlink" title="引子"></a>引子</h2><p>本篇论文是DQN中Experience Replay的后续工作。</p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>The key idea is that an RL agent can learn more effectively from some transitions than from others.</p><h3 id="Example-Blind-Cliffwalk"><a href="#Example-Blind-Cliffwalk" class="headerlink" title="Example: Blind Cliffwalk"></a>Example: Blind Cliffwalk</h3><p>论文中用了一个例子来说明不同样本需要不同权重的必要性。</p><p>在下图中的 Cliffwalk中，显然，agent只有 $1/2^{n-1}$ 的概率到达终点，所以，replay memory中有大量的雷同的transition，只有极少数到达终点的episode的transition。而这些极少数的transition相对而言更加重要。</p><p>（注：这个example的详细setup可以在 Appendix中找到。 由于Cliffwalk的可能的episode个数是有限的，所以replay memory不会进行淘汰，包含了所有可能的episode的transition。对于 n 个 state的Cliffwalk来说，可能的transition个数为 $2^{n+1} - 2$，成功到达终点的transition个数为 $n-1$，如果均匀抽样，选中的概率可以说是非常小了）</p><p>下图右展示了最优抽样（oracle)和均匀抽样情况下，Q函数收敛所需update次数与 cliffwalk中state[注1]的个数的关系。显然，两者有数量级的差异。图中折线是多次实验的中位数。</p><p>（注1：图中用的是#samples，和#states是等价的，成正比关系，#samples = $2^{n+1} - 2$ , n = #states）</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210804174010767.png" alt="image-20210804174010767"></p><p>尽管这是个非常极端的例子，也某种程度上说明了  Prioritized Experience Replay 的重要性。</p><h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><h3 id="Prioritizing-with-TD-error"><a href="#Prioritizing-with-TD-error" class="headerlink" title="Prioritizing with TD error"></a>Prioritizing with TD error</h3><p>这篇论文的核心问题为 如何estimate一个transition的重要性？这时候，可能有人会想到用另一个神经网络来estimate。且慢，有更简单的方法：</p><blockquote><p>One idealised criterion would be the amount the RL agent can learn from a transition in its current state (expected learning progress).</p></blockquote><p>而 <em>the amount the RL agent can learn from a transition</em> 用 TD error 来衡量。$\delta$ 越大，说明 loss 越大，$Q$ network 估计的越不准，被选中的优先级越高。</p><h3 id="Stocastic-Prioritization"><a href="#Stocastic-Prioritization" class="headerlink" title="Stocastic Prioritization"></a>Stocastic Prioritization</h3><p>只使用 $\delta$ 作为priority，根据 $\delta$ 的大小逐个选取transition 会产生了一系列的问题。比如：</p><ul><li><p>TD-error要实时更新。可能一开始某个transition的 $\delta$ 很低，但后来$\delta$ 变得很高。如果不实时更新，该transition的优先级一直都很低，永远得不到更新。</p><p>然而，Replay memory 的数量级在 $10^6$ ，不可能做到实时更新。$\delta$ 只在transition被放入memory时 或 再次被放入时的 $\delta$ 才更新。</p></li><li><p>一些其他问题，参见论文。</p></li></ul><p>基于以上问题，不能只根据 $\delta$ 来抽取样本，而是要随机一点。每个transition 被选中的概率：</p><script type="math/tex; mode=display">\begin{equation}P(i)=\frac{p_{i}^{\alpha}}{\sum_{k} p_{k}^{\alpha}}\end{equation}</script><p>其中 $\alpha$ 为超参，在Atari 2600中设为 0.6 / 0.7。$p_k$ 为 priority，有两种不同的选择，这两种选择的performance互有好坏，需要按照情况选择。</p><ol><li><strong>Proportional Prioritization</strong> : $p_{i}=\left|\delta_{i}\right|+\epsilon$ ，其中 $\epsilon$ 的作用是使 $p_i &gt; 0$</li><li><strong>Rank-based Prioritization</strong> :  $p_{i}=\frac{1}{\operatorname{rank}(i)}$  rank(i) 为 按照 $\left|\delta_{i}\right|$ 从大到小排序的序号。</li></ol><blockquote><p>看上去 Rank-based Prioritization 的效果应该会更好，但实际上不是，在Discussion部分作者解释为：</p><p>we expected the rank-based variant to be more robust because it is not affected by outliers nor error magnitudes. Furthermore, its heavy-tail property also guarantees that samples will be diverse, and the stratified sampling from partitions of different errors will keep the total minibatch gradient at a stable magnitude throughout training.</p><p>Perhaps surprisingly, both variants perform similarly in practice; we suspect this is due to the heavy use of clipping (of rewards and TD-errors) in the DQN algorithm, which removes outliers.</p></blockquote><p>上述两者都是  $\left|\delta_{i}\right|$ 越大，$p_i$ 越大。只不过作者认为第二种更robust一点。</p><h3 id="Importance-Sampling"><a href="#Importance-Sampling" class="headerlink" title="Importance Sampling"></a>Importance Sampling</h3><p>当我们根据Stocastic Prioritization来采样transition时，high priority的transition会被采样许多次，而low priority的transition会被采样较少的次数。用数学语言来说就是，原本我们uniformly 采样，求得 $\mathbb{E_{\sim uniform}[gradient]}$ ，现在换了一个分布采样，显然会引入bias。</p><p>（一个比较容易理解的例子是利用蒙特卡洛积分求曲线下面积，也引入了Importance Sampling修正bias，否则算出的曲线下面积是错误的）</p><p>为了修正这点，引入了Importance Sampling：</p><script type="math/tex; mode=display">\begin{equation}w_{i}=\left(\frac{1}{N} \cdot \frac{1}{P(i)}\right)^{\beta}\end{equation}</script><p>当 $\beta = 1$ 时，bias完全被修正了。</p><p>在论文中采取的是逐步增大 $\beta$ 的操作，从 $\beta = \beta_0$ 逐步增大到 $\beta = 1$ 。</p><h3 id="Algorithm-Double-DQN-with-proportional-prioritization"><a href="#Algorithm-Double-DQN-with-proportional-prioritization" class="headerlink" title="Algorithm: Double DQN with proportional prioritization"></a>Algorithm: Double DQN with proportional prioritization</h3><p>论文结合当时的SOTA Double DQN给出了最终的算法，红框中为与 Double DQN 不同的地方 ：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210804190132858.png" alt="image-20210804190132858"></p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><p>实验的细节、参数基本和 DQN一致，这里不再赘述，只讲几点比较值得注意的：</p><ol><li>memory 的淘汰方式并不是淘汰 $P(i)$ 最低的，仍然是淘汰最先进入memory的。</li><li>超参 $\alpha$ = 0.7, $\beta_0$ = 0.5 （只有 $\beta$ 有 annealing）</li></ol><h3 id="1-Normalized-Score"><a href="#1-Normalized-Score" class="headerlink" title="1. Normalized Score"></a>1. Normalized Score</h3><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210804193245644.png" alt="image-20210804193245644"></p><h3 id="2-Learning-speed"><a href="#2-Learning-speed" class="headerlink" title="2. Learning speed"></a>2. Learning speed</h3><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210804193228198.png" alt="image-20210804193228198"></p><h2 id="Discussion"><a href="#Discussion" class="headerlink" title="Discussion"></a>Discussion</h2><p>待补充</p><h2 id="Extension"><a href="#Extension" class="headerlink" title="Extension"></a>Extension</h2><p>Prioritized Supervised Learning：一般在 Supervised Learning中，由于我们知道各个类的数据量，对于unbalanced的dataset，一般是对数据量少的那一类做data augmentation，使得不同类的数据量持平。不过，假如我们不知道各个类的数据量呢？这篇论文将 Prioritized sampling ( $\alpha$ = 0.7, $\beta$ = 0，no annealing）应用在imbalanced MNIST上：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210804201330159.png" alt="image-20210804201330159"></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>[1]  <a href="https://datascience.stackexchange.com/questions/32873/prioritized-replay-what-does-importance-sampling-really-do">Prioritized-replay-what-does-importance-sampling-really-do</a></p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[Paper][强化学习][ICML 2016] A3C</title>
      <link href="2021/07/28/ml/qiang-hua-xue-xi/paper/a3c/"/>
      <url>2021/07/28/ml/qiang-hua-xue-xi/paper/a3c/</url>
      
        <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/pdf/1602.01783.pdf">Asynchronous Methods for Deep Reinforcement Learning</a> ICML 2016</p><h2 id="引子-amp-总结"><a href="#引子-amp-总结" class="headerlink" title="引子 &amp; 总结"></a>引子 &amp; 总结</h2><p>这篇文章的Motivation比较简单，据论文所说，是为了在Experience Replay之外，找到一种训练深度强化学习网络的方法。</p><p>文章中提出了多种算法，但是最出名的是A3C算法，在更短时间内，达到甚至超过了 Experience Replay 的 Performance。A3C的思路极其简单，可以在此之上做很多A+B之类的研究。</p><p>A3C的想法是这样的：每个agent在自己独立的环境中进行探索，互不干扰。每个agent都在执行自己的Actor-Critic 算法，进行一定次数的探索之后，得到policy network 和 value-network的梯度，<strong>异步更新</strong>全局的 policy network 和 value-network。</p><p>A3C与Experience Replay都在试图解决同一个问题：</p><ul><li><p>用强化学习的方法训练Network的一个难点在于 强化学习获得的样本是连续的，这种连续性会干扰神经网络的学习，降低网络的收敛速度和性能。为了解决这个问题，Experience Replay通过随机采样memory中的transition，使得同一batch中各个transition之间在时间顺序上互不相干，从而解决了这个问题；但是Experience Replay的性质，导致它不能用于on-policy RL算法。</p></li><li><p>A3C某种程度上也是在做同样的事情</p><blockquote><p>This parallelism also decorrelates the agents’ data into a more stationary process, since at any given time-step the parallel agents will be experiencing a variety of different states.</p></blockquote><p>但是，与Experience Replay不同的是，它可以用于on-policy RL算法。</p></li></ul><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>Experience Replay存在缺点：</p><ul><li>需要大量计算资源（GPU，memory）</li><li>仅适用于 off-policy 算法</li></ul><p>有没有其他方法？</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>这篇论文的基本想法在引子中提到了，这里不再重复。这种思想可以与很多RL算法结合，比如 Q-learning：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210722151323505.png" alt="image-20210722151323505"></p><p>还可以和 Actor-Critic结合，得到著名的A3C：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210722222151395.png" alt="image-20210722222151395"></p><ul><li><p>这个算法和 Actor-Critic没区别，只是多了并行异步的部分。</p></li><li><p>n-step bootstrapping：每个线程中对于value network和 policy network的更新都是 n-step bootstrapping的。与 Sutton Book中给出的n-step bootstrapping算法基本相同，只是有一点区别：</p><p>在A3C中，每隔 $t_{max}$ 步骤停下来进行更新，从 $t-1 $ 更新到 $t_{start}$。对于 $t-1$ ,  $G_{t-1} = R_{t} + V(s_t)$（ $G_t$ 即上图中的 $R$），对于 $t - 2$ , $G_{t-2} = R_{t-1} + R_{t} + V(s_t)$ ….。可以这么不严谨的理解，对于 $t-1$，相当于 TD(0)，对于 $t-2$ 相当于 TD(1)，对于 t_start，相当于 TD(t_max - 1)，是bootstrapping from last state。</p></li><li><p>正则项：为了鼓励exploration，在Loss Function末尾加上 $H\left(\pi\left(s_{t}\right)\right)$ ，这里 $H$ 代表entropy 。</p><p>可以这么理解entropy和鼓励exploration之间的关系：当 $\pi$ 给出的概率分比较分散的时候，entropy比较大，即 $H\left(\pi\left(s_{t} ; \theta^{\prime}\right)\right)$ 比较大。加上这一 ”正则项“ 使得 $\pi$ 给出的概率不集中于某一个action，而是比较分散，即鼓励exploration。、</p></li><li><p>网络结构：feature extractor (CNN) 共享，最后有两个独立的输出层——1）policy network的fc + softmax层；2）value network的 fc 层。</p></li></ul><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Atari-2600-与Experience-Replay的对比"><a href="#Atari-2600-与Experience-Replay的对比" class="headerlink" title="Atari 2600: 与Experience Replay的对比"></a>Atari 2600: 与Experience Replay的对比</h3><p>与Experience Replay的对比分两部分。</p><h4 id="1-收敛速度"><a href="#1-收敛速度" class="headerlink" title="1. 收敛速度"></a>1. 收敛速度</h4><p>A3C（在16 个CPU核上）比 DQN（K40）收敛更快。</p><p>鉴于这篇文章想要challenge的是DQN中的Experience Replay，所以做了大量与DQN相关的对比实验。论文选取了5个Atari 2600中的游戏，比较DQN与这篇论文提出的算法的性能。明显看到，A3C的收敛速度要快很多：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210727012205577.png" alt="image-20210727012205577"></p><p><em>注：出于公平，A3C没有选DQN表现最差的游戏（可能A3C在这些游戏上表现也不好），比如 Breakout（打砖块）在DQN下面的结果也很好（不过不是最好的）。</em></p><p><em>注：论文里没有比较A3C和 Prioritized DQN or Dueling D-DQN的收敛速度</em></p><h4 id="2-Scores"><a href="#2-Scores" class="headerlink" title="2. Scores"></a>2. Scores</h4><p>不仅收敛速度 A3C更快，充分训练后的performance也是最好的：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210728185737820.png" alt="image-20210728185737820"></p><p>注：公平的对比是红框之间的对比，因为DQN没有用到LSTM，红框之间的网络结构是相同的。而<code>A3C,LSTM</code> 用到了LSTM。</p><h3 id="更多实验：TORCS-MoJoCo-Labyrinth"><a href="#更多实验：TORCS-MoJoCo-Labyrinth" class="headerlink" title="更多实验：TORCS, MoJoCo, Labyrinth"></a>更多实验：TORCS, MoJoCo, Labyrinth</h3><p>论文中还做了一些其他实验，但是没有涉及和Experience Replay的比较，这里就略了。</p><h3 id="Scalability"><a href="#Scalability" class="headerlink" title="Scalability"></a>Scalability</h3><p>这部分展现的是 线程个数对收敛速度的影响。</p><p>对于前三种方法而言，令人惊讶的是 加速比竟然大于了 线程数量（这在普通多线程中是不可能的的），这说明多个线程同时更新一个模型可以加速模型的训练。</p><blockquote><p>We believe this is due to positive effect of multiple threads to reduce the bias in one-step methods.</p></blockquote><p>然后对于最有效的 A3C 来说，似乎没有这种效果（论文中似乎没有解释原因），但是加速比也很好了。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210726210743380.png" alt="image-20210726210743380"></p><h3 id="Robustness"><a href="#Robustness" class="headerlink" title="Robustness"></a>Robustness</h3><p>这里展现的是A3C对与learning rate和网络初始化方式不敏感，不需要特别细致的调参。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210727010725301.png" alt="image-20210727010725301"></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>这篇文章提出了一种比较泛用的算法框架，可以非常方便的与online RL算法做结合，并取得不错的效果。</p><p>这篇文章提出的方法具有以下优点：</p><ul><li>考虑到 Experience Replay需要大量的内存存放transition以及GPU，这篇文章的方法运作在CPU上，需要的资源更少，但也具有加速/稳定深度强化学习算法的作用。</li><li>此外，A3C的算法与Replay并没有冲突，所以二者可以结合，达到更好的效果，尤其是某些环境的transiton来之不易，需要多次复用（比如论文中的TORCS）。</li><li>这篇文章提出的算法非常的general，所以有很多future work可以做，比如backward view n-step bootstrapping；与double Q-learning的结合等等</li></ul>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> Actor Critic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[强化学习][python] 在 tic-tac-toe 上实现 蒙特卡洛搜索树 MCTS 算法</title>
      <link href="2021/07/20/ml/qiang-hua-xue-xi/meng-te-qia-luo-sou-suo-shu/"/>
      <url>2021/07/20/ml/qiang-hua-xue-xi/meng-te-qia-luo-sou-suo-shu/</url>
      
        <content type="html"><![CDATA[<h2 id="前置知识-UCB"><a href="#前置知识-UCB" class="headerlink" title="前置知识 UCB"></a>前置知识 UCB</h2><p>强化学习的核心问题之一是 探索&amp;利用 问题。最简单的解决方法是 $\epsilon-greedy$ 。 UCB是比 $\epsilon-greedy$  稍微复杂一点的方法，对于UCB而言，其选择下一个动作的方式为：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210716180827344.png" alt=""></p><blockquote><p> 注意到，不同于 $\epsilon-greedy$ 会产生一个概率分布，然后用 <code>np.random.choice</code> 从其中sample一个动作；UCB是determinstic的，每次选择的动作都是确定的。</p></blockquote><p>观察上式，可以分为两部分：</p><ul><li><strong>exploitation</strong>：$argmax_a{Q_t(a)}$ 就是贪心算法。</li><li><strong>exploration</strong>：$c \sqrt{\frac{\ln t}{N_t(a)}}$ 用于衡量动作 $a$ 的 $Q$值的不确定性。  （ c是超参，一般选 $\sqrt2$，t是iteration次数，$N_t(a)$ 是t时刻动作a被选中的次数）。</li></ul><blockquote><p>可以这么理解 $c \sqrt{\frac{\ln t}{N_t(a)}}$ ： 它衡量了 $Q_t(a)$ 的不确定性， 当 $N_t(a)$ 趋向于无穷大时，根据大数定律，$Q_t(a)$ 是准确的， 所以此时$c \sqrt{\frac{\ln t}{N_t(a)}}$也正好趋向于0。详细的解释可以参看 <a href="https://zhuanlan.zhihu.com/p/32356077">https://zhuanlan.zhihu.com/p/32356077</a></p></blockquote><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>(来源：<a href="https://vgarciasc.github.io/mcts-viz/">tic-tac-toe MCTS可视化 </a>  强烈推荐)</p><p>以tic-tac-toe为例，人（下图中的h）走先手。</p><p>对于左子结点，$ucb = Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} = 1 + \sqrt2 \sqrt{\frac{\ln 2}{1}} \approx 2.18$ </p><p>对于右子结点而言，$ucb = Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} = -1 + \sqrt2 \sqrt{\frac{\ln 2}{1}} \approx 0.18$</p><p> <img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210716182648833.png" alt="image-20210716182648833"></p><h2 id="MCTS算法"><a href="#MCTS算法" class="headerlink" title="MCTS算法"></a>MCTS算法</h2><p>MCTS分为 selection，expansion，simulation，backup。</p><p>我在tic-tac-toe上实现了MCTS，该实现参考了<a href="http://www.incompleteideas.net/609%20dropbox/other%20readings%20and%20resources/MCTS-survey.pdf">A Survey of Monte Carlo Tree Search Methods</a> 中的算法，代码见 <a href="https://github.com/LamForest/machine-learning-reimplementation">github (仍在整理中)</a> 。</p><p>在开始叙述MCTS算法之前，先明确几点:</p><ul><li>MCTS是一个 Decision-time Planning, Rollout Algorithm。这代表 MCTS在遇到每个state之后，为该state做多趟蒙特卡洛模拟，估计出各个action的state-value（即 $Q$） 值之后，选择某个动作，然后丢弃刚学到的所有 state-value值。</li><li>MCTS的数据结构是一颗树，树的每一个结点是一个state，边代表动作，子结点是父结点的afterstate，根结点是智能体当前所处的state。</li><li>对于井字棋，$Q$ 的意义是选择这个action之后，赢的平均概率。</li></ul><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210720185116147.png" alt="MCTS 概念图 From Sutton Book 8.1"></p><h3 id="Selection"><a href="#Selection" class="headerlink" title="Selection"></a>Selection</h3><p>selection：通过Tree-Policy从树中选择一个结点进行expansion。</p><p>这里的Tree-Policy选择的是前置知识中的UCB：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210720191801324.png" alt="image-20210720191801324"></p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210720191814652.png" alt="image-20210720191814652"></p><p>（这里的 $Q$ 指的是累计奖赏，不是平均奖赏，不过区别不大）</p><h4 id="例子-1"><a href="#例子-1" class="headerlink" title="例子"></a>例子</h4><p>在下图中，人走了第一步，现在agent需要做MCTS走第二步。</p><p>在第11次蒙特卡洛时，由于根结点Fully Expanded了，所以选择UCB最大的，即第一个子结点。该子结点是not fully expanded的，所以选择该子结点（下图中红色的结点）</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210716183014526.png" alt="image-20210716183014526"></p><h3 id="Expansion"><a href="#Expansion" class="headerlink" title="Expansion"></a>Expansion</h3><p>Expansion：对于selection选中的结点，生成它的一个子结点，如果有多个可能子结点，随机选择一个。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210720192255614.png" alt="image-20210720192255614"></p><p>（这一步没什么好说的，单纯的new一个结点）</p><h3 id="Simulation"><a href="#Simulation" class="headerlink" title="Simulation"></a>Simulation</h3><p>Simulation：以生成的子结点 $v^{\prime}$ 为起点，利用Rollout Policy（也叫 Default Policy）选择动作，并进入下一个结点，直到达到终止状态，得到reward。对于tic-tac-toe，reward为 -1，0，1。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210720192621210.png" alt="image-20210720192621210"></p><ul><li>Simulation 与 Selection都用了Policy，但是Simulation使用的是 更简单的Policy，比如均匀随机选择一个动作。</li></ul><h3 id="Backup"><a href="#Backup" class="headerlink" title="Backup"></a>Backup</h3><p>Backup: 从根结点到新生成的结点，在树中形成了一条路径。 backup所做的，就是利用simulation得到的reward，更新这条路径上的所有结点（包括根结点和新结点）。</p><p>需要更新的有：</p><ul><li>$N(v)$：状态 $v$ 被backup的总次数。</li><li>$Q(v)$：从 $v$ 出发，获得的累计总奖赏。</li></ul><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210720193046997.png" alt="image-20210720193046997"></p><p>这里特别注意的是，对于tic-tac-toe这种两人游戏，假设根结点处在第1层，那么奇数层是站在智能体的角度，偶数层站在对手的角度。当站在智能体的角度，reward不需要修改；当站在对手的角度，<code>reward = - reward</code>。这里实现的时候要小心。</p><h3 id="决策"><a href="#决策" class="headerlink" title="决策"></a>决策</h3><p>在利用蒙特卡洛模拟学习出了 $Q$ 之后，接着要选择一个动作，选择的方式可以有很多种，这里还是根据UCB的值来选择，选择UCB最大的那个action。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210720195140305.png" alt="image-20210720195140305"></p><h2 id="MCTS玩tic-tac-toe"><a href="#MCTS玩tic-tac-toe" class="headerlink" title="MCTS玩tic-tac-toe"></a>MCTS玩tic-tac-toe</h2><p>下面分别在人走先手和agent走先手的情况下，演示了我实现的MCTS：</p><h3 id="人走先手"><a href="#人走先手" class="headerlink" title="人走先手"></a>人走先手</h3><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210720194609634.png" alt="image-20210720194609634"></p><p>最后打成了平局。</p><h3 id="MCTS-Agent走先手"><a href="#MCTS-Agent走先手" class="headerlink" title="MCTS Agent走先手"></a>MCTS Agent走先手</h3><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210720194737508.png" alt=""></p><p>MCTS Agent赢了（人放了点水）</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><h3 id="迭代次数的影响"><a href="#迭代次数的影响" class="headerlink" title="迭代次数的影响"></a>迭代次数的影响</h3><p>本以为对于tic-tac-toe这样简单的游戏，迭代次数不需要特别大，然而似乎并不是这样，当迭代次数分别为1000，5000，10000时，MCTS给出的最佳走法有概率不一致。</p><p>（我检查了好几遍我的代码，没找到问题所在，不知道是我的实现问题，还是MCTS的缺陷）</p><p>比如，当Human走先手，下了这一步棋之后：</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210717213614947.png" alt="image-20210717213614947"></p><p>现在MCTS需要计算这个state下，最优的下法，当MCTS的迭代次数为1000时，重复100次MCTS算法，MCTS有 77次选择中间这一步，23次选择其他位置。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210717214259384.png" alt="image-20210717214259384"></p><p>当迭代次数为 5000 的时候， MCTS有97次选择中间这一步，3次选择其他位置。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210717214425183.png" alt="image-20210717214425183"></p><p>当迭代次数为 10000 的时候， MCTS有99次选择中间这一步，1次选择其他位置。</p><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210718033026438.png" alt="image-20210718033026438"></p>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> python </tag>
            
            <tag> MCTS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[C++][标准库] 随机数的生成方式、性能对比、mingw的问题</title>
      <link href="2021/07/08/cpp/c-biao-zhun-ku-sui-ji-shu-de-sheng-cheng-fang-shi-xing-neng-dui-bi-mingw-de-wen-ti/"/>
      <url>2021/07/08/cpp/c-biao-zhun-ku-sui-ji-shu-de-sheng-cheng-fang-shi-xing-neng-dui-bi-mingw-de-wen-ti/</url>
      
        <content type="html"><![CDATA[<h3 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h3><p>本篇文章包含了以下内容：</p><ul><li>随机数的两种生成方式：HRNG 、PRNG</li><li>HRNG 和 PRNG 在 C++中的执行时间的比较</li><li>mingw 的 <code>std::random_device</code> 存在的问题</li></ul><h3 id="随机数的生成方式"><a href="#随机数的生成方式" class="headerlink" title="随机数的生成方式"></a>随机数的生成方式</h3><blockquote><p>参考资料：</p><p>[1] <a href="https://en.wikipedia.org/wiki/Random_number_generation">https://en.wikipedia.org/wiki/Random_number_generation</a></p><p>[2] <a href="https://www.zhihu.com/question/20423025/answer/15097735">电脑取随机数是什么原理，是真正的随机数吗？ - 王納米的回答 - 知乎</a></p></blockquote><p>随机数生成（Random Number Generation, RNG）的方式一般有两种，分别为：</p><ul><li><p>硬件生成随机数<a href="https://en.wikipedia.org/wiki/Hardware_random_number_generator">Hardware RNG</a>，原理是用某个仪器一直探测环境中的物理量，将该物理量作为随机数，见[2]。由于人类目前还无法对真实的物理环境进行建模，所以无从预测下一个产生的随机数是什么。因此，HRNG可以看作真随机数，[2]中给出了一个具体的例子。</p><p>另一个例子是 Intel 和 AMD CPU指令集中的 <a href="https://en.wikipedia.org/wiki/RDRAND">RDRAND</a> 指令，该指令基于clock shift生成随机数：</p><blockquote><p>One way to build a <a href="https://en.wikipedia.org/wiki/Hardware_random_number_generator#Clock_drift">hardware random number generator</a> is to use two independent <a href="https://en.wikipedia.org/wiki/Crystal_oscillator">clock crystals</a>, one that for instance ticks 100 times per second and one that ticks 1 million times per second. </p><p>On average the faster crystal will then tick 10,000 times for each time the slower one ticks. <strong>But since clock crystals are not precise, the exact number of ticks will vary.</strong> <strong>That variation can be used to create random bits.</strong> </p><p>For instance, if the number of fast ticks is even, a 0 is chosen, and if the number of ticks is odd, a 1 is chosen. Thus such a 100/1000000 RNG circuit can produce 100 somewhat random bits per second. Typically such a system is biased—it might for instance produce more zeros than ones—and so hundreds of somewhat-random bits are <a href="https://en.wikipedia.org/wiki/Decorrelation">“whitened”</a> to produce a few unbiased bits.</p></blockquote></li><li><p>算法生成随机数，比如c++中的 mt19937梅森旋转算法即为一种软件层面的随机数生成算法算法。如果知道了seed和算法的具体实现，别人就可以知道你生成的随机数序列。所以，又被称为 <a href="https://en.wikipedia.org/wiki/Pseudorandom_number_generator">伪随机数生成器 Pseudo RNG</a>。其他的PRNG算法包括 Xorshift，linear congruential generators等。</p></li></ul><h3 id="HRNG-vs-PRNG"><a href="#HRNG-vs-PRNG" class="headerlink" title="HRNG vs PRNG"></a>HRNG vs PRNG</h3><blockquote><p>参考资料：</p><p>[1] <a href="https://stackoverflow.com/questions/39288595/why-not-just-use-random-device">https://stackoverflow.com/questions/39288595/why-not-just-use-random-device</a></p></blockquote><p>从上面的描述中可以看出，硬件生成的随机数似乎更好一些，是真随机数，而软件层面的随机数容易被人破解。但是在C++代码中，人们一般不直接使用HRNG。而是利用HRNG为PRNG生成一个种子，然后利用PRNG生成随机数，比如如下代码：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">std<span class="token operator">::</span>random_device rd<span class="token punctuation">;</span> <span class="token comment">//linux下，读取/dev/random获取硬件产生的随机数</span>std<span class="token operator">::</span>mt19937 e<span class="token punctuation">&#123;</span><span class="token function">rd</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span class="token punctuation">;</span> <span class="token comment">// or std::default_random_engine e&#123;rd()&#125;; 用HRNG作为PRNG的种子</span>std<span class="token operator">::</span>uniform_int_distribution<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> dist<span class="token punctuation">&#123;</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">&#125;</span><span class="token punctuation">;</span><span class="token function">dist</span><span class="token punctuation">(</span>e<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// get random numbers with PRNG</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>为什么不直接使用 <code>std::random_device</code>呢？ 可以从效率和可解释性两个角度解释了这个问题，见[1]，讲的比较清楚，这里我就不过多解释了，只补充一个两者的执行速度的对比。</p><p>这里用 <a href="https://github.com/google/benchmark">google-benchmark</a> 对比了linux（ubuntu 20.04, gcc 9.2.0)下 <code>std::random_device</code> 和 <code>std::mt19937</code> 的执行速度：</p><ul><li><p>所使用的代码如下：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;benchmark/benchmark.h></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;random></span></span><span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">BM_PRNG</span><span class="token punctuation">(</span>benchmark<span class="token operator">::</span>State<span class="token operator">&amp;</span> state<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    std<span class="token operator">::</span>random_device rd<span class="token punctuation">;</span>    std<span class="token operator">::</span>mt19937 e<span class="token punctuation">&#123;</span><span class="token function">rd</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">&#125;</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">auto</span> _ <span class="token operator">:</span> state<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>        <span class="token function">e</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span class="token function">BENCHMARK</span><span class="token punctuation">(</span>BM_PRNG<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">BM_HRNG</span><span class="token punctuation">(</span>benchmark<span class="token operator">::</span>State<span class="token operator">&amp;</span> state<span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    std<span class="token operator">::</span>random_device rd<span class="token punctuation">;</span>    std<span class="token operator">::</span>uniform_int_distribution<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span> <span class="token function">dist</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">auto</span> _ <span class="token operator">:</span> state<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>        <span class="token function">rd</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">&#125;</span><span class="token punctuation">&#125;</span><span class="token function">BENCHMARK</span><span class="token punctuation">(</span>BM_HRNG<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token function">BENCHMARK_MAIN</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>运行结果如下：</p><pre class="line-numbers language-none"><code class="language-none">Run on (1 X 2494.14 MHz CPU )CPU Caches:  L1 Data 32 KiB (x1)  L1 Instruction 32 KiB (x1)  L2 Unified 4096 KiB (x1)  L3 Unified 28160 KiB (x1)Load Average: 2.70, 3.15, 16.35-----------------------------------------------------Benchmark           Time             CPU   Iterations-----------------------------------------------------BM_PRNG          35.9 ns         17.8 ns     39254461 BM_HRNG          1310 ns          640 ns      1091579<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><ul><li>“真”随机数的耗时大概是“伪”随机数的 37倍，非常之慢。</li></ul><p>这非常类似非对称加密和对称加密，非对称加密（RSA）通常不用于直接加密信息，而是用于加密并交换对称密钥，然后用对称密钥（比如AES-256）交换要传输的信息，因为对称密钥加密解密的速度更快，但是相对更不安全。</p><h3 id="mingw-中-std-random-device的问题"><a href="#mingw-中-std-random-device的问题" class="headerlink" title="mingw 中 std::random_device的问题"></a>mingw 中 std::random_device的问题</h3><p>分别在GCC，MSVC，Mingw中执行下述代码：</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;random></span></span><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;iostream></span></span><span class="token keyword">void</span> <span class="token function">test</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    std<span class="token operator">::</span>random_device rd<span class="token punctuation">;</span>    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token function">rd</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;&lt;</span> <span class="token string">", "</span><span class="token punctuation">;</span>    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token function">rd</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;&lt;</span> <span class="token string">", "</span><span class="token punctuation">;</span>    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token function">rd</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">&#123;</span>    <span class="token function">test</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">test</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token function">test</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="msvc"><a href="#msvc" class="headerlink" title="msvc"></a>msvc</h4><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210706202721923.png" alt="image-20210706202721923"></p><h4 id="gcc"><a href="#gcc" class="headerlink" title="gcc"></a>gcc</h4><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210706203142452.png" alt="image-20210706203142452"></p><h4 id="Mingw-8-1-0"><a href="#Mingw-8-1-0" class="headerlink" title="Mingw 8.1.0"></a>Mingw 8.1.0</h4><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/image-20210706203235855.png" alt="image-20210706203235855"></p><hr><p>显然，gcc和msvc下的random_device每次都产生了不一样的随机数序列。</p><p>然而，mingw下的random_device每次都产生了同样的序列（deterministic）。</p><blockquote><p>A notable implementation where <code>std::random_device</code> is deterministic is old versions of MinGW (<a href="https://sourceforge.net/p/mingw-w64/bugs/338/">bug 338</a>, fixed since GCC 9.2). The latest MinGW Versions can be downloaded from <a href="https://gcc-mcf.lhmouse.com/">GCC with the MCF thread model</a>.</p><p>From: <a href="https://en.cppreference.com/w/cpp/numeric/random/random_device">https://en.cppreference.com/w/cpp/numeric/random/random_device</a></p></blockquote><p>幸运的是，这个问题在mingw 9.2中被修复了。</p><blockquote><p>Add support for additional sources of randomness to std::random_device,<br>to allow using RDSEED for Intel CPUs and rand_s for Windows. When<br>supported these can be selected using the tokens “rdseed” and “rand_s”.<br>For <em>-w64-mingw32 targets the “default” token will now use rand_s, and<br>for other i?86-</em>-<em> and x86_64-</em>-* targets it will try to use “rdseed”<br>first, then “rdrand”, and finally “/dev/urandom”.</p><p>From :  <a href="https://patchwork.ozlabs.org/project/gcc/patch/20190529144517.GA9078@redhat.com/">https://patchwork.ozlabs.org/project/gcc/patch/20190529144517.GA9078@redhat.com/</a></p></blockquote><p>这算mingw的一个bug吗？实际上，并不算，因为C++标准过于宽松，它允许random_device每次产生同样的随机数序列：</p><blockquote><p><code>std::random_device</code> may be implemented in terms of an implementation-defined pseudo-random number engine if a non-deterministic source (e.g. a hardware device) is not available to the implementation. In this case each <code>std::random_device</code> object may generate the same number sequence.</p><p>From: <a href="https://en.cppreference.com/w/cpp/numeric/random/random_device">https://en.cppreference.com/w/cpp/numeric/random/random_device</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> Template </tag>
            
            <tag> C++ STL </tag>
            
            <tag> GNU C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我的布偶猫灰原</title>
      <link href="2021/06/17/others/hui-yuan-de-cheng-chang-ri-ji/"/>
      <url>2021/06/17/others/hui-yuan-de-cheng-chang-ri-ji/</url>
      
        <content type="html"><![CDATA[<p>我们有了一个小猫咪，是一只蓝双布偶猫妹妹！<br><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/20210617021816.png" alt=""></p><h2 id="灰原的档案"><a href="#灰原的档案" class="headerlink" title="灰原的档案"></a>灰原的档案</h2><ul><li>名字：灰原<br><img src="https://img.mix.sina.com.cn/auto/resize?img=https%3A%2F%2Fn.sinaimg.cn%2Fsinakd10100%2F416%2Fw640h576%2F20200416%2Fe7e2-iskepxs3899437.jpg&size=640_0&blur=1&blur_sigma=2" width="20%" height="20%"></li><li>性别：女</li><li>生日：2021年3月30日</li><li>爱好：玩绳子，鞋带，电源线，拖鞋，逗猫棒，翻垃圾桶，晒太阳，</li><li>体重：</li></ul><div class="table-container"><table><thead><tr><th>日期</th><th>2021年6月17号</th><th>2021年6月24号</th><th>2021年7月1号</th></tr></thead><tbody><tr><td>体重</td><td>0.9kg</td><td>1.05kg （+0.15 kg)</td><td>1.25kg （+0.20 kg)</td></tr></tbody></table></div><p> <img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/20210624001939.png" width="40%" height="40%"></p><ul><li>长度：2021年6月15日 大概32cm（从头到尾巴根部）</li></ul><div class="table-container"><table><thead><tr><th>日期</th><th>2021年6月17号</th><th>2021年6月24号</th></tr></thead><tbody><tr><td>长度</td><td>31cm</td><td>33cm (+ 2cm)</td></tr></tbody></table></div><ul><li>食物：皇家猫奶糕、鸡胸肉、熟鸡蛋黄</li></ul><h2 id="一些照片"><a href="#一些照片" class="headerlink" title="一些照片"></a>一些照片</h2><h3 id="2021年6月17日"><a href="#2021年6月17日" class="headerlink" title="2021年6月17日"></a>2021年6月17日</h3><p><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/20210617021900.png" alt=""><br><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/70d2a7561c16ce82f090638910f1301.jpg" alt=""><br><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/045e4d296083a2b1bd0590ecb5cd3c2.jpg" alt=""><br><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/20210617022249.png" alt=""><br><img src="https://gitee.com/getleft/pics/raw/master/classnotes_2/20210617022322.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 灰原 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 灰原 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>アークナイツ</title>
      <link href="2021/05/26/others/ming-ri-fang-zhou-ji-lu/"/>
      <url>2021/05/26/others/ming-ri-fang-zhou-ji-lu/</url>
      
        <content type="html"><![CDATA[<h3 id="5-23"><a href="#5-23" class="headerlink" title="5-23"></a>5-23</h3><p>新手池2抽，出了2次洁哥</p><p><img src="https://gitee.com/getleft/pics/raw/master/cs229/image-20210526135953342.png" alt="image-20210526135953342"></p><p><img src="https://gitee.com/getleft/pics/raw/master/cs229/image-20210526135937644.png" alt="image-20210526135937644"></p><p>联合池单抽了几次，出了温蒂（前期用不上，蛋疼）</p><p><img src="https://gitee.com/getleft/pics/raw/master/cs229/image-20210526140052834.png" alt="image-20210526140052834"></p><h3 id="5-24"><a href="#5-24" class="headerlink" title="5-24"></a>5-24</h3><p>第一次出资深、高级资深tag，竟然还是一起出的，只选了高级资深赌一把，没想到还是莫斯提马：</p><p><img src="https://gitee.com/getleft/pics/raw/master/cs229/image-20210526140229678.png" alt="image-20210526140229678"></p><p><img src="https://gitee.com/getleft/pics/raw/master/cs229/image-20210526140240185.png" alt="image-20210526140240185"></p><h3 id="5-25"><a href="#5-25" class="headerlink" title="5-25"></a>5-25</h3><p>没忍住，十抽了联合寻访，想出棘刺，却出了铃兰，也还行吧。。。就是不太用得上，赫墨也重复了</p><p><img src="https://gitee.com/getleft/pics/raw/master/cs229/image-20210526140356752.png" alt="image-20210526140356752"></p><h3 id="5-26"><a href="#5-26" class="headerlink" title="5-26"></a>5-26</h3><p>今天出了几个好tag，再梅尔和凛冬之间选了凛冬，希望不会后悔。</p><p>（忘记截图了）</p>]]></content>
      
      
      <categories>
          
          <category> アークナイツ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> アークナイツ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAMES202作业2旋转部分</title>
      <link href="2021/05/05/cg/games202-zuo-ye-2-xuan-zhuan-bu-fen/"/>
      <url>2021/05/05/cg/games202-zuo-ye-2-xuan-zhuan-bu-fen/</url>
      
        <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>本文是写在做完<a href="https://www.bilibili.com/video/BV1YK4y1T7yY">GAMES202 高质量实时渲染</a>作业2之后的总结。通过不严谨的数学公式，从球谐的旋转不变性出发，一步步推导出如何求旋转后的系数${c_l^m}^{\prime}$。</p><blockquote><p>非常感谢闫老师和助教们的课程和代码框架！！！</p></blockquote><p>效果展示：</p><p>(注：环境光为Cornell Box)</p><p><img src="https://gitee.com/getleft/pics/raw/master/imgs/%E6%AD%A3%E7%A1%AE%E7%9A%84%E6%97%8B%E8%BD%AC%E6%95%88%E6%9E%9C_small.gif" alt=""></p><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>定义在球面上的环境光 $L_i(\omega_i)$可以被展开成球谐基函数的线性组合：</p><script type="math/tex; mode=display">L_i(\omega_i) = \sum_{l  = 0}^{+\infty} \sum_{m  = -l}^{l} c_l^mB_l^m(\omega_i)</script><p>假设现在有一个旋转矩阵 $R^{-1}$，现在有一个旋转矩阵，将环境光 $L_i(\omega_i)$进行了旋转，得到了 $L_i(R\omega_i)$。</p><blockquote><p>注：将$y = x^2$向x轴正半轴方向移动了a个单位后，变为了$y = (x-a)^2$，而不是$y = (x+a)^2$，旋转也是这个道理。</p></blockquote><p><strong>问题来了</strong>，<strong>如何求得 $L_i(R\omega_i)$ 的球谐系数${c_l^m}^{\prime}$？</strong></p><p>最简单的想法，重新计算 $L_i(R\omega_i)$ 对每个球谐基函数的投影：</p><script type="math/tex; mode=display">{c_l^m}^{\prime} = \int_{\Omega} B_l^m(\omega_i) L_i(R\omega_i) d\omega_i</script><p>假如环境光源在每一帧都在旋转，那么每一帧渲染之前，都要进行这么一个积分，这是无法接受的。</p><blockquote><p>回顾之前的内容，该积分使用的是黎曼和的形式求解，复杂度很高。</p></blockquote><p>是否有更简单的方法计算${c_l^m}^{\prime}$ 呢？</p><p><strong>比如，是否存在某个方法，可以直接从${c_l^m}$得到${c_l^m}^{\prime}$呢？</strong></p><p>这就是本文探讨的话题。</p><h2 id="什么是旋转不变性"><a href="#什么是旋转不变性" class="headerlink" title="什么是旋转不变性"></a>什么是旋转不变性</h2><p>对于某个球谐基函数$B_l^m(\omega_i)$，假设现在有一个旋转矩阵 $R^{-1}$ 将$B_l^m(\omega_i)$进行了旋转，</p><blockquote><p>注：旋转应该是指，按照空间中过原点的某一个轴旋转了某个角度，由于环境光是无限远的，所以转轴虽然在空间中任意位置，但可以视作在原点。</p></blockquote><p>那么旋转后的函数的表达式为$B_l^m(R^{}\omega_i)$。</p><p><strong>旋转不变性</strong>指的是，$B_l^m(R\omega_i)$可以被拆分成同阶（band）的其他基函数的线性组合，而与其他阶的基函数无关，即：</p><script type="math/tex; mode=display">B_l^m(R\omega_i) = \sum_{k  = -l}^{l} a_k B_l^m(\omega_i) \tag 1</script><h2 id="从旋转不变性出发，开始推导"><a href="#从旋转不变性出发，开始推导" class="headerlink" title="从旋转不变性出发，开始推导"></a>从旋转不变性出发，开始推导</h2><p>根据球谐展开的概念，对环境光 $L_i(\omega_i)$，我们有：</p><script type="math/tex; mode=display">\begin{align}L_i(\omega_i) &= \sum_{l  = 0}^{+\infty} \sum_{m  = -l}^{l} c_l^mB_l^m(\omega_i) \\\end{align}</script><p>在这篇文章里，我们关注 $l = 1$的情况，忽略其他阶，则上式可以写成：</p><script type="math/tex; mode=display">\begin{align}L_i(\omega_i) &= ... + c_1^{-1}B_1^{-1}(\omega_i) + c_1^{0}B_1^{0}(\omega_i) + c_1^{1}B_1^{1}(\omega_i) +...\end{align}</script><p>在继续进行之前，要先达成一个共识（我不确定这是否需要证明）：</p><ul><li><p>考虑这么一件事情：如果将 $L_i(R\omega_i)$在 $B_l^m(R\omega_i)$上进行分解，所得到的球谐系数是多少呢？</p></li><li><p>由于我们将光源和球谐基函数同时用 $R$ 进行了旋转，那么它们之间的相对关系应该是不会发生变化的，也就是球谐系数仍然是 $c_l^m$：</p></li></ul><script type="math/tex; mode=display">c_l^m = \int_{\Omega} B_l^m(R\omega_i) L_i(R\omega_i) d\omega_i</script><p>有了这个共识，我们就可以直接获得 $L_i(R\omega_i)$在 $B_l^m(R\omega_i)$上进行分解的球谐系数，就是${c_l^m}$（同样只关注 第1阶）：</p><script type="math/tex; mode=display">\begin{align}L_i(R\omega_i) &= ... + c_1^{-1}B_1^{-1}(R\omega_i) + c_1^{0}B_1^{0}(R\omega_i) + c_1^{1}B_1^{1}(R\omega_i) + ... \tag 2\end{align}</script><p>有了公式（2）后，借助球谐函数的旋转不变性（式1），我们可以将$B_l^m(R\omega_i)$ 表示为同band其他基函数的线性组合：</p><script type="math/tex; mode=display">\begin{align}B_1^{-1}(R\omega_i) &= \sum_{k  = -1}^{1} x_k B_1^k(\omega_i) =  x_{-1}B_1^{-1}(\omega_i) + x_{0}B_1^{0}(\omega_i) + x_{1}B_1^{1}(\omega_i) \tag 3\\B_1^{0}(R\omega_i) &= \sum_{k  = -1}^{1} y_k B_1^k(\omega_i) =  y_{-1}B_1^{-1}(\omega_i) + y_{0}B_1^{0}(\omega_i) + y_{1}B_1^{1}(\omega_i)\\B_1^{1}(R\omega_i) &= \sum_{k  = -1}^{1} z_k B_1^k(\omega_i) =  z_{-1}B_1^{-1}(\omega_i) + z_{0}B_1^{0}(\omega_i) + z_{1}B_1^{1}(\omega_i)\end{align}</script><p>假设 $x_k,y_k, z_k$全部已知，那将上面的式子代入式（2），有：</p><script type="math/tex; mode=display">\begin{aligned}L_i(R\omega_i) = ... &+ c_1^{-1}\left(x_{-1}B_1^{-1}(\omega_i) + x_{0}B_1^{0}(\omega_i) + x_{1}B_1^{1}(\omega_i) \right) \\&+ c_1^{0} \left(y_{-1}B_1^{-1}(\omega_i) + y_{0}B_1^{0}(\omega_i) + y_{1}B_1^{1}(\omega_i) \right)\\&+ c_1^{1} \left(z_{-1}B_1^{-1}(\omega_i) + z_{0}B_1^{0}(\omega_i) + z_{1}B_1^{1}(\omega_i) \right)\\&+...\end{aligned}</script><p>进行一下合并同类项：</p><script type="math/tex; mode=display">\begin{align}L_i(R\omega_i) = ... &+ (c_1^{-1}x_{-1} + c_1^{0}y_{-1} + c_1^{1}z_{-1})B_1^{-1}(\omega_i)\\&+ (c_1^{-1}x_{0} + c_1^{0}y_{0} + c_1^{1}z_{0})B_1^{0}(\omega_i)\\&+ (c_1^{-1}x_{1} + c_1^{0}y_{1} + c_1^{1}z_{1})B_1^{1}(\omega_i)\\&+...\end{align}</script><p>令 ${c_1^{-1}}^{\prime} = (c_1^{-1}x_{-1} + c_1^{0}y_{-1} + c_1^{1}z_{-1})$ </p><p>${c_1^{0}}^{\prime} = (c_1^{-1}x_{0} + c_1^{0}y_{0} + c_1^{1}z_{0})$</p><p> ${c_1^{1}}^{\prime} = (c_1^{-1}x_{1} + c_1^{0}y_{1} + c_1^{1}z_{1})$ </p><p>则上式被重写成：</p><script type="math/tex; mode=display">\begin{align}L_i(R\omega_i) = ... + {c_1^{-1}}^{\prime}B_1^{-1}(\omega_i)+ {c_1^{0}}^{\prime}B_1^{0}(\omega_i)+ {c_1^{1}}^{\prime}B_1^{1}(\omega_i)&+...\end{align}</script><p>这样，我们就得到了旋转之后，各个球谐基函数的系数${c_l^m}^{\prime}$了。</p><hr><p>下一节定义了 $M_R$, 利用 $M_R$可以将 ${c_1^{-1}}^{\prime},{c_1^{0}}^{\prime},{c_1^{1}}^{\prime} $用矩阵乘法表示：</p><script type="math/tex; mode=display">\begin{pmatrix}{c_1^{-1}}^{\prime}\\{c_1^{0}}^{\prime}\\{c_1^{1}}^{\prime}\\\end{pmatrix} = {M_R}^T\begin{pmatrix}c_1^{-1}\\c_1^{0}\\c_1^{1}\\\end{pmatrix} \tag 4</script><h2 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h2><p>之前，我们假设 $x_k,y_k, z_k$全部已知，但是它们其实是未知的，接下来我们来求解它们。</p><p>以式（3）为例，在这里，我们要求的是 $x_{-1}, x_0, x_1$，如何求解呢？</p><p>其实很简单，3个未知数，需要3个方程，我们只需要随便找3个 $n_0, n_1, n_2$代入，得到三元一次方程组。因为 $B_l^m(R\omega_i)$是已知的函数，所以上述方程组可以很简单的解出来：</p><script type="math/tex; mode=display">\begin{align}\left\{\begin{array}{**lr**} B_1^{-1}(Rn_0) =   x_{-1}B_1^{-1}(n_0) + x_{0}B_1^{0}(n_0) + x_{1}B_1^{1}(n_0) \\B_1^{-1}(Rn_1) =   x_{-1}B_1^{-1}(n_1) + x_{0}B_1^{0}(n_1) + x_{1}B_1^{1}(n_1) \\B_1^{-1}(Rn_2) =   x_{-1}B_1^{-1}(n_2) + x_{0}B_1^{0}(n_2) + x_{1}B_1^{1}(n_2) \\\end{array}  \tag 5\right.  \end{align}</script><p>同样的，对于 $y_k$ 和 $z_k$，也带入 $n_0, n_1, n_2$，可以解出$y_k$ 和 $z_k$。（求解$y_k$ 和 $z_k$ 时，选取的 $n_0, n_1, n_2$不一定要是一样的，但是选一样的接下来比较方便）。</p><p><strong>式（5）就是求解 $x_k,y_k, z_k$ 的核心。</strong>本节剩下的内容就是对其进行向量化，变成矩阵运算的形式，这样一来我们可以少写一些循环语句，二来运算也会快一些。</p><hr><p>为了求解 $x_k,y_k, z_k$，我们用到了3个三元一次方程组，每个方程组3个方程，这9个方程可以被写成更简洁的矩阵形式（前提是，三个方程组所用的$n_0, n_1, n_2$是一致的）：</p><script type="math/tex; mode=display">\begin{pmatrix}B_1^{-1}(Rn_0) & B_1^{-1}(Rn_1)  & B_1^{-1}(Rn_2)\\B_1^{0}(Rn_0) & B_1^{0}(Rn_1)  & B_1^{0}(Rn_2)\\B_1^{1}(Rn_0) & B_1^{1}(Rn_1)  & B_1^{1}(Rn_2)\\\end{pmatrix} = \begin{pmatrix}x_{-1} & x_{0} & x_1 \\y_{-1} & y_{0} & y_1 \\z_{-1} & z_{0} & z_1 \\\end{pmatrix}\begin{pmatrix}B_1^{-1}(n_0) & B_1^{-1}(n_1)  & B_1^{-1}(n_2)\\B_1^{0}(n_0) & B_1^{0}(n_1)  & B_1^{0}(n_2)\\B_1^{1}(n_0) & B_1^{1}(n_1)  & B_1^{1}(n_2)\\\end{pmatrix}</script><p>为了表述方便，这里定义了 $P(\omega_i)$ （有的地方也称之为对$\omega_i$的投影）和$M_R$：</p><script type="math/tex; mode=display">P(\omega_i) =\begin{pmatrix}B_1^{-1}(\omega_i) \\B_1^{0}(\omega_i) \\B_1^{1}(\omega_i) \\\end{pmatrix}, M_R = \begin{pmatrix}x_{-1} & x_{0} & x_1 \\y_{-1} & y_{0} & y_1 \\z_{-1} & z_{0} & z_1 \\\end{pmatrix}</script><p>利用新定义的符号，可以继续写成更简洁的形式：</p><script type="math/tex; mode=display">\begin{bmatrix}P(Rn_0) & P(Rn_1) & P(Rn_2)  \end{bmatrix} = M_R\begin{bmatrix}P(n_0) & P(n_1) & P(n_2)\end{bmatrix}</script><p>再记 $S = [P(Rn_0) \quad P(Rn_1) \quad  P(Rn_2)] $ ， $A = [P(n_0) \quad P(n_1) \quad P(n_2) ]$ ：</p><script type="math/tex; mode=display">S = M_R A</script><blockquote><p>注：这里 $S, M_R, A$ 都是3x3的方阵。如果我们在处理2阶球谐函数的时候， $S M_R A$ 都变为了5x5的方阵，3阶则7x7，以此类推。</p></blockquote><p>至此， $M_R$ （即$x_k,y_k, z_k$的矩阵形式）的解变得更加一目了然了：</p><script type="math/tex; mode=display">M_R = S A^{-1} \tag 6</script><blockquote><p>式（6）和式（5）其实是一样的，只是变成了矩阵的形式。</p></blockquote><h2 id="具体的算法"><a href="#具体的算法" class="headerlink" title="具体的算法"></a>具体的算法</h2><p>根据 $M_R = S A^{-1}$，要求 $M_R$ 我们只需算出 $S$ 和 $A$即可。所以有了下面的算法：</p><ol><li>随机选3个向量，这里选择 $n_0 = [0,0,1], n_1 = [0,1,0], n_2 = [1,0,0]$</li><li>代入 $P(\omega_i)$求得 $A$，并得到 $A^{-1}$</li><li>利用旋转矩阵，求得 $Rn_k$，并带入  $P(\omega_i)$求得 $S$</li><li>$M_R = S A^{-1}$</li><li>给定空间中任意一渲染点的坐标 $p$ 的在未旋转光源下的系数 $c = ({c_1^{-1}}^{},{c_1^{0}}^{},{c_1^{1}}^{})^T $, $c^{\prime} = {M_R}^T c$。</li></ol><p>更具体的例子可以参考[1][1]文章尾部的示例一节。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>关于这部分，有几个问题我也没完全搞懂。</p><ol><li><p>乘以 $M_R$还是 ${M_R}^T$？[1][1]和作业2的任务书里面都是乘以 $M_R$，但是我上面的推导的结果是${M_R}^T$，我实现的时候对比了一下，乘以${M_R}^T$的结果才是对的，乘以${M_R}$的结果会使得渲染的结果与模型的旋转方向相反。</p></li><li><p>不同轴，如果使用同一个RotationMatrix进行模型的旋转和球谐系数的计算，会使得它们在绕不同轴旋转。。。这我暂时不能理解。</p></li></ol><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://zhuanlan.zhihu.com/p/51267461">[1]一种简易的旋转球谐函数系数的方法</a>    </p>]]></content>
      
      
      <categories>
          
          <category> 图形学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图形学 </tag>
            
            <tag> PRT </tag>
            
            <tag> Real Time Rendering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>作业2补充</title>
      <link href="2021/05/04/cg/ass2/"/>
      <url>2021/05/04/cg/ass2/</url>
      
        <content type="html"><![CDATA[<h3 id="作业2的提高部分"><a href="#作业2的提高部分" class="headerlink" title="作业2的提高部分"></a>作业2的提高部分</h3><p>Interreflection:<br><img src="CornellBox_inter.gif" alt=""></p><p>Shadowed:<br><img src="正确的旋转效果2.gif" alt=""></p><p>Unshadowed:<br><img src="CornellBox_unshadowed.gif" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 图形学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 图形学 </tag>
            
            <tag> PRT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>胡适日记 2021年5月</title>
      <link href="2021/05/01/diary/2021-nian-5-yue/"/>
      <url>2021/05/01/diary/2021-nian-5-yue/</url>
      
        <content type="html"><![CDATA[<h3 id="5-1-5-7"><a href="#5-1-5-7" class="headerlink" title="5.1-5.7"></a>5.1-5.7</h3><h4 id="5-1"><a href="#5-1" class="headerlink" title="5.1"></a>5.1</h4><p>劳动节。学习了PRT。<br>对女朋友生了大气，对不起。</p><h4 id="5-2"><a href="#5-2" class="headerlink" title="5.2"></a>5.2</h4><p>完成了作业2的基础部分的大半（Light项预计算，Shadow和Unshadow的传输项预计算），还是比较容易的，会使用现成的函数就好了。明天要写JS了，想想就蛋疼。还了解到了Yan的黑历史，太牛了。</p><p>晚上吃了10个麦辣鸡翅，真好吃。</p><p>爸爸买了灭蚊灯，不知道效果怎么样，最近被咬的好惨。</p><p>决定在博客上写日记了，不过要好好藏起来</p><h4 id="5-11"><a href="#5-11" class="headerlink" title="5.11"></a>5.11</h4><p>今天看完了 101 P19 和 P20.这两个视频都是科普视频，光场相机，色彩空间，color matching，没啥意思，就是听听看。</p><p>接下来的学习路线是什么呢？</p><p>c++：<br>根据面经和知乎搜藏的查漏补缺</p><p>图形学：</p><ol><li>把当前欠的课补上</li><li>游戏引擎的课程 GAMES 201</li><li>经典论文看原文。哪些是经典的论文呢？大家都推荐的，以及引擎中会用到的一些常用算法</li><li>对于引擎中常用的算法，研究引擎的实现，有需要的时候可以自己实现一下</li><li>opengl的学习，这点暂时没头绪。 learnopengl还是从项目中学习。</li><li>小项目：选自己感兴趣的小项目做一下。比如101的课设</li><li>根据面经查漏补缺</li></ol><p>cv：</p>]]></content>
      
      
      <categories>
          
          <category> 汇编 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 汇编 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>[C++] 完美转发 = 引用折叠 + 万能引用 + std::forward</title>
      <link href="2021/04/29/cpp/wan-mei-zhuan-fa-yin-yong-zhe-die-wan-neng-yin-yong-std-forward/"/>
      <url>2021/04/29/cpp/wan-mei-zhuan-fa-yin-yong-zhe-die-wan-neng-yin-yong-std-forward/</url>
      
        <content type="html"><![CDATA[<h3 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h3><p>完美转发是一个比较简单，却又比较复杂的东西。</p><p>简单之处在于理解<strong>动机</strong>：C++为什么需要完美转发？</p><p>复杂之处在于理解<strong>原理</strong>：完美转发基于万能引用，引用折叠以及std::forward模板函数。</p><p>本文将会结合GCC源码，详细解读完美转发的动机和原理。</p><h3 id="动机：C-为什么需要完美转发？"><a href="#动机：C-为什么需要完美转发？" class="headerlink" title="动机：C++为什么需要完美转发？"></a>动机：C++为什么需要完美转发？</h3><p>我们从一个简单的例子出发。<br>假设有这么一种情况，用户一般使用testForward函数，testForward什么也不做，只是简单的转调用到print函数。</p><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">T</span><span class="token operator">></span><span class="token keyword">void</span> <span class="token function">print</span><span class="token punctuation">(</span>T <span class="token operator">&amp;</span> t<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"Lvalue ref"</span> <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">T</span><span class="token operator">></span><span class="token keyword">void</span> <span class="token function">print</span><span class="token punctuation">(</span>T <span class="token operator">&amp;&amp;</span> t<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"Rvalue ref"</span> <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">T</span><span class="token operator">></span><span class="token keyword">void</span> <span class="token function">testForward</span><span class="token punctuation">(</span>T <span class="token operator">&amp;&amp;</span> v<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>     <span class="token function">print</span><span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment">//v此时已经是个左值了,永远调用左值版本的print</span>    <span class="token function">print</span><span class="token punctuation">(</span>std<span class="token operator">::</span>forward<span class="token operator">&lt;</span>T<span class="token operator">></span><span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">//本文的重点</span>    <span class="token function">print</span><span class="token punctuation">(</span>std<span class="token operator">::</span><span class="token function">move</span><span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">//永远调用右值版本的print</span>    std<span class="token operator">::</span>cout <span class="token operator">&lt;&lt;</span> <span class="token string">"======================"</span> <span class="token operator">&lt;&lt;</span> std<span class="token operator">::</span>endl<span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span> <span class="token operator">*</span> argv<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">&#123;</span>    <span class="token keyword">int</span> x <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span>    <span class="token function">testForward</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">//实参为左值</span>    <span class="token function">testForward</span><span class="token punctuation">(</span>std<span class="token operator">::</span><span class="token function">move</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">//实参为右值</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>上面的程序的运行结果：<br><pre class="line-numbers language-none"><code class="language-none">Lvalue refLvalue refRvalue ref&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;Lvalue refRvalue refRvalue ref&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>用户希望<code>testForward(x);</code>最终调用的是左值版本的print，而<code>testForward(std::move(x));</code>最终调用的是右值版本的print。</p><p><strong>可惜的是，在testForward中，虽然参数v是右值类型的，但此时v在内存中已经有了位置，所以v其实是个左值！</strong></p><p>所以，<code>print(v)</code>永远调用左值版本的print，与用户的本意不符。<code>print(std::move(v));</code>永远调用右值版本的print，与用户的本意也不符。只有<code>print(std::forward&lt;T&gt;(v));</code>才符合用户的本意，这就是本文的主题。</p><p>不难发现，本质问题在于，左值右值在函数调用时，都转化成了左值，使得函数转调用时无法判断左值和右值。</p><p>在STL中，随处可见这种问题。比如C++11引入的<code>emplace_back</code>，它接受左值也接受右值作为参数，接着，它转调用了空间配置器的construct函数，而construct又转调用了<code>placement new</code>，<code>placement new</code>根据参数是左值还是右值，决定调用拷贝构造函数还是移动构造函数。</p><p>这里要保证从<code>emplace_back</code>到<code>placement new</code>，参数的左值和右值属性保持不变。这其实不是一件简单的事情。</p><h3 id="前置知识-引用折叠-万能引用"><a href="#前置知识-引用折叠-万能引用" class="headerlink" title="前置知识 引用折叠 万能引用"></a>前置知识 引用折叠 万能引用</h3><p>C++ Primer 里面写的比较容易理解，在P608（我的是第5版）。<br>略</p><h3 id="原理：完美转发"><a href="#原理：完美转发" class="headerlink" title="原理：完美转发"></a>原理：完美转发</h3><p>std::forward不是独自运作的，在我的理解里，完美转发 = std::forward + 万能引用 + 引用折叠。三者合一才能实现完美转发的效果。</p><p>std::forward的正确运作的前提，是引用折叠机制，为T &amp;&amp;类型的万能引用中的模板参数T赋了一个恰到好处的值。我们用T去指明std::forward<T>的模板参数，从而使得std::forward返回的是正确的类型。</p><p>当然，我们还是先回到一开始的例子。</p><h4 id="testForward-x"><a href="#testForward-x" class="headerlink" title="testForward(x)"></a>testForward(x)</h4><p>回到上面的例子。先考虑<code>testForward(x);</code>这一行代码。</p><h5 id="step-1-实例化testForward"><a href="#step-1-实例化testForward" class="headerlink" title="step 1 实例化testForward"></a>step 1 实例化testForward</h5><p>根据万能引用的实例化规则，实例化的testForward大概长这样：<br><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">T <span class="token operator">=</span> <span class="token keyword">int</span> <span class="token operator">&amp;</span><span class="token keyword">void</span> <span class="token function">testForward</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">&amp;</span> <span class="token operator">&amp;&amp;</span> v<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>    <span class="token function">print</span><span class="token punctuation">(</span>std<span class="token operator">::</span>forward<span class="token operator">&lt;</span>T<span class="token operator">></span><span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><br>又根据引用折叠，上面的等价于下面的代码：<br><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">T <span class="token operator">=</span> <span class="token keyword">int</span> <span class="token operator">&amp;</span><span class="token keyword">void</span> <span class="token function">testForward</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">&amp;</span> v<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>    <span class="token function">print</span><span class="token punctuation">(</span>std<span class="token operator">::</span>forward<span class="token operator">&lt;</span><span class="token keyword">int</span> <span class="token operator">&amp;</span><span class="token operator">></span><span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><br>如果你正确的理解了引用折叠，那么这一步是很好理解的。</p><h5 id="step-2-实例化std-forward"><a href="#step-2-实例化std-forward" class="headerlink" title="step 2 实例化std::forward"></a>step 2 实例化std::forward</h5><blockquote><p>注：C++ Primer：forward必须通过显式模板实参来调用，不能依赖函数模板参数推导。</p></blockquote><p>接下来我们来看下<code>std::forward</code>在libstdc++中的实现：<br><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token number">68</span>   <span class="token comment">/**69    *  @brief  Forward an lvalue.70    *  @return The parameter cast to the specified type.71    *72    *  This function is used to implement "perfect forwarding".73    */</span><span class="token number">74</span>   <span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">_Tp</span><span class="token operator">></span><span class="token number">75</span>     <span class="token keyword">constexpr</span> _Tp<span class="token operator">&amp;&amp;</span><span class="token number">76</span>     <span class="token function">forward</span><span class="token punctuation">(</span><span class="token keyword">typename</span> <span class="token class-name">std</span><span class="token operator">::</span>remove_reference<span class="token operator">&lt;</span>_Tp<span class="token operator">></span><span class="token operator">::</span>type<span class="token operator">&amp;</span> __t<span class="token punctuation">)</span> <span class="token keyword">noexcept</span><span class="token number">77</span>     <span class="token punctuation">&#123;</span> <span class="token keyword">return</span> <span class="token keyword">static_cast</span><span class="token operator">&lt;</span>_Tp<span class="token operator">&amp;&amp;</span><span class="token operator">></span><span class="token punctuation">(</span>__t<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>由于Step1中我们调用<code>std::forward&lt;int &amp;&gt;</code>，所以此处我们代入<code>T = int &amp;</code>，我们有：<br><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token keyword">constexpr</span> <span class="token keyword">int</span> <span class="token operator">&amp;</span> <span class="token operator">&amp;&amp;</span> <span class="token comment">//折叠</span><span class="token function">forward</span><span class="token punctuation">(</span><span class="token keyword">typename</span> <span class="token class-name">std</span><span class="token operator">::</span>remove_reference<span class="token operator">&lt;</span><span class="token keyword">int</span> <span class="token operator">&amp;</span><span class="token operator">></span><span class="token operator">::</span>type<span class="token operator">&amp;</span> __t<span class="token punctuation">)</span> <span class="token keyword">noexcept</span> <span class="token comment">//remove_reference的作用与名字一致，不过多解释</span> <span class="token punctuation">&#123;</span> <span class="token keyword">return</span> <span class="token keyword">static_cast</span><span class="token operator">&lt;</span><span class="token keyword">int</span> <span class="token operator">&amp;</span> <span class="token operator">&amp;&amp;</span><span class="token operator">></span><span class="token punctuation">(</span>__t<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">&#125;</span> <span class="token comment">//折叠</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><br>这里又发生了2次引用折叠，所以上面的代码等价于：<br><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token keyword">constexpr</span> <span class="token keyword">int</span> <span class="token operator">&amp;</span> <span class="token comment">//折叠</span><span class="token function">forward</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">&amp;</span> __t<span class="token punctuation">)</span> <span class="token keyword">noexcept</span> <span class="token comment">//remove_reference的作用与名字一致，不过多解释</span> <span class="token punctuation">&#123;</span> <span class="token keyword">return</span> <span class="token keyword">static_cast</span><span class="token operator">&lt;</span><span class="token keyword">int</span> <span class="token operator">&amp;</span><span class="token operator">></span><span class="token punctuation">(</span>__t<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">&#125;</span> <span class="token comment">//折叠</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><br>所以最终<code>std::forward&lt;int &amp;&gt;(v)</code>的作用就是将参数强制转型成<code>int &amp;</code>，而<code>int &amp;</code>为左值。所以，调用左值版本的print。</p><h4 id="testForward-std-move-x"><a href="#testForward-std-move-x" class="headerlink" title="testForward(std::move(x))"></a>testForward(std::move(x))</h4><p>接下来，考虑<code>testForward(std::move(x))</code>的情况。</p><h5 id="step-1-实例化testForward-1"><a href="#step-1-实例化testForward-1" class="headerlink" title="step 1 实例化testForward"></a>step 1 实例化testForward</h5><p><code>testForward(std::move(x))</code>也就是<code>testForward(static_cast&lt;int &amp;&amp;&gt;(x))</code>。根据万能引用的实例化规则，实例化的testForward大概长这样：<br><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp">T <span class="token operator">=</span> <span class="token keyword">int</span> <span class="token keyword">void</span> <span class="token function">testForward</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">&amp;&amp;</span> v<span class="token punctuation">)</span><span class="token punctuation">&#123;</span>    <span class="token function">print</span><span class="token punctuation">(</span>std<span class="token operator">::</span>forward<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span><span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><br>万能引用绑定到右值上时，不会发生引用折叠，所以这里没有引用折叠。</p><h5 id="step-2-实例化std-forward-1"><a href="#step-2-实例化std-forward-1" class="headerlink" title="step 2 实例化std::forward"></a>step 2 实例化std::forward</h5><blockquote><p>注：C++ Primer：forward必须通过显式模板实参来调用，不能依赖函数模板参数推导。</p><p>这里用到的std::forward的代码和上面的一样，故略去。</p><p>由于Step1中我们调用<code>std::forward&lt;int&gt;</code>，所以此处我们代入<code>T = int</code>，我们有：<br><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token keyword">constexpr</span> <span class="token keyword">int</span> <span class="token operator">&amp;&amp;</span> <span class="token function">forward</span><span class="token punctuation">(</span><span class="token keyword">typename</span> <span class="token class-name">std</span><span class="token operator">::</span>remove_reference<span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">></span><span class="token operator">::</span>type<span class="token operator">&amp;</span> __t<span class="token punctuation">)</span> <span class="token keyword">noexcept</span> <span class="token comment">//remove_reference的作用与名字一致，不过多解释</span> <span class="token punctuation">&#123;</span> <span class="token keyword">return</span> <span class="token keyword">static_cast</span><span class="token operator">&lt;</span><span class="token keyword">int</span> <span class="token operator">&amp;&amp;</span><span class="token operator">></span><span class="token punctuation">(</span>__t<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><br>这里又发生了2次引用折叠，所以上面的代码等价于：<br><pre class="line-numbers language-cpp" data-language="cpp"><code class="language-cpp"><span class="token keyword">constexpr</span> <span class="token keyword">int</span> <span class="token operator">&amp;&amp;</span><span class="token function">forward</span><span class="token punctuation">(</span><span class="token keyword">int</span> <span class="token operator">&amp;</span> __t<span class="token punctuation">)</span> <span class="token keyword">noexcept</span> <span class="token comment">//remove_reference的作用与名字一致，不过多解释</span> <span class="token punctuation">&#123;</span> <span class="token keyword">return</span> <span class="token keyword">static_cast</span><span class="token operator">&lt;</span><span class="token keyword">int</span> <span class="token operator">&amp;&amp;</span><span class="token operator">></span><span class="token punctuation">(</span>__t<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">&#125;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><br>所以最终<code>std::forward&lt;int&gt;(v)</code>的作用就是将参数强制转型成<code>int &amp;&amp;</code>，而<code>int &amp;&amp;</code>为右值。所以，调用右值版本的print。</p></blockquote><h3 id="写在后面"><a href="#写在后面" class="headerlink" title="写在后面"></a>写在后面</h3><p>在GCC源码中，std::forward还有第二个版本：<a href="https://gcc.gnu.org/onlinedocs/gcc-5.4.0/libstdc++/api/a01395_source.html#l00087">link</a>，分析的方法与本文一致，这里就不讲了。。</p><p>右值的概念其实很微妙，一旦某个右值，有了名字，也就在内存中有了位置，它就变成了1个左值。但它又是一个很有用的概念，<strong>允许程序员更加细粒度的处理对象拷贝时的内存分配问题，提高了对临时对象和不需要的对象的利用率</strong>，极大提高程序的效率。当然，也会引入更多的bug。不过，这就是C++的哲学，什么都允许你做，但出了问题，可别赖C++这门语言。</p><p>完美转发基于万能引用，引用折叠以及std::forward模板函数。据我所知，STL出现std::forward，一定出现万能引用。其实这也很好理解，完美转发机制，是为了将左值和右值统一处理，节约代码量，而只有万能引用会出现同时接受左值和右值的情况，所以完美转发只存在于万能引用中。</p>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> Template </tag>
            
            <tag> C++ STL </tag>
            
            <tag> GNU C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>XML 与 JSON 在设计目的和应用场景上的区别</title>
      <link href="2021/02/21/others/xml-yu-json-zai-she-ji-mu-de-he-ying-yong-chang-jing-shang-de-qu-bie/"/>
      <url>2021/02/21/others/xml-yu-json-zai-she-ji-mu-de-he-ying-yong-chang-jing-shang-de-qu-bie/</url>
      
        <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>今天在看《第一行代码》中关于SharedPreference的部分时，突然觉得用XML保存数据很不方便，比如如果要储存数组，用JSON中一行代码就可以解决的问题，XML还得自己写一些代码，比如：<a href="https://link.zhihu.com/?target=https%3A//stackoverflow.com/questions/3876680/is-it-possible-to-add-an-array-or-object-to-sharedpreferences-on-android">Is it possible to add an array or object to SharedPreferences on Android</a>。</p><p>所以我很好奇，XML在竞争如此激烈的时代，还在广泛被使用，是因为它是一个无法解决的历史遗留问题，还是因为XML确实有比JSON更优越的地方。</p><blockquote><p>注：以下部分段落节选并翻译自<a href="https://link.zhihu.com/?target=https%3A//stackoverflow.com/questions/2620270/what-is-the-difference-between-json-and-xml">StackOverflow</a>，不代表本人观点。</p></blockquote><h2 id="XML与JSON最本质的区别"><a href="#XML与JSON最本质的区别" class="headerlink" title="XML与JSON最本质的区别"></a>XML与JSON最本质的区别</h2><blockquote><p><a href="https://link.zhihu.com/?target=https%3A//stackoverflow.com/a/2620466/6109336">来源</a>，我做了一些注解</p></blockquote><p>最根本上来说，XML是一个markup language（标记语言），而JSON是一种用于数据交换（data-interchange）的序列化对象的语言。</p><p>根据Wiki的说法，标记语言是：</p><blockquote><p>In computer text processing, a markup language is a system for annotating a document in a way that is syntactically distinguishable from the text, meaning when the document is processed for display, the markup language is not shown, and is only used to format the text.</p></blockquote><p>标记语言除了文本信息，还包括了一些元信息，这些元信息用来标注如何处理文本信息，比如：</p><pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Document</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Paragraph</span> <span class="token attr-name">Align</span><span class="token attr-value"><span class="token punctuation attr-equals">=</span><span class="token punctuation">"</span>Center<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>                 <span class="token comment">&lt;!-- Align是元信息 --></span>        Here <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Bold</span><span class="token punctuation">></span></span>is<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Bold</span><span class="token punctuation">></span></span> some text.    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Paragraph</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Document</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>假如试图用JSON完完整整的表述上述的信息：</p><pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>    <span class="token property">"Paragraphs"</span><span class="token operator">:</span> <span class="token punctuation">[</span>        <span class="token punctuation">&#123;</span>            <span class="token property">"align"</span><span class="token operator">:</span> <span class="token string">"center"</span><span class="token punctuation">,</span>            <span class="token property">"content"</span><span class="token operator">:</span> <span class="token punctuation">[</span>                <span class="token string">"Here "</span><span class="token punctuation">,</span> <span class="token punctuation">&#123;</span>                    <span class="token property">"style"</span> <span class="token operator">:</span> <span class="token string">"bold"</span><span class="token punctuation">,</span>                    <span class="token property">"content"</span><span class="token operator">:</span> <span class="token punctuation">[</span> <span class="token string">"is"</span> <span class="token punctuation">]</span>                <span class="token punctuation">&#125;</span><span class="token punctuation">,</span>                <span class="token string">" some text."</span>            <span class="token punctuation">]</span>        <span class="token punctuation">&#125;</span>    <span class="token punctuation">]</span><span class="token punctuation">&#125;</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>是不是觉得JSON比XML还要复杂的多？</p><p>原因在于，JSON里面没有<strong>元数据和数据的区别</strong>，<strong>所有的东西都是数据</strong>，所以要人为的加上一些多余的字符串（比如content）进行区分。</p><hr><p>同样的，XML也不擅长做JSON所擅长做的事，那就是序列化对象：</p><pre class="line-numbers language-json" data-language="json"><code class="language-json"><span class="token punctuation">&#123;</span>    <span class="token property">"firstName"</span><span class="token operator">:</span> <span class="token string">"Homer"</span><span class="token punctuation">,</span>    <span class="token property">"lastName"</span><span class="token operator">:</span> <span class="token string">"Simpson"</span><span class="token punctuation">,</span>    <span class="token property">"relatives"</span><span class="token operator">:</span> <span class="token punctuation">[</span> <span class="token string">"Grandpa"</span><span class="token punctuation">,</span> <span class="token string">"Marge"</span><span class="token punctuation">,</span> <span class="token string">"The Boy"</span><span class="token punctuation">,</span> <span class="token string">"Lisa"</span><span class="token punctuation">,</span> <span class="token string">"I think that's all of them"</span> <span class="token punctuation">]</span><span class="token punctuation">&#125;</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果用XML表示上述对象：</p><pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Person</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>FirstName</span><span class="token punctuation">></span></span>Homer<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>FirstName</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>LastName</span><span class="token punctuation">></span></span>Simpsons<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>LastName</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Relatives</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Relative</span><span class="token punctuation">></span></span>Grandpa<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Relative</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Relative</span><span class="token punctuation">></span></span>Marge<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Relative</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Relative</span><span class="token punctuation">></span></span>The Boy<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Relative</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Relative</span><span class="token punctuation">></span></span>Lisa<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Relative</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Relative</span><span class="token punctuation">></span></span>I think that's all of them<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Relative</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Relatives</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Person</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>从这个例子可以看到JSON有2点优于XML的地方：</p><ul><li>对象的内部结构一目了然，简洁明了。</li><li>JSON语法规定[]是数组，{}是对象，而XML没有如此的语法规定，我们只能临时发明一种方式来表示数组，然后自己添加代码来识别这个数组。</li></ul><p>如果我们人为施加一种策略，那么XML的确可以完成JSON的工作，但是JSON本身内建了这种策略。</p><p>比如<a href="https://link.zhihu.com/?target=https%3A//stackoverflow.com/a/7998630/6109336">这里</a>，有人提出了XJSON，可以用XML完成JSON的工作：</p><pre class="line-numbers language-xml" data-language="xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>xjson</span><span class="token punctuation">></span></span>     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>object</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>name</span><span class="token punctuation">></span></span>persons<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>name</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>array</span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>object</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>Ford Prefect<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>gender</span><span class="token punctuation">></span></span>male<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>gender</span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>object</span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>object</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>Arthur Dent<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>gender</span><span class="token punctuation">></span></span>male<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>gender</span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>object</span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>object</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>value</span><span class="token punctuation">></span></span>Tricia McMillan<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>gender</span><span class="token punctuation">></span></span>female<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>gender</span><span class="token punctuation">></span></span>         <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>object</span><span class="token punctuation">></span></span>      <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>array</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>value</span><span class="token punctuation">></span></span>     <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>object</span><span class="token punctuation">></span></span>  <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>xjson</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><blockquote><p>Once you wrote an XJSON processor, it could do exactly what JSON processor does, for all the types of data that JSON can represent, and you could translate data losslessly between JSON and XJSON.<br>如果你写出一个针对XJSON的解析包，那么它可以完成JSON所有的工作。</p></blockquote><p>不过，这未免也太眼花缭乱了一些。</p><p>所以在表示对象这个问题上面，JSON是远比XML优越的一种语言。</p><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>在我看来，XML和JSON是乍一看有点相似，但设计出发点和应用场景却并不重叠的语言。</p><p>就好像以前我发现MATLAB除了可以做矩阵计算，还可以写GUI，但我觉得不会有人真的用它去写复杂的GUI界面。Python这种语言似乎很万能很流行，什么都可以做，甚至游戏，但不会真的有人去用Python去写游戏引擎的底层部分。</p><p>延伸阅读：</p><p><a href="https://www.zhihu.com/question/25636060">为什么都反对 XML 而支持使用 JSON？</a></p><p>（END）</p>]]></content>
      
      
      <categories>
          
          <category> 编程随想 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> XML </tag>
            
            <tag> JSON </tag>
            
            <tag> 题外话 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
